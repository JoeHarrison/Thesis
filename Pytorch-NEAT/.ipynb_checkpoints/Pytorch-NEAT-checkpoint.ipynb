{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import multiprocessing\n",
    "from collections import OrderedDict, defaultdict\n",
    "from itertools import product\n",
    "from copy import deepcopy, copy\n",
    "from namegenerator import NameGenerator\n",
    "\n",
    "import gym\n",
    "import rubiks\n",
    "\n",
    "# Debugging and profiling\n",
    "import cProfile\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "    - Add Bias term\n",
    "    - Custom weights\n",
    "    - Drop out connections (set weight to 0)\n",
    "    - Custom activation per node\n",
    "    \n",
    "    - Add function that resets stagnation for all species\n",
    "    - Network Visualisation\n",
    "    - Species Visualisation\n",
    "    - Seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(x):\n",
    "    return F.leaky_relu(x)\n",
    "\n",
    "def tanh(x):\n",
    "    return F.Tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return F.relu(x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return F.Sigmoid(x)\n",
    "\n",
    "string_to_activation = {\n",
    "    'leaky_relu' : leaky_relu,\n",
    "    'relu' : relu,\n",
    "    'sigmoid' : sigmoid,\n",
    "    'tanh' : tanh\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,layer_sizes):\n",
    "        super(Model, self).__init__()\n",
    "        layers = OrderedDict()\n",
    "        \n",
    "        previous_layer_size = layer_sizes[0]\n",
    "        for idx, current_layer_size in enumerate(layer_sizes[1:]):\n",
    "            layers[str(idx)] = nn.Linear(previous_layer_size, current_layer_size)\n",
    "            previous_layer_size = current_layer_size\n",
    "            \n",
    "        self.layers = nn.Sequential(layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstname_generator = NameGenerator('names.csv', 2, 12)\n",
    "new_individual_name = firstname_generator.generate_name()\n",
    "previous_names = []\n",
    "surname_generator = NameGenerator('surnames.csv', 2, 12)\n",
    "new_specie_name = surname_generator.generate_name()\n",
    "previous_surnames = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Genotype(object):\n",
    "    def __init__(self, \n",
    "                 inputs = 144, \n",
    "                 outputs = 12, \n",
    "                 nonlinearities = ['relu','sigmoid','tanh'],\n",
    "                 topology = None,\n",
    "                 feedforward = True,\n",
    "                 max_depth = None,\n",
    "                 max_nodes = float('inf'),\n",
    "                 response_default = 4.924273,\n",
    "                 initial_weight_stdev = 2.0,\n",
    "                 bias_as_node = False,\n",
    "                 p_add_neuron = 0.03, \n",
    "                 p_add_connection = 0.3, \n",
    "                 p_mutate_weight = 0.8,\n",
    "                 p_reset_weight = 0.1,\n",
    "                 p_reenable_connection = 0.01,\n",
    "                 p_disable_connection = 0.01, \n",
    "                 p_reenable_parent = 0.25, \n",
    "                 p_mutate_bias = 0.2,\n",
    "                 p_mutate_response = 0.0,\n",
    "                 p_mutate_type = 0.2,\n",
    "                 stdev_mutate_weight = 1.5,\n",
    "                 stdev_mutate_bias = 0.5,\n",
    "                 stdev_mutate_response = 0.5,\n",
    "                 weight_range = (-50.,50.),\n",
    "                 distance_excess_weight = 1.0, \n",
    "                 distance_disjoint_weight = 1.0, \n",
    "                 distance_weight = 0.4):\n",
    "        \n",
    "        self.name = next(new_individual_name)\n",
    "        self.specie = None\n",
    "        \n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "        self.nonlinearities = nonlinearities\n",
    "        self.feedforward = feedforward\n",
    "        self.bias_as_node = bias_as_node\n",
    "        \n",
    "        self.max_depth = max_depth\n",
    "        self.max_nodes = max_nodes\n",
    "        \n",
    "        self.response_default = response_default\n",
    "        self.initial_weight_stdev = initial_weight_stdev\n",
    "        self.stdev_mutate_weight = stdev_mutate_weight\n",
    "        self.stdev_mutate_bias = stdev_mutate_bias\n",
    "        self.stdev_mutate_response = stdev_mutate_response\n",
    "        self.weight_range = weight_range\n",
    "        \n",
    "        # Mutation Probabilities\n",
    "        self.p_add_neuron = p_add_neuron\n",
    "        self.p_add_connection = p_add_connection\n",
    "        self.p_mutate_weight = p_mutate_weight\n",
    "        self.p_reset_weight = p_reset_weight\n",
    "        self.p_reenable_connection = p_reenable_connection\n",
    "        self.p_disable_connection = p_disable_connection\n",
    "        self.p_reenable_parent = p_reenable_parent\n",
    "        self.p_mutate_bias = p_mutate_bias\n",
    "        self.p_mutate_response = p_mutate_response\n",
    "        self.p_mutate_type = p_mutate_type\n",
    "        \n",
    "        # Distance weights\n",
    "        self.distance_excess_weight = distance_excess_weight\n",
    "        self.distance_disjoint_weight = distance_disjoint_weight\n",
    "        self.distance_weight = distance_weight\n",
    "        \n",
    "        # Tuples of: id, non_linearity, bias, layer, ff_order, response\n",
    "        self.neuron_genes = []\n",
    "        # Tuples of: innovation number, input, output, weight, enabled\n",
    "        self.connection_genes = {}\n",
    "        # Hyperparameter genes\n",
    "        self.hyperparameter_genes = []\n",
    "        \n",
    "        self.input_ids = []\n",
    "        self.output_ids = []\n",
    "        \n",
    "        self._initialise_topology(topology)\n",
    "    \n",
    "    def change_specie(self,specie):\n",
    "        self.specie = specie\n",
    "        \n",
    "    def _initialise_topology(self, topology):\n",
    "#         if self.bias_as_node:\n",
    "#             self.inputs += 1\n",
    "        \n",
    "        max_layer = 2048 if (self.max_depth is None) else (self.max_depth - 1)\n",
    "        \n",
    "        if topology is None:\n",
    "            # Initialise inputs\n",
    "            for i in range(self.inputs):\n",
    "                self.neuron_genes.append([i, random.choice(self.nonlinearities),1.0,0, i * 2048, self.response_default])\n",
    "                self.input_ids.append(i)\n",
    "            # Initialise outputs\n",
    "            for i in range(self.outputs):\n",
    "                self.neuron_genes.append([(self.inputs + i), random.choice(self.nonlinearities),1.0,max_layer, (self.inputs + i) * 2048, self.response_default])\n",
    "                self.output_ids.append((self.inputs + i))\n",
    "            # Initialise connections\n",
    "            innovation_number = 0\n",
    "            for i in range(self.inputs):\n",
    "                for j in range(self.inputs,self.inputs + self.outputs):\n",
    "                    weight = self._initialise_weight(self.inputs,self.outputs)\n",
    "                    self.connection_genes[(i,j)] = [innovation_number, i, j, weight ,True]\n",
    "                    innovation_number += 1\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "                \n",
    "    def _initialise_weight(self, input_neurons, output_neurons):\n",
    "        weight = np.random.rand()*np.sqrt(1/(input_neurons + output_neurons))\n",
    "        return weight\n",
    "        \n",
    "    def recombinate(self, other):\n",
    "        child = deepcopy(self)\n",
    "        child.neuron_genes = []\n",
    "        child.connection_genes = {}\n",
    "        \n",
    "        max_neurons = max(len(self.neuron_genes), len(other.neuron_genes))\n",
    "        min_neurons = min(len(self.neuron_genes), len(other.neuron_genes))\n",
    "        \n",
    "        for i in range(max_neurons):\n",
    "            neuron_gene = None\n",
    "            if i < min_neurons:\n",
    "                neuron_gene = random.choice((self.neuron_genes[i], other.neuron_genes[i]))\n",
    "            else:\n",
    "                try:\n",
    "                    neuron_gene = self.neuron_genes[i]\n",
    "                except IndexError:\n",
    "                    neuron_gene = other.neuron_genes[i]\n",
    "            child.neuron_genes.append(deepcopy(neuron_gene))\n",
    "            \n",
    "        self_connections = dict(((c[0], c) for c in self.connection_genes.values()))\n",
    "        other_connections = dict(((c[0], c) for c in other.connection_genes.values()))\n",
    "        max_innovation_number = max(list(self_connections.keys()) + list(other_connections.keys()))\n",
    "        \n",
    "        for i in range(max_innovation_number + 1):\n",
    "            connection_gene = None\n",
    "            if i in self_connections and i in other_connections:\n",
    "                connection_gene = random.choice((self_connections[i],other_connections[i]))\n",
    "                enabled = self_connections[i][4] and other_connections[i][4]\n",
    "            else:\n",
    "                if i in self_connections:\n",
    "                    connection_gene = self_connections[i]\n",
    "                    enabled = connection_gene[4]\n",
    "                elif i in other_connections:\n",
    "                    connection_gene = other_connections[i]\n",
    "                    enabled = connection_gene[4]\n",
    "            if connection_gene is not None:\n",
    "                child.connection_genes[(connection_gene[1],connection_gene[2])] = deepcopy(connection_gene)\n",
    "                child.connection_genes[(connection_gene[1],connection_gene[2])][4] = enabled or np.random.rand() < self.p_reenable_parent\n",
    "\n",
    "            def is_feedforward(item):\n",
    "                ((fr, to), cg) = item\n",
    "                return child.neuron_genes[fr][0] < child.neuron_genes[to][0]\n",
    "\n",
    "            if self.feedforward:\n",
    "                child.connection_genes = dict(filter(is_feedforward, child.connection_genes.items()))\n",
    "            \n",
    "        return child\n",
    "        \n",
    "    def mutate(self, innovations = {}, global_innovation_number = 0):\n",
    "        maximum_innovation_number = max(global_innovation_number, max(cg[0] for cg in self.connection_genes.values()))\n",
    "        # TODO: move to separate functions\n",
    "        if len(self.neuron_genes) < self.max_nodes and np.random.rand() < self.p_add_neuron:\n",
    "            possible_to_split = self.connection_genes.keys()\n",
    "            \n",
    "            if self.max_depth is not None:\n",
    "                possible_to_split = [(fr, to) for (fr, to) in possible_to_split if self.neuron_genes[fr][4] + 1 < self.neuron_genes[to][4]]\n",
    "            \n",
    "            if possible_to_split:\n",
    "\n",
    "                # Choose connection to split\n",
    "                split_neuron = self.connection_genes[random.choice(list(self.connection_genes.keys()))]\n",
    "                # Disable old connection\n",
    "                split_neuron[4] = False\n",
    "\n",
    "                input_neuron, output_neuron, weight = split_neuron[1:4]\n",
    "                fforder = (self.neuron_genes[input_neuron][0] + self.neuron_genes[input_neuron][0]) * 0.5\n",
    "                nonlinearity = random.choice(self.nonlinearities)\n",
    "                layer = self.neuron_genes[input_neuron][3] + 1\n",
    "                \n",
    "                new_id = len(self.neuron_genes)\n",
    "\n",
    "                neuron = [new_id, nonlinearity, 1.0, layer, fforder, self.response_default]\n",
    "\n",
    "                self.neuron_genes.append(neuron)\n",
    "                \n",
    "                if (input_neuron, new_id) in innovations:\n",
    "                    innovation_number = innovations[(input_neuron,new_id)]\n",
    "                else:\n",
    "                    maximum_innovation_number += 1\n",
    "                    innovation_number = innovations[(input_neuron,new_id)] = maximum_innovation_number\n",
    "                    \n",
    "                # 1.0 to initialise_weight?\n",
    "                self.connection_genes[(input_neuron, new_id)] = [innovation_number, input_neuron, new_id, 1.0, True]\n",
    "                \n",
    "                if (new_id, output_neuron) in innovations:\n",
    "                    innovation_number = innovations[(new_id, output_neuron)]\n",
    "                else:\n",
    "                    maximum_innovation_number += 1\n",
    "                    innovation_number = innovations[(new_id, output_neuron)] = maximum_innovation_number\n",
    "                    \n",
    "                self.connection_genes[(new_id, output_neuron)] = [innovation_number, new_id, output_neuron, weight, True]\n",
    "                \n",
    "                \n",
    "        elif np.random.rand() < self.p_add_connection:\n",
    "            potential_connections = product(range(len(self.neuron_genes)),range(self.inputs, len(self.neuron_genes)))\n",
    "            potential_connections = (connection for connection in potential_connections if connection not in self.connection_genes)\n",
    "            \n",
    "            if self.feedforward:\n",
    "                potential_connections = ((f, t) for (f, t) in potential_connections if self.neuron_genes[f][4] < self.neuron_genes[t][4])\n",
    "            \n",
    "            potential_connections = list(potential_connections)\n",
    "            if potential_connections:\n",
    "                (fr, to) = random.choice(potential_connections)\n",
    "                if (fr, to) in innovations:\n",
    "                    innovation = innovations[(fr, to)]\n",
    "                else:\n",
    "                    maximum_innovation_number += 1\n",
    "                    innovation = innovations[(fr, to)] = maximum_innovation_number\n",
    "                # get number of neurons in layers of fr and to\n",
    "                connection_gene = [innovation, fr, to, self._initialise_weight(2,2), True]\n",
    "                self.connection_genes[(fr, to)] = connection_gene\n",
    "        else:\n",
    "            for cg in self.connection_genes.values():\n",
    "                if np.random.rand() < self.p_mutate_weight:\n",
    "                    cg[3] += np.random.normal(0.0, self.stdev_mutate_weight)\n",
    "                    cg[3] = np.clip(cg[3], self.weight_range[0], self.weight_range[1])\n",
    "                    # clipping?\n",
    "                if np.random.rand() < self.p_reset_weight:\n",
    "                    cg[3] = np.random.normal(0.0,self.stdev_mutate_weight)\n",
    "                    \n",
    "                # bigger chance to disable in this way\n",
    "                if np.random.rand() < self.p_reenable_connection:\n",
    "                    cg[4] = True\n",
    "                    \n",
    "                if np.random.rand() < self.p_disable_connection:\n",
    "                    cg[4] = False\n",
    "                    \n",
    "            for neuron_gene in self.neuron_genes[self.inputs:]:\n",
    "                if np.random.rand() < self.p_mutate_bias:\n",
    "                    neuron_gene[2] += np.random.normal(0.0, 1)\n",
    "\n",
    "                    neuron_gene[2] = np.clip(neuron_gene[2], self.weight_range[0], self.weight_range[1])\n",
    "                \n",
    "                if np.random.rand() < self.p_mutate_type:\n",
    "                    neuron_gene[1] = random.choice(self.nonlinearities)\n",
    "                    \n",
    "                if np.random.rand() < self.p_mutate_response:\n",
    "                    neuron_gene[5] += np.random.normal(0.0, self.stdev_mutate_response)\n",
    "                    \n",
    "        return self\n",
    "        \n",
    "    def distance(self, other):\n",
    "        self_connections = dict(((c[0], c) for c in self.connection_genes.values()))\n",
    "        other_connections = dict(((c[0], c) for c in other.connection_genes.values()))\n",
    "\n",
    "        all_innovations = list(self_connections.keys()) + list(other_connections.keys())\n",
    "\n",
    "        minimum_innovation = min(all_innovations)\n",
    "        \n",
    "        e = 0\n",
    "        d = 0\n",
    "        w = 0.0\n",
    "        m = 0\n",
    "        \n",
    "        for innovation_key in all_innovations:\n",
    "            if innovation_key in self_connections and innovation_key in other_connections:\n",
    "                w += np.abs(self_connections[innovation_key][3] - other_connections[innovation_key][3])\n",
    "                m += 1\n",
    "            elif innovation_key in self_connections or innovation_key in other_connections:\n",
    "                # Disjoint genes\n",
    "                if innovation_key < minimum_innovation:\n",
    "                    d += 1\n",
    "                # Excess genes\n",
    "                else:\n",
    "                    e += 1\n",
    "                    \n",
    "        # Average weight differences of matching genes\n",
    "        w = (w/m) if m>0 else w\n",
    "        \n",
    "        return (self.distance_excess_weight * e +\n",
    "               self.distance_disjoint_weight * d +\n",
    "               self.distance_weight * w)\n",
    "    \n",
    "#     def get_network_data(self):\n",
    "#         \"\"\" Returns a tuple of (connection_matrix, node_types) \n",
    "#             that is reordered by the \"feed-forward order\" of the network,\n",
    "#             Such that if feedforward was set to true, the matrix will be\n",
    "#             lower-triangular.\n",
    "#             The node bias is inserted as \"node 0\", the leftmost column\n",
    "#             of the matrix.\n",
    "#         \"\"\"\n",
    "        \n",
    "#         # Assemble connectivity matrix\n",
    "#         cm = np.zeros((len(self.neuron_genes), len(self.neuron_genes)))\n",
    "#         cm.fill(np.nan)\n",
    "        \n",
    "#         for (_, fr, to, weight, enabled) in self.connection_genes.values():\n",
    "#             if enabled:\n",
    "#                 cm[to, fr] = weight\n",
    "        \n",
    "#         # Reorder the nodes/connections\n",
    "# #         ff, node_types, bias, response, layer = zip(*self.neuron_genes)\n",
    "#         ff, node_types, bias, layer = zip(*self.neuron_genes)\n",
    "#         order = [i for _,i in sorted(zip(ff, range(len(ff))))]\n",
    "#         cm = cm[:,order][order,:]\n",
    "#         node_types = np.array(node_types)[order]\n",
    "#         bias = np.array(bias)[order]\n",
    "# #         response = np.array(response)[order]\n",
    "#         layers = np.array(layer)[order]\n",
    "\n",
    "# #         # Then, we multiply all the incoming connection weights by the response\n",
    "#         cm *= np.atleast_2d(4.9).T\n",
    "#         # Finally, add the bias as incoming weights from node-0\n",
    "#         if not self.bias_as_node:\n",
    "#             cm = np.hstack( (np.atleast_2d(bias).T, cm) )\n",
    "#             cm = np.insert(cm, 0, 0.0, axis=0)\n",
    "#             # TODO: this is a bit ugly, we duplicate the first node type for \n",
    "#             # bias node. It shouldn't matter though since the bias is used as an input.\n",
    "#             node_types = [node_types[0]] + list(node_types)\n",
    "\n",
    "#         if self.feedforward and np.triu(np.nan_to_num(cm)).any():\n",
    "#             import pprint\n",
    "#             pprint.pprint(self.neuron_genes)\n",
    "#             pprint.pprint(self.connection_genes)\n",
    "#             print(ff)\n",
    "#             print(order)\n",
    "#             print(np.sign(cm))\n",
    "#             raise Exception(\"Network is not feedforward.\")\n",
    "        \n",
    "#         return cm, node_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Species(object):\n",
    "    def __init__(self, initial_member):\n",
    "        self.name = next(new_specie_name)\n",
    "        self.members = [initial_member]\n",
    "        self.representative = initial_member\n",
    "        self.offspring = 0\n",
    "        self.age = 0\n",
    "        self.average_fitness = 0.\n",
    "        self.max_fitness = 0.\n",
    "        self.max_fitness_previous = 0.0\n",
    "        self.stagnation = 0\n",
    "        self.has_best = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_individual(item):\n",
    "    (individual, evaluator) = item\n",
    "    if callable(evaluator):\n",
    "        individual.stats = evaluator(individual)\n",
    "    elif hasattr(evaluator, 'evaluate'):\n",
    "        individual.stats = evaluator.evaluate(individual)\n",
    "    else:\n",
    "        raise Exception(\"Evaluator must be a callable or object\" \\\n",
    "                    \"with a callable attribute 'evaluate'.\")\n",
    "    return individual\n",
    "\n",
    "class Population(object):\n",
    "    def __init__(self, genome_factory,\n",
    "                population_size = 100,\n",
    "                elitism = True,\n",
    "                stop_when_solved = False,\n",
    "                tournament_selection_k = 3,\n",
    "                verbose = True,\n",
    "                max_cores = 1,\n",
    "                compatibility_threshold = 3.0,\n",
    "                compatibility_threshold_delta = 0.4,\n",
    "                target_species = 12,\n",
    "                minimum_elitism_size = 5,\n",
    "                young_age = 10,\n",
    "                young_multiplier = 1.2,\n",
    "                old_age = 30,\n",
    "                old_multiplier = 0.2,\n",
    "                stagnation_age = 15,\n",
    "                reset_innovations = False,\n",
    "                survival = 0.2):\n",
    "        \n",
    "        self.genome_factory = genome_factory\n",
    "        self.population_size = population_size\n",
    "        self.elitism = elitism\n",
    "        self.stop_when_solved = stop_when_solved\n",
    "        self.tournament_selection_k = tournament_selection_k\n",
    "        self.verbose = verbose\n",
    "        self.max_cores = max_cores\n",
    "        \n",
    "        cpus = multiprocessing.cpu_count()\n",
    "        use_cores = min(self.max_cores, cpus-1)\n",
    "        if use_cores > 1:\n",
    "            self.pool = multiprocessing.Pool(processes=use_cores, maxtasksperchild=5)\n",
    "        else:\n",
    "            self.pool = None\n",
    "        \n",
    "        self.compatibility_threshold = compatibility_threshold\n",
    "        self.compatibility_threshold_delta = compatibility_threshold_delta\n",
    "        \n",
    "        self.target_species = target_species\n",
    "        self.minimum_elitism_size = minimum_elitism_size\n",
    "        \n",
    "        self.young_age = young_age\n",
    "        self.young_multiplier = young_multiplier\n",
    "        self.old_age = old_age\n",
    "        self.old_multiplier = old_multiplier\n",
    "        \n",
    "        self.stagnation_age = stagnation_age\n",
    "        \n",
    "        self.reset_innovations = reset_innovations\n",
    "        self.survival = survival\n",
    "        \n",
    "    def _evaluate_all(self, population, evaluator):\n",
    "        to_eval = [(individual, evaluator) for individual in population]\n",
    "        if self.pool is not None:\n",
    "            population = list(self.pool.map(evaluate_individual, to_eval))\n",
    "        else:\n",
    "            population = list(map(evaluate_individual, to_eval))\n",
    "        \n",
    "        return population\n",
    "        \n",
    "    def _reset(self):\n",
    "        self.champions = []\n",
    "        self.generation = 0\n",
    "        self.solved_at = None\n",
    "        self.stats = defaultdict(list)\n",
    "        self.species = []\n",
    "        self.global_innovation_number = 0\n",
    "        self.innovations = {}\n",
    "        self.current_compatibility_threshold = self.compatibility_threshold\n",
    "        \n",
    "    def _find_best(self, population, solution = None):\n",
    "        self.champions.append(max(population, key=lambda individual: individual.stats['fitness']))\n",
    "        \n",
    "        if solution is not None:\n",
    "            if isinstance(solution, (int, float)):\n",
    "                solved = (self.champions[-1].stats['fitness'] >= solution)\n",
    "            elif callable(solution):\n",
    "                solved = solution(self.champions[-1])\n",
    "            elif hasattr(solution, 'solve'):\n",
    "                solved = solution.solve(self.champions[-1])\n",
    "                \n",
    "            if solved and self.solved_at is None:\n",
    "                self.solved_at = self.generation + 1\n",
    "            \n",
    "    @property\n",
    "    def population(self):\n",
    "        for specie in self.species:\n",
    "            for member in specie.members:\n",
    "                yield member\n",
    "    \n",
    "    def _evolve(self, evaluator, solution=None):\n",
    "        population = list(self.population)\n",
    "        \n",
    "        while len(population) < self.population_size:\n",
    "            individual = self.genome_factory()\n",
    "            population.append(individual)        \n",
    "            \n",
    "        population = self._evaluate_all(population, evaluator)\n",
    "        \n",
    "        # Speciation\n",
    "        for specie in self.species:\n",
    "            # Choose random specie representative for distance comparison\n",
    "            specie.representative = random.choice(specie.members)\n",
    "            specie.name = specie.representative.specie\n",
    "            specie.members = []\n",
    "            specie.age += 1\n",
    "            \n",
    "        # Add each individual to a species\n",
    "        for individual in population:\n",
    "            found = False\n",
    "            for specie in self.species:\n",
    "                if individual.distance(specie.representative) <= self.current_compatibility_threshold:\n",
    "                    specie.members.append(individual)\n",
    "                    individual.change_specie(specie.name)\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                s = Species(individual)\n",
    "                individual.change_specie(s.name)\n",
    "                self.species.append(s)\n",
    "        \n",
    "        # Remove empty species\n",
    "        self.species = list(filter(lambda s: len(s.members) > 0, self.species))\n",
    "        \n",
    "        # Adjust compatibility threshold\n",
    "        if len(self.species) < self.target_species:\n",
    "            self.current_compatibility_threshold -= self.compatibility_threshold_delta\n",
    "        elif len(self.species) > self.target_species:\n",
    "            self.current_compatibility_threshold += self.compatibility_threshold_delta\n",
    "        \n",
    "        # Find champion and check for solution\n",
    "        self._find_best(population, solution)\n",
    "        \n",
    "        # Recombination\n",
    "        \n",
    "        for specie in self.species:\n",
    "            specie.max_fitness_previous = specie.max_fitness\n",
    "            specie.average_fitness = np.mean([individual.stats['fitness'] for individual in specie.members])\n",
    "            specie.max_fitness = np.max([individual.stats['fitness'] for individual in specie.members])\n",
    "            if specie.max_fitness <= specie.max_fitness_previous:\n",
    "                specie.stagnation += 1\n",
    "            else:\n",
    "                specie.stagnation = 0\n",
    "            specie.has_best = self.champions[-1] in specie.members\n",
    "        \n",
    "        # Keep species that have the best or within stagnation age range\n",
    "        self.species = list(filter(lambda s: s.stagnation < self.stagnation_age or s.has_best, self.species))\n",
    "        \n",
    "        average_fitness = np.array([specie.average_fitness for specie in self.species])\n",
    "        \n",
    "        # Adjust fitness based on age\n",
    "        age = np.array([specie.age for specie in self.species])\n",
    "        for specie in self.species:\n",
    "            if specie.age < self.young_age:\n",
    "                specie.average_fitness *= self.young_multiplier\n",
    "            if specie.age > self.old_age:\n",
    "                specie.average_fitness *= self.old_multiplier\n",
    "                \n",
    "        # Compute offspring size\n",
    "        total_fitness = sum(specie.average_fitness for specie in self.species)\n",
    "        for specie in self.species:\n",
    "            specie.offspring = int(round(self.population_size * specie.average_fitness / total_fitness))\n",
    "            \n",
    "        \n",
    "        \n",
    "        # Remove species without offspring\n",
    "        self.species = list(filter(lambda s: s.offspring > 0, self.species))\n",
    "\n",
    "        for specie in self.species:\n",
    "            specie.members.sort(key=lambda individual: individual.stats['fitness'], reverse = True)\n",
    "            keep = max(1, int(round(len(specie.members)*self.survival)))\n",
    "            pool = specie.members[:keep]\n",
    "            \n",
    "            if self.elitism and len(specie.members) > self.minimum_elitism_size:\n",
    "                specie.members = specie.members[:1]\n",
    "            else:\n",
    "                specie.members = []\n",
    "                \n",
    "            while len(specie.members) < specie.offspring:\n",
    "                k = min(len(pool), self.tournament_selection_k)\n",
    "                p1 = max(random.sample(pool,k), key=lambda individual: individual.stats['fitness'])\n",
    "                p2 = max(random.sample(pool,k), key=lambda individual: individual.stats['fitness'])\n",
    "                \n",
    "                child = p1.recombinate(p2)\n",
    "                child.mutate(innovations=self.innovations, global_innovation_number = self.global_innovation_number)\n",
    "                specie.members.append(child)\n",
    "                \n",
    "        if self.innovations:\n",
    "            self.global_innovation_number = max(self.innovations.values())\n",
    "            \n",
    "        self._gather_stats(population)\n",
    "            \n",
    "    def epoch(self, evaluator, generations, solution=None, reset=True, callback= None):\n",
    "        if reset:\n",
    "            self._reset()\n",
    "            \n",
    "        for i in range(generations):\n",
    "            self.time = time.time()\n",
    "            self._evolve(evaluator, solution)\n",
    "            self.generation += 1\n",
    "            \n",
    "            if self.verbose:\n",
    "                self._status_report()\n",
    "                \n",
    "            if callback is not None:\n",
    "                callback(self)\n",
    "            \n",
    "            if self.solved_at is not None and self.stop_when_solved:\n",
    "                break\n",
    "        \n",
    "        return {'stats': self.stats, 'champions': self.champions}\n",
    "    \n",
    "    def _gather_stats(self, population):\n",
    "        for key in population[0].stats:\n",
    "            self.stats[key+'_avg'].append(np.mean([individual.stats[key] for individual in population]))\n",
    "            self.stats[key+'_max'].append(np.max([individual.stats[key] for individual in population]))\n",
    "            self.stats[key+'_min'].append(np.min([individual.stats[key] for individual in population]))\n",
    "        self.stats['solved'].append( self.solved_at is not None )\n",
    "    \n",
    "    def _status_report(self):\n",
    "        print(\"\\n****** Running Generation %d ******\" % self.generation)\n",
    "        fitness_list = np.array([i.stats['fitness'] for i in self.population])\n",
    "        number_neurons = len(self.champions[-1].neuron_genes)\n",
    "        number_enabled_connections = np.sum([1 for conn in self.champions[-1].connection_genes.values() if conn[4]])\n",
    "        print(\"Population's average fitness: %.5f stdev: %.5f\" % (np.average(fitness_list), np.std(fitness_list)))\n",
    "        print(\"Best individual: %s %s\" % (self.champions[-1].name, self.champions[-1].specie))\n",
    "        print(\"Best fitness: %.2f - #neurons: %i - #enabled connections: %i\" % (self.champions[-1].stats['fitness'],number_neurons,number_enabled_connections))\n",
    "        print(\"Population of %i members in %i species:\" % (len(list(self.population)), len(self.species)))\n",
    "        print(\"Species         age    size    fitness    stag\")\n",
    "        print(\"============    ===    ====    =======    ====\")\n",
    "        for specie in self.species:\n",
    "            print(\"{: >12}    {: >3}    {: >4}    {:.5f}    {: >4}\".format(specie.name,specie.age,len(specie.members),specie.max_fitness,specie.stagnation))\n",
    "        print(\"Generation time: %d seconds\" % (time.time()-self.time))\n",
    "        print(\"Solved in generation: %s\" % (self.solved_at))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_from_coo(shape, conns, dtype=torch.float64):\n",
    "    mat = torch.zeros(shape, dtype=dtype)\n",
    "    idxs, weights = conns\n",
    "    if len(idxs) == 0:\n",
    "        return mat\n",
    "    rows, cols = np.array(idxs).transpose()\n",
    "    mat[torch.tensor(rows), torch.tensor(cols)] = torch.tensor(\n",
    "        weights, dtype=dtype)\n",
    "    return mat\n",
    "\n",
    "def required_for_output(inputs, outputs, connections):\n",
    "    \"\"\"\n",
    "    Collect the nodes whose state is required to compute the final network output(s).\n",
    "    :param inputs: list of the input identifiers\n",
    "    :param outputs: list of the output node identifiers\n",
    "    :param connections: list of (input, output) connections in the network.\n",
    "    NOTE: It is assumed that the input identifier set and the node identifier set are disjoint.\n",
    "    By convention, the output node ids are always the same as the output index.\n",
    "    Returns a set of identifiers of required nodes.\n",
    "    \"\"\"\n",
    "    required = set(outputs)\n",
    "    s = set(outputs)\n",
    "    while 1:\n",
    "        # Find nodes not in S whose output is consumed by a node in s.\n",
    "        t = set(a for (a, b) in connections if b in s and a not in s)\n",
    "\n",
    "        if not t:\n",
    "            break\n",
    "\n",
    "        layer_nodes = set(x for x in t if x not in inputs)\n",
    "        if not layer_nodes:\n",
    "            break\n",
    "\n",
    "        required = required.union(layer_nodes)\n",
    "        s = s.union(t)\n",
    "\n",
    "    return list(required)\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self, n_inputs, n_hidden, n_outputs,\n",
    "                 input_to_hidden, hidden_to_hidden, output_to_hidden,\n",
    "                 input_to_output, hidden_to_output, output_to_output,\n",
    "                 hidden_responses, output_responses,\n",
    "                 hidden_biases, output_biases,\n",
    "                 batch_size=1,\n",
    "                 activation = 'relu',\n",
    "                 use_current_activs=False,\n",
    "                 n_internal_steps=1,\n",
    "                 dtype=torch.float64):\n",
    "\n",
    "        self.use_current_activs = use_current_activs\n",
    "        self.activation = string_to_activation[activation]\n",
    "        self.n_internal_steps = n_internal_steps\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_outputs = n_outputs\n",
    "        \n",
    "        if n_hidden > 0:\n",
    "            self.input_to_hidden = dense_from_coo(\n",
    "                (n_hidden, n_inputs), input_to_hidden, dtype=dtype)\n",
    "            self.hidden_to_hidden = dense_from_coo(\n",
    "                (n_hidden, n_hidden), hidden_to_hidden, dtype=dtype)\n",
    "            self.output_to_hidden = dense_from_coo(\n",
    "                (n_hidden, n_outputs), output_to_hidden, dtype=dtype)\n",
    "            self.hidden_to_output = dense_from_coo(\n",
    "                (n_outputs, n_hidden), hidden_to_output, dtype=dtype)\n",
    "        self.input_to_output = dense_from_coo(\n",
    "            (n_outputs, n_inputs), input_to_output, dtype=dtype)\n",
    "        self.output_to_output = dense_from_coo(\n",
    "            (n_outputs, n_outputs), output_to_output, dtype=dtype)\n",
    "        \n",
    "        if n_hidden > 0:\n",
    "            self.hidden_responses = torch.tensor(hidden_responses, dtype=dtype)\n",
    "            self.hidden_biases = torch.tensor(hidden_biases, dtype=dtype)\n",
    "\n",
    "        self.output_responses = torch.tensor(\n",
    "            output_responses, dtype=dtype)\n",
    "        self.output_biases = torch.tensor(output_biases, dtype=dtype)\n",
    "\n",
    "        self.reset(batch_size)\n",
    "\n",
    "    def reset(self, batch_size=1):\n",
    "        if self.n_hidden > 0:\n",
    "            self.activs = torch.zeros(\n",
    "                batch_size, self.n_hidden, dtype=self.dtype)\n",
    "        else:\n",
    "            self.activs = None\n",
    "        self.outputs = torch.zeros(\n",
    "            batch_size, self.n_outputs, dtype=self.dtype)\n",
    "        \n",
    "    def activate(self, inputs):\n",
    "        '''\n",
    "        inputs: (batch_size, n_inputs)\n",
    "        returns: (batch_size, n_outputs)\n",
    "        '''\n",
    "        with torch.no_grad():\n",
    "            inputs = torch.tensor(inputs, dtype=self.dtype)\n",
    "            activs_for_output = self.activs\n",
    "            if self.n_hidden > 0:\n",
    "                for _ in range(self.n_internal_steps):\n",
    "                    self.activs = self.activation(self.hidden_responses * (\n",
    "                        self.input_to_hidden.mm(inputs.t()).t() +\n",
    "                        self.hidden_to_hidden.mm(self.activs.t()).t() +\n",
    "                        self.output_to_hidden.mm(self.outputs.t()).t()) +\n",
    "                        self.hidden_biases)\n",
    "                if self.use_current_activs:\n",
    "                    activs_for_output = self.activs\n",
    "\n",
    "            output_inputs = (self.input_to_output.mm(inputs.t()).t() +\n",
    "                             self.output_to_output.mm(self.outputs.t()).t())\n",
    "            if self.n_hidden > 0:\n",
    "                output_inputs += self.hidden_to_output.mm(\n",
    "                    activs_for_output.t()).t()\n",
    "            self.outputs = self.activation(\n",
    "                self.output_responses * output_inputs + self.output_biases)\n",
    "        return self.outputs\n",
    "    \n",
    "    @staticmethod\n",
    "    def create(genome, batch_size = 1, activation = 'relu', use_current_activs = False, n_internal_steps = 1):\n",
    "        required = required_for_output(genome.input_ids, genome.output_ids, genome.connection_genes)\n",
    "        \n",
    "        input_keys = genome.input_ids\n",
    "        hidden_keys = [k[0] for k in genome.neuron_genes if k[0] not in genome.output_ids and k[0] not in genome.input_ids]\n",
    "        output_keys = genome.output_ids\n",
    "        \n",
    "        ipdb.set_trace()\n",
    "        \n",
    "        hidden_responses = [genome.neuron_genes[k][5] for k in hidden_keys]\n",
    "        output_responses = [genome.neuron_genes[k][5] for k in output_keys]\n",
    "\n",
    "        hidden_biases = [genome.neuron_genes[k][2] for k in hidden_keys]\n",
    "        output_biases = [genome.neuron_genes[k][2] for k in output_keys]\n",
    "        \n",
    "        n_inputs = len(input_keys)\n",
    "        n_hidden = len(hidden_keys)\n",
    "        n_outputs = len(output_keys)\n",
    "        \n",
    "        input_to_hidden = ([], [])\n",
    "        hidden_to_hidden = ([], [])\n",
    "        output_to_hidden = ([], [])\n",
    "        input_to_output = ([], [])\n",
    "        hidden_to_output = ([], [])\n",
    "        output_to_output = ([], [])\n",
    "        \n",
    "        input_key_to_idx = {k: i for i, k in enumerate(input_keys)}\n",
    "        hidden_key_to_idx = {k: i for i, k in enumerate(hidden_keys)}\n",
    "        output_key_to_idx = {k: i for i, k in enumerate(output_keys)}\n",
    "        \n",
    "        def key_to_idx(key):\n",
    "            if key in input_keys:\n",
    "                return input_key_to_idx[key]\n",
    "            elif key in hidden_keys:\n",
    "                return hidden_key_to_idx[key]\n",
    "            elif key in output_keys:\n",
    "                return output_key_to_idx[key]\n",
    "        \n",
    "        for connection in genome.connection_genes.values():\n",
    "            if not connection[4]:\n",
    "                continue\n",
    "                \n",
    "            input_key = key_to_idx(connection[1])\n",
    "            output_key = key_to_idx(connection[2])\n",
    "            \n",
    "            if output_key not in required and input_key not in required:\n",
    "                continue\n",
    "                \n",
    "            if input_key in input_keys and output_key in hidden_keys:\n",
    "                idxs, vals = input_to_hidden\n",
    "            elif input_key in hidden_keys and output_key in hidden_keys:\n",
    "                idxs, vals = hidden_to_hidden\n",
    "            elif input_key in output_keys and output_key in hidden_keys:\n",
    "                idxs, vals = output_to_hidden\n",
    "            elif input_key in input_keys and output_key in output_keys:\n",
    "                idxs, vals = input_to_output\n",
    "            elif input_key in hidden_keys and output_key in output_keys:\n",
    "                idxs, vals = hidden_to_output\n",
    "            elif input_key in output_keys and output_key in output_keys:\n",
    "                idxs, vals = output_to_output\n",
    "                \n",
    "            idxs.append((output_key, input_key))  # to, from\n",
    "            vals.append(connection[3])\n",
    "        \n",
    "        return NeuralNetwork(n_inputs, n_hidden, n_outputs,\n",
    "                            input_to_hidden, hidden_to_hidden, output_to_hidden,\n",
    "                            input_to_output, hidden_to_output, output_to_output,\n",
    "                            hidden_responses, output_responses,\n",
    "                            hidden_biases, output_biases,\n",
    "                            batch_size,\n",
    "                            activation,\n",
    "                            use_current_activs,\n",
    "                            n_internal_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome = Genotype(2,1)\n",
    "genome.connection_genes[(0,2)][4] = False\n",
    "genome.neuron_genes.append([3,'sigmoid',1.0,1,0,4.9])\n",
    "genome.connection_genes[(0,3)] = [2,0,3,3.0,True]\n",
    "genome.neuron_genes.append([4,'sigmoid',1.0,1,0,4.9])\n",
    "genome.connection_genes[(0,4)] = [4,0,4,2.0,True]\n",
    "print(genome.neuron_genes)\n",
    "print(genome.connection_genes)\n",
    "network = NeuralNetwork.create(genome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XORTask(object):\n",
    "    \n",
    "    # Default XOR input/output pairs\n",
    "    INPUTS  = [(0,0), (0,1), (1,0), (1,1)]\n",
    "    OUTPUTS = [(-1,), (1,), (1,), (-1,)]\n",
    "    EPSILON = 1e-100\n",
    "    \n",
    "    def __init__(self, do_all=True):\n",
    "        self.do_all = do_all\n",
    "        self.INPUTS = np.array(self.INPUTS, dtype=float)\n",
    "        self.OUTPUTS = np.array(self.OUTPUTS, dtype=float)\n",
    "    \n",
    "    def evaluate(self, network, verbose=False):\n",
    "        if not isinstance(network, NeuralNetwork):\n",
    "            network = NeuralNetwork.create(network)\n",
    "        \n",
    "        pairs = list(zip(self.INPUTS, self.OUTPUTS))\n",
    "        random.shuffle(pairs)\n",
    "        if not self.do_all:\n",
    "            pairs = [random.choice(pairs)]\n",
    "        rmse = 0.0\n",
    "        for (i, target) in pairs:\n",
    "            # Feed with bias\n",
    "            output = network.activate(np.array([i]))\n",
    "            # Grab the output\n",
    "            output = output[-len(target):]\n",
    "            err = (target - output)\n",
    "            err[abs(err) < self.EPSILON] = 0;\n",
    "            err = (err ** 2).mean()\n",
    "            # Add error\n",
    "            if verbose:\n",
    "                print(\"%r -> %r (%.2f)\" % (i, output, err))\n",
    "            rmse += err \n",
    "\n",
    "        score = 1/(1+np.sqrt(rmse / len(pairs)))\n",
    "        return {'fitness': score}\n",
    "        \n",
    "    def solve(self, network):\n",
    "        return int(self.evaluate(network)['fitness'] > 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RubiksTask(object):\n",
    "    def __init__(self):\n",
    "        self.env = rubiks.RubiksEnv(2)\n",
    "    \n",
    "    def evaluate(self, network, verbose=False):\n",
    "        if not isinstance(network, NeuralNetwork):\n",
    "            network = NeuralNetwork.create(network)\n",
    "        \n",
    "        fitness = 0.000001\n",
    "        \n",
    "        for i in range(100):\n",
    "            done = False\n",
    "            tries = 0\n",
    "            \n",
    "            max_tries = 1\n",
    "            state = self.env.reset(1)\n",
    "            \n",
    "            while tries < max_tries and not done:\n",
    "                action_probabilities = network.feed(state)\n",
    "                action = np.argmax(action_probabilities)\n",
    "                \n",
    "                next_state, reward, done, info = self.env.step(int(action))\n",
    "                \n",
    "                tries += 1\n",
    "                state = next_state\n",
    "            if done:\n",
    "                fitness += 1.0\n",
    "                \n",
    "        fitness = fitness / 100\n",
    "        \n",
    "        return {'fitness' : fitness}\n",
    "        \n",
    "    def solve(self, network):\n",
    "        return int(self.evaluate(network)['fitness'] > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = 2\n",
    "outputs = 1\n",
    "nonlinearities = ['tanh','relu','sigmoid']\n",
    "topology = None\n",
    "feedforward = True\n",
    "max_depth = None\n",
    "max_nodes = float('inf')\n",
    "response_default = 4.924273\n",
    "bias_as_node = False\n",
    "initial_weight_stdev = 2.0\n",
    "p_add_neuron = 0.03\n",
    "p_add_connection = 0.3\n",
    "p_mutate_weight = 0.8\n",
    "p_reset_weight = 0.1\n",
    "p_reenable_connection = 0.01\n",
    "p_disable_connection = 0.01\n",
    "p_reenable_parent=0.25\n",
    "p_mutate_bias = 0.2\n",
    "p_mutate_response = 0.0\n",
    "p_mutate_type = 0.2\n",
    "stdev_mutate_weight = 1.5\n",
    "stdev_mutate_bias = 0.5\n",
    "stdev_mutate_response = 0.5\n",
    "weight_range = (-50.,50.)\n",
    "\n",
    "distance_excess_weight = 1.0\n",
    "distance_disjoint_weight = 1.0\n",
    "distance_weight = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "population_size = 100\n",
    "elitism = False\n",
    "stop_when_solved = True \n",
    "tournament_selection_k = 3 \n",
    "verbose = True\n",
    "max_cores = 1\n",
    "compatibility_threshold = 3.0\n",
    "compatibility_threshold_delta = 0.4 \n",
    "target_species = 12\n",
    "minimum_elitism_size = 5 \n",
    "young_age = 10\n",
    "young_multiplier = 1.2 \n",
    "old_age = 30\n",
    "old_multiplier = 0.2 \n",
    "stagnation_age = 15\n",
    "reset_innovations = False\n",
    "survival = 0.2\n",
    "\n",
    "genome_factory = lambda: Genotype(inputs, outputs, nonlinearities, topology, feedforward,\n",
    "                                  max_depth, max_nodes, response_default, initial_weight_stdev,\n",
    "                                  bias_as_node, p_add_neuron, p_add_connection, p_mutate_weight, \n",
    "                                  p_reset_weight, p_reenable_connection, p_disable_connection,\n",
    "                                  p_reenable_parent, p_mutate_bias, p_mutate_response, p_mutate_type,\n",
    "                                  stdev_mutate_weight, stdev_mutate_bias, stdev_mutate_response,\n",
    "                                  weight_range, distance_excess_weight, distance_disjoint_weight,\n",
    "                                  distance_weight)\n",
    "\n",
    "population = Population(genome_factory, population_size, elitism, stop_when_solved, tournament_selection_k, verbose, max_cores, compatibility_threshold, compatibility_threshold_delta, target_species, minimum_elitism_size, young_age, young_multiplier, old_age, old_multiplier, stagnation_age, reset_innovations, survival)\n",
    "task = XORTask()\n",
    "# task = RubiksTask()\n",
    "\n",
    "# cProfile.run('population.epoch(evaluator = task, generations = 1, solution = task)', 'restats')\n",
    "# import pstats\n",
    "# p = pstats.Stats('restats')\n",
    "# p.strip_dirs().sort_stats('cumtime').print_stats()\n",
    "result = population.epoch(evaluator = task, generations = 100, solution = task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.random.rand(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
