{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "- DDQN\n",
    "- Dueling architecture\n",
    "- Test\n",
    "- Code Cleaning\n",
    "- Tqdm (print statements)\n",
    "- Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import rubiks\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward','done'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    \"\"\"\"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        \n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self,batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayMemory(object):\n",
    "    \"\"\"\"\"\"\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameters\n",
    "TODO:\n",
    "    - Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "max_episodes = 3000000\n",
    "batch_size = 32\n",
    "max_epsilon_steps = 90000\n",
    "max_test_episodes = 100000\n",
    "max_tau = 10000\n",
    "replay_memory_size = 32000\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epsilon Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsilon(global_steps, max_epsilon_steps=5000, final_probability=0.05):\n",
    "    \"\"\"Epsilon is linearly decayed over n steps from 1 to the final probability. \"\"\"\n",
    "    if global_steps < max_epsilon_steps:\n",
    "        return 1 - (global_steps/max_epsilon_steps)*(1-final_probability)\n",
    "    else:\n",
    "        return final_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(number_actions, state, network, global_steps, max_epsilon_steps):\n",
    "    \"\"\"With probability epsilon a random action is retured. With probability 1-epsilon the actor network returns the action that maximizes the q-value for the given state.\"\"\"\n",
    "    epsilon = get_epsilon(global_steps, max_epsilon_steps)\n",
    "    if np.random.rand() > epsilon:\n",
    "        with torch.no_grad():\n",
    "            return network(state).max(1)[1].view(1, 1).item(), epsilon\n",
    "    else:\n",
    "        return np.random.randint(number_actions), epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Network that maps states to actions.\"\"\"\n",
    "    def __init__(self, input_size, num_actions=2):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size,64)\n",
    "        self.layer2 = nn.Linear(64,64)\n",
    "        self.layer3 = nn.Linear(64,num_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_q_val(model, state, action):\n",
    "    \"\"\"Computes the q-value for a certain action and state.\"\"\"\n",
    "    qactions = model(state)\n",
    "    return torch.gather(qactions,1,action.view(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_target_dqn(model, reward, next_state, done, gamma):\n",
    "    \"\"\"Computes the target. When done is true 0 is added to the reward.\"\"\"\n",
    "    m = torch.cat(((gamma*torch.max(model(next_state),1)[0]).view(-1,1),torch.zeros(reward.size(), device=device).view(-1,1)),1)\n",
    "    return reward.view(-1,1) + torch.gather(m, 1, done.long().view(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_target_ddqn(model, target_network, reward, next_state, done, gamma):\n",
    "    \"\"\"Computes the target. When done is true 0 is added to the reward.\"\"\"\n",
    "    m = torch.cat(((gamma*torch.gather(target_network(next_state),1,(torch.max(model(next_state),1)[1]).view(-1,1))).view(-1,1),torch.zeros(reward.size(), device=device).view(-1,1)),1)\n",
    "    return reward.view(-1,1) + torch.gather(m, 1, done.long().view(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(q1, target_network, memory, optimizer, batch_size, gamma, training_type):\n",
    "    if len(memory) < batch_size:\n",
    "        return None\n",
    "    \n",
    "    transitions = memory.sample(batch_size)\n",
    "    \n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    state_batch = torch.cat(batch.state).view(batch_size,-1)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    next_state_batch = torch.cat(batch.next_state).view(batch_size,-1)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    done_batch = torch.cat(batch.done)\n",
    "\n",
    "    q_val = compute_q_val(q1, state_batch, action_batch)\n",
    "    \n",
    "    if training_type is 'vanilla':\n",
    "        # Vanilla\n",
    "        with torch.no_grad():\n",
    "            target = compute_target_dqn(q1, reward_batch, next_state_batch, done_batch, gamma)\n",
    "    if training_type is 'target':\n",
    "        # DQN\n",
    "        with torch.no_grad():\n",
    "            target = compute_target_dqn(target_network, reward_batch, next_state_batch, done_batch, gamma)\n",
    "    \n",
    "    if training_type is 'ddqn':\n",
    "        #DDQN\n",
    "        with torch.no_grad():\n",
    "            target = compute_target_ddqn(q1, target_network, reward_batch, next_state_batch, done_batch, gamma)\n",
    "        \n",
    "    loss = F.smooth_l1_loss(q_val, target)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = rubiks.RubiksEnv(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dqn(max_episodes, max_epsilon_steps, max_tau, learning_rate, replay_memory_size, gamma, training_type, seed = None):\n",
    "    #Initialise networks\n",
    "    q1 = QNetwork(6*6*env.size*env.size,env._n_actions).to(device)\n",
    "    target_network = copy.deepcopy(q1)\n",
    "    \n",
    "    optimizer = optim.Adam(q1.parameters(),lr=learning_rate,amsgrad=True)    \n",
    "\n",
    "    env.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    memory = ReplayMemory(replay_memory_size)\n",
    "    \n",
    "    difficulty = 1\n",
    "    max_tries = 1\n",
    "    tries = 0\n",
    "    \n",
    "    times_done = 0\n",
    "    global_steps = 0\n",
    "    tau = 0\n",
    "    \n",
    "    sample_success = np.array([])\n",
    "    \n",
    "    for episode in range(max_episodes):\n",
    "        state = torch.tensor([env.reset(difficulty)], dtype=torch.float, device=device)\n",
    "\n",
    "        done = False\n",
    "        loss_episode = 0.0\n",
    "        tries = 0\n",
    "        while tries < max_tries and not done:\n",
    "            action, epsilon = select_action(env._n_actions,state,q1,global_steps, max_epsilon_steps)\n",
    "\n",
    "            next_state, reward, done, info = env.step(action)       \n",
    "            next_state = torch.tensor([next_state], dtype=torch.float, device=device)\n",
    "            memory.push(state, torch.tensor([action], dtype=torch.int64, device=device), next_state, torch.tensor([reward],dtype=torch.float,device=device), torch.tensor([done],dtype=torch.uint8,device=device))\n",
    "            loss = train_dqn(q1,target_network,memory,optimizer,batch_size,gamma, training_type)\n",
    "            \n",
    "            if loss is not None:\n",
    "                loss_episode += loss\n",
    "\n",
    "            global_steps += 1\n",
    "            tries += 1\n",
    "\n",
    "            state = next_state\n",
    "            \n",
    "        if global_steps > 1000: \n",
    "            sample_success = np.append(sample_success,[float(done)])\n",
    "        \n",
    "        if len(sample_success) > 1000:\n",
    "            sample_success = np.delete(sample_success,0,0)\n",
    "#             if episode % 1000 == 0:\n",
    "#                 print(episode,np.sum(sample_success)/1000, epsilon,loss_episode)\n",
    "            if np.sum(sample_success)/1000 > 0.8:\n",
    "                global_steps = 0\n",
    "                max_tries += 1\n",
    "                difficulty += 1\n",
    "                sample_success = np.array([])\n",
    "                max_epsilon_steps *= 4\n",
    "                print('Level up!', difficulty)\n",
    "                \n",
    "        if tau==max_tau:\n",
    "            tau = 0\n",
    "            target_network = copy.deepcopy(q1)\n",
    "    return np.sum(sample_success)/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_episodes = 500000\n",
    "\n",
    "# max_epsilon_steps_list = [1000,10000,100000]\n",
    "# max_tau_list = [100,1000,10000]\n",
    "# learning_rate_list = [1e-4,5e-4,1e-5]\n",
    "# replay_memory_size_list = [32,1024,32768]\n",
    "# gamma_list = [0.95, 0.925, 0.9, 0.8875]\n",
    "# training_type_list = ['vanilla', 'target', 'ddqn'] \n",
    "\n",
    "# for i in range(100):\n",
    "#     experiment = [max_episodes,random.sample(max_epsilon_steps_list,1)[0],random.sample(max_tau_list,1)[0],random.sample(learning_rate_list,1)[0],random.sample(replay_memory_size_list,1)[0],random.sample(gamma_list,1)[0],random.sample(training_type_list,1)[0],None]\n",
    "#     print('Max_episodes:{}, max_epsilon_steps:{}, max_tau:{}, learning_rate:{}, replay_memory_size:{}, gamma:{}, training_type: {}, seed:{}'.format(*experiment))\n",
    "#     sample_succ = run_dqn(experiment[0], experiment[1], experiment[2], experiment[3], experiment[4], experiment[5], experiment[6])\n",
    "#     print(sample_succ)\n",
    "#     # experiments = [[50000, 100000, 100, 1e-5, 256, 0.95, 45],\n",
    "# #               [50000, 100000, 100, 1e-5, 256, 0.9, 45],\n",
    "# #               [50000, 100000, 100, 1e-5, 256, 0.85, 45],\n",
    "# #               [50000, 100000, 100, 1e-5, 256, 0.8, 45]\n",
    "# #               ]\n",
    "\n",
    "# # for experiment in experiments:\n",
    "# #     print('Max_episodes:{}, max_epsilon_steps:{}, max_tau:{}, learning_rate:{}, replay_memory_size:{}, gamma:{}, seed:{}'.format(*experiment))\n",
    "# #     run_dqn(experiment[0], experiment[1], experiment[2], experiment[3], experiment[4], experiment[5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level up! 2\n",
      "Level up! 3\n",
      "Level up! 4\n",
      "Level up! 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-da3c16c1ebdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample_succ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5e-4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32768\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.925\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'vanilla'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_succ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-8e1f8a720873>\u001b[0m in \u001b[0;36mrun_dqn\u001b[0;34m(max_episodes, max_epsilon_steps, max_tau, learning_rate, replay_memory_size, gamma, training_type, seed)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_network\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-3b6c0feb8378>\u001b[0m in \u001b[0;36mtrain_dqn\u001b[0;34m(q1, target_network, memory, optimizer, batch_size, gamma, training_type)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtransitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtransitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-872f67aef1bd>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/random.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, population, k)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mselected_add\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselected\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                     \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/random.py\u001b[0m in \u001b[0;36m_randbelow\u001b[0;34m(self, n, int, maxsize, type, Method, BuiltinMethod)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     def _randbelow(self, n, int=int, maxsize=1<<BPF, type=type,\n\u001b[0m\u001b[1;32m    224\u001b[0m                    Method=_MethodType, BuiltinMethod=_BuiltinMethodType):\n\u001b[1;32m    225\u001b[0m         \u001b[0;34m\"Return a random int in the range [0,n).  Raises ValueError if n==0.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sample_succ = run_dqn(5000000,10000,1000,5e-4,32768,0.925,'vanilla',42)\n",
    "\n",
    "print(sample_succ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "global_steps = 0\n",
    "q1 = QNetwork(4,2).to(device)\n",
    "target_network = copy.deepcopy(q1)\n",
    "memory = ReplayMemory(replay_memory_size)\n",
    "    \n",
    "optimizer = optim.Adam(q1.parameters(),lr=learning_rate,amsgrad=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0456, -0.0171,  0.0280, -0.0107]], device='cuda:0')\n",
      "tensor([[ 0.0323, -0.0277, -0.0267, -0.0106]], device='cuda:0')\n",
      "tensor([[-0.0049, -0.0455, -0.0173,  0.0446]], device='cuda:0')\n",
      "tensor([[ 0.0378,  0.0136, -0.0474, -0.0261]], device='cuda:0')\n",
      "tensor([[-0.0293, -0.0496, -0.0261, -0.0483]], device='cuda:0')\n",
      "tensor([[-0.0308, -0.0054, -0.0158, -0.0389]], device='cuda:0')\n",
      "tensor([[-0.0079,  0.0408,  0.0228,  0.0409]], device='cuda:0')\n",
      "tensor([[-0.0089, -0.0380,  0.0277,  0.0242]], device='cuda:0')\n",
      "tensor([[0.0331, 0.0428, 0.0265, 0.0062]], device='cuda:0')\n",
      "tensor([[ 0.0244, -0.0445,  0.0154,  0.0244]], device='cuda:0')\n",
      "tensor([[-0.0310,  0.0078,  0.0391,  0.0462]], device='cuda:0')\n",
      "tensor([[ 0.0018, -0.0078,  0.0168, -0.0013]], device='cuda:0')\n",
      "tensor([[0.0419, 0.0160, 0.0416, 0.0454]], device='cuda:0')\n",
      "tensor([[ 0.0251,  0.0175, -0.0178,  0.0236]], device='cuda:0')\n",
      "tensor([[-0.0404,  0.0341, -0.0339, -0.0079]], device='cuda:0')\n",
      "tensor([[ 0.0358, -0.0471, -0.0262,  0.0122]], device='cuda:0')\n",
      "tensor([[ 0.0092, -0.0041, -0.0146,  0.0211]], device='cuda:0')\n",
      "tensor([[-0.0263, -0.0349, -0.0328,  0.0448]], device='cuda:0')\n",
      "tensor([[-0.0338, -0.0167,  0.0456, -0.0204]], device='cuda:0')\n",
      "tensor([[-0.0451, -0.0279, -0.0283, -0.0295]], device='cuda:0')\n",
      "tensor([[-0.0412,  0.0496, -0.0467,  0.0442]], device='cuda:0')\n",
      "tensor([[ 0.0086, -0.0037,  0.0331,  0.0009]], device='cuda:0')\n",
      "tensor([[ 0.0053, -0.0439,  0.0295, -0.0038]], device='cuda:0')\n",
      "tensor([[-0.0204,  0.0454, -0.0248,  0.0346]], device='cuda:0')\n",
      "tensor([[ 0.0397, -0.0237,  0.0416, -0.0488]], device='cuda:0')\n",
      "tensor([[ 0.0077, -0.0071, -0.0391, -0.0472]], device='cuda:0')\n",
      "tensor([[ 0.0212,  0.0092,  0.0365, -0.0336]], device='cuda:0')\n",
      "tensor([[-0.0046, -0.0044, -0.0082,  0.0408]], device='cuda:0')\n",
      "tensor([[-0.0426, -0.0477,  0.0376,  0.0105]], device='cuda:0')\n",
      "tensor([[ 0.0225, -0.0003,  0.0290,  0.0252]], device='cuda:0')\n",
      "tensor([[-0.0421, -0.0084,  0.0107, -0.0063]], device='cuda:0')\n",
      "tensor([[-0.0341, -0.0338, -0.0356, -0.0298]], device='cuda:0')\n",
      "tensor([[ 0.0424, -0.0018, -0.0419, -0.0026]], device='cuda:0')\n",
      "tensor([[ 0.0314, -0.0378,  0.0197,  0.0347]], device='cuda:0')\n",
      "tensor([[0.0479, 0.0433, 0.0070, 0.0175]], device='cuda:0')\n",
      "tensor([[-0.0401, -0.0392, -0.0342,  0.0397]], device='cuda:0')\n",
      "tensor([[-0.0030,  0.0471,  0.0476, -0.0013]], device='cuda:0')\n",
      "tensor([[-0.0259, -0.0465,  0.0277,  0.0141]], device='cuda:0')\n",
      "tensor([[-0.0094,  0.0073, -0.0249, -0.0344]], device='cuda:0')\n",
      "tensor([[-0.0468,  0.0038,  0.0033,  0.0130]], device='cuda:0')\n",
      "tensor([[-0.0202, -0.0201, -0.0129, -0.0338]], device='cuda:0')\n",
      "tensor([[-0.0415, -0.0362, -0.0395, -0.0196]], device='cuda:0')\n",
      "tensor([[-0.0311,  0.0034, -0.0320, -0.0365]], device='cuda:0')\n",
      "tensor([[ 0.0307,  0.0007, -0.0313, -0.0197]], device='cuda:0')\n",
      "tensor([[ 0.0365,  0.0273, -0.0410,  0.0024]], device='cuda:0')\n",
      "tensor([[-0.0450,  0.0372, -0.0197,  0.0212]], device='cuda:0')\n",
      "tensor([[ 0.0420, -0.0164, -0.0349, -0.0331]], device='cuda:0')\n",
      "tensor([[-0.0451,  0.0455,  0.0078, -0.0261]], device='cuda:0')\n",
      "tensor([[0.0203, 0.0027, 0.0418, 0.0212]], device='cuda:0')\n",
      "tensor([[-0.0479, -0.0324,  0.0176,  0.0136]], device='cuda:0')\n",
      "tensor([[-0.0062, -0.0090, -0.0308, -0.0170]], device='cuda:0')\n",
      "tensor([[-0.0241, -0.0164, -0.0117, -0.0101]], device='cuda:0')\n",
      "tensor([[-0.0073,  0.0197,  0.0327, -0.0059]], device='cuda:0')\n",
      "tensor([[ 0.0395,  0.0155, -0.0010,  0.0028]], device='cuda:0')\n",
      "tensor([[-0.0093, -0.0128,  0.0289,  0.0118]], device='cuda:0')\n",
      "tensor([[ 0.0233,  0.0087, -0.0102, -0.0028]], device='cuda:0')\n",
      "tensor([[-0.0126,  0.0489,  0.0413, -0.0432]], device='cuda:0')\n",
      "tensor([[-0.0051, -0.0188,  0.0374,  0.0271]], device='cuda:0')\n",
      "tensor([[ 0.0072, -0.0070, -0.0366,  0.0086]], device='cuda:0')\n",
      "tensor([[-0.0330, -0.0105, -0.0373,  0.0030]], device='cuda:0')\n",
      "tensor([[0.0083, 0.0236, 0.0273, 0.0184]], device='cuda:0')\n",
      "tensor([[-0.0083,  0.0223,  0.0209,  0.0450]], device='cuda:0')\n",
      "tensor([[-0.0047,  0.0390,  0.0134,  0.0067]], device='cuda:0')\n",
      "tensor([[-0.0427, -0.0353,  0.0171, -0.0223]], device='cuda:0')\n",
      "tensor([[-0.0021, -0.0384,  0.0103,  0.0084]], device='cuda:0')\n",
      "tensor([[-0.0138, -0.0019,  0.0170,  0.0224]], device='cuda:0')\n",
      "tensor([[-0.0439, -0.0378, -0.0134, -0.0284]], device='cuda:0')\n",
      "tensor([[-0.0410,  0.0271, -0.0473,  0.0424]], device='cuda:0')\n",
      "tensor([[-0.0366,  0.0071, -0.0049,  0.0189]], device='cuda:0')\n",
      "tensor([[-0.0221, -0.0150,  0.0281, -0.0112]], device='cuda:0')\n",
      "tensor([[-0.0443, -0.0490, -0.0382,  0.0407]], device='cuda:0')\n",
      "tensor([[ 0.0361, -0.0477,  0.0484,  0.0269]], device='cuda:0')\n",
      "tensor([[ 0.0130, -0.0359,  0.0207,  0.0204]], device='cuda:0')\n",
      "tensor([[-0.0450,  0.0211, -0.0287,  0.0193]], device='cuda:0')\n",
      "tensor([[-0.0440, -0.0415, -0.0310, -0.0267]], device='cuda:0')\n",
      "tensor([[ 0.0446, -0.0320, -0.0265,  0.0052]], device='cuda:0')\n",
      "tensor([[ 0.0115, -0.0182, -0.0043, -0.0233]], device='cuda:0')\n",
      "tensor([[ 0.0074,  0.0003, -0.0134,  0.0275]], device='cuda:0')\n",
      "tensor([[ 0.0492, -0.0255, -0.0008,  0.0283]], device='cuda:0')\n",
      "tensor([[-0.0030,  0.0090,  0.0065, -0.0371]], device='cuda:0')\n",
      "tensor([[ 0.0484,  0.0164, -0.0119, -0.0009]], device='cuda:0')\n",
      "tensor([[-0.0417, -0.0231, -0.0174,  0.0096]], device='cuda:0')\n",
      "tensor([[-0.0291,  0.0228, -0.0268, -0.0142]], device='cuda:0')\n",
      "tensor([[-0.0043, -0.0225, -0.0052,  0.0024]], device='cuda:0')\n",
      "tensor([[-0.0473,  0.0270, -0.0261,  0.0107]], device='cuda:0')\n",
      "tensor([[ 0.0449, -0.0233,  0.0281,  0.0165]], device='cuda:0')\n",
      "tensor([[ 0.0036, -0.0351,  0.0097, -0.0079]], device='cuda:0')\n",
      "tensor([[-0.0170, -0.0163, -0.0071, -0.0114]], device='cuda:0')\n",
      "tensor([[ 0.0092,  0.0491,  0.0243, -0.0429]], device='cuda:0')\n",
      "tensor([[-0.0110,  0.0290, -0.0048, -0.0437]], device='cuda:0')\n",
      "tensor([[-3.4604e-05, -4.6193e-02, -1.4558e-02, -1.2626e-02]], device='cuda:0')\n",
      "tensor([[ 0.0166,  0.0139,  0.0212, -0.0189]], device='cuda:0')\n",
      "tensor([[ 0.0008, -0.0268, -0.0444,  0.0138]], device='cuda:0')\n",
      "tensor([[0.0396, 0.0138, 0.0431, 0.0014]], device='cuda:0')\n",
      "tensor([[ 0.0437, -0.0206, -0.0347, -0.0266]], device='cuda:0')\n",
      "tensor([[0.0463, 0.0171, 0.0283, 0.0366]], device='cuda:0')\n",
      "tensor([[-0.0054,  0.0334,  0.0319, -0.0360]], device='cuda:0')\n",
      "tensor([[ 0.0137,  0.0265,  0.0009, -0.0248]], device='cuda:0')\n",
      "tensor([[-0.0019, -0.0066, -0.0432,  0.0267]], device='cuda:0')\n",
      "tensor([[ 0.0303,  0.0423, -0.0223, -0.0482]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "memory = ReplayMemory(replay_memory_size)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    done = False\n",
    "    state = torch.tensor([env.reset()], dtype=torch.float, device=device)\n",
    "    print(state)\n",
    "    while not done:\n",
    "        action, epsilon = select_action(2, state,q1,global_steps, max_epsilon_steps)\n",
    "\n",
    "        next_state, reward, done, info = env.step(action)       \n",
    "        next_state = torch.tensor([next_state], dtype=torch.float, device=device)\n",
    "        memory.push(state, torch.tensor([action], dtype=torch.int64, device=device), next_state, torch.tensor([reward],dtype=torch.float,device=device), torch.tensor([done],dtype=torch.uint8,device=device))\n",
    "        loss = train_dqn(q1,target_network,memory,optimizer,batch_size,gamma, 'vanilla')\n",
    "        \n",
    "        state = next_state\n",
    "        global_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
