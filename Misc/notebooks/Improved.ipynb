{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import rubiks\n",
    "import rubiks2\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os \n",
    "import copy\n",
    "import time\n",
    "\n",
    "from collections import deque\n",
    "from collections import defaultdict\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from data_structures import SegmentTree, MinSegmentTree, SumSegmentTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n",
      "Torch Version:  1.0.1.post2\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)\n",
    "print(\"Torch Version: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    \"\"\"Standard replay buffer\"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "        \n",
    "    def push(self, batch):\n",
    "        self.memory.append((batch))\n",
    "        \n",
    "    def sample(self, batch_size, _):\n",
    "        # Lazy programming. Returns 1, 1 so that I don't have write additional code that discerns between replay memory and prioritized replay memory\n",
    "        return random.sample(self.memory, batch_size), 1, torch.tensor(1, device=device, dtype=torch.float) \n",
    "    \n",
    "    def update_priorities(self, idxes, priorities):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even more laziness. Stole this code from https://github.com/higgsfield/\n",
    "class PrioritizedReplayMemory(object):\n",
    "    def __init__(self, size, alpha=0.7, beta_start=0.5 , beta_frames=10000):\n",
    "        \"\"\"Create Prioritized Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        alpha: float\n",
    "            how much prioritization is used\n",
    "            (0 - no prioritization, 1 - full prioritization)\n",
    "        See Also\n",
    "        --------\n",
    "        ReplayBuffer.__init__\n",
    "        \"\"\"\n",
    "        super(PrioritizedReplayMemory, self).__init__()\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "        self._next_idx = 0\n",
    "\n",
    "        assert alpha >= 0\n",
    "        self._alpha = alpha\n",
    "\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_frames = beta_frames\n",
    "\n",
    "        it_capacity = 1\n",
    "        while it_capacity < size:\n",
    "            it_capacity *= 2\n",
    "\n",
    "        self._it_sum = SumSegmentTree(it_capacity)\n",
    "        self._it_min = MinSegmentTree(it_capacity)\n",
    "        self._max_priority = 1.0\n",
    "\n",
    "    def beta_by_frame(self, frame_idx):\n",
    "        return min(1.0, self.beta_start + frame_idx * (1.0 - self.beta_start) / self.beta_frames)\n",
    "\n",
    "    def push(self, data):\n",
    "        \"\"\"See ReplayBuffer.store_effect\"\"\"\n",
    "        idx = self._next_idx\n",
    "\n",
    "        if self._next_idx >= len(self._storage):\n",
    "            self._storage.append(data)\n",
    "        else:\n",
    "            self._storage[self._next_idx] = data\n",
    "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
    "\n",
    "\n",
    "        self._it_sum[idx] = self._max_priority ** self._alpha\n",
    "        self._it_min[idx] = self._max_priority ** self._alpha\n",
    "\n",
    "    def _encode_sample(self, idxes):\n",
    "        return [self._storage[i] for i in idxes]\n",
    "\n",
    "    def _sample_proportional(self, batch_size):\n",
    "        res = []\n",
    "        for _ in range(batch_size):\n",
    "            # TODO(szymon): should we ensure no repeats?\n",
    "            mass = random.random() * self._it_sum.sum(0, len(self._storage) - 1)\n",
    "            idx = self._it_sum.find_prefixsum_idx(mass)\n",
    "            res.append(idx)\n",
    "        return res\n",
    "\n",
    "    def sample(self, batch_size, global_steps):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "        compared to ReplayBuffer.sample\n",
    "        it also returns importance weights and idxes\n",
    "        of sampled experiences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        beta: float\n",
    "            To what degree to use importance weights\n",
    "            (0 - no corrections, 1 - full correction)\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            batch of observations\n",
    "        act_batch: np.array\n",
    "            batch of actions executed given obs_batch\n",
    "        rew_batch: np.array\n",
    "            rewards received as results of executing act_batch\n",
    "        next_obs_batch: np.array\n",
    "            next set of observations seen after executing act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        weights: np.array\n",
    "            Array of shape (batch_size,) and dtype np.float32\n",
    "            denoting importance weight of each sampled transition\n",
    "        idxes: np.array\n",
    "            Array of shape (batch_size,) and dtype np.int32\n",
    "            idexes in buffer of sampled experiences\n",
    "        \"\"\"\n",
    "\n",
    "        idxes = self._sample_proportional(batch_size)\n",
    "\n",
    "        weights = []\n",
    "\n",
    "        #find smallest sampling prob: p_min = smallest priority^alpha / sum of priorities^alpha\n",
    "        p_min = self._it_min.min() / self._it_sum.sum()\n",
    "\n",
    "        beta = self.beta_by_frame(global_steps)\n",
    "        \n",
    "        #max_weight given to smallest prob\n",
    "        max_weight = (p_min * len(self._storage)) ** (-beta)\n",
    "\n",
    "        for idx in idxes:\n",
    "            p_sample = self._it_sum[idx] / self._it_sum.sum()\n",
    "            weight = (p_sample * len(self._storage)) ** (-beta)\n",
    "            weights.append(weight / max_weight)\n",
    "        weights = torch.tensor(weights, device=device, dtype=torch.float) \n",
    "        encoded_sample = self._encode_sample(idxes)\n",
    "        return encoded_sample, idxes, weights\n",
    "\n",
    "    def update_priorities(self, idxes, priorities):\n",
    "        \"\"\"Update priorities of sampled transitions.\n",
    "        sets priority of transition at index idxes[i] in buffer\n",
    "        to priorities[i].\n",
    "        Parameters\n",
    "        ----------\n",
    "        idxes: [int]\n",
    "            List of idxes of sampled transitions\n",
    "        priorities: [float]\n",
    "            List of updated priorities corresponding to\n",
    "            transitions at the sampled idxes denoted by\n",
    "            variable `idxes`.\n",
    "        \"\"\"\n",
    "        assert len(idxes) == len(priorities)\n",
    "#         print(idxes, priorities)\n",
    "#         print(list(zip(idxes, priorities)))\n",
    "        for idx, priority in zip(idxes, priorities):\n",
    "            assert 0 <= idx < len(self._storage)\n",
    "            self._it_sum[idx] = (priority+1e-5) ** self._alpha\n",
    "\n",
    "            self._it_min[idx] = (priority+1e-5) ** self._alpha\n",
    "\n",
    "            self._max_priority = max(self._max_priority, (priority+1e-5))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self._storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise rubiks cube environment. First argument determines the size of the cube. \n",
    "# The unsolved_reward parameter determines the penalty the reinforcementagent incurs of each move that does not lead to the solved state.\n",
    "# This forces the agent to favour shorter paths. E.g. cube that is scrambled with U can be solved by the sequence U,U,U-> r=-1 or by U'->r=1\n",
    "env = rubiks2.RubiksEnv2(2, unsolved_reward = -1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon decay\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 10000\n",
    "\n",
    "epsilon_by_step = lambda step_idx: epsilon_final + (epsilon_start - epsilon_final) * np.exp(-1. * step_idx / epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc8a8330cf8>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Epsilon decay')\n",
    "plt.xlabel('Global steps')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.plot([epsilon_by_step(i) for i in range(100000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classic DQN. Increase_capacity method adds new nodes to layers according to increment\n",
    "# TODO: decrease capacity does not work as of yet\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_inputs, hidden, num_actions, non_linearity):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.num_inputs = num_inputs\n",
    "        self.hidden = hidden\n",
    "        self.num_actions = num_actions\n",
    "        self.non_linearity = non_linearity\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(num_inputs, self.hidden[0]))\n",
    "        \n",
    "        previous = self.hidden[0]\n",
    "        for hidden_layer_size in self.hidden[1:]:\n",
    "            self.layers.append(nn.Linear(previous, hidden_layer_size))\n",
    "            previous = hidden_layer_size\n",
    "            \n",
    "        self.layers.append(nn.Linear(previous, num_actions))        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            x = self.non_linearity(self.layers[i](x))\n",
    "        return self.layers[-1](x)\n",
    "    \n",
    "    def increase_capacity(self, increment):\n",
    "        for i in range(len(self.hidden)):\n",
    "            self.hidden[i] += increment[i]\n",
    "        \n",
    "        weight = self.layers[0].weight.data\n",
    "        self.layers[0] = nn.Linear(self.num_inputs, self.hidden[0])\n",
    "        if increment[0]>0:\n",
    "            self.layers[0].weight.data[0:-increment[0],:] = weight\n",
    "        else:\n",
    "            self.layers[0].weight.data[0:,:] = weight\n",
    "        \n",
    "        for i in range(1, len(self.layers) - 1):\n",
    "            weight = self.layers[i].weight.data\n",
    "            self.layers[i] = nn.Linear(self.hidden[i-1], self.hidden[i])\n",
    "            if increment[i] > 0:\n",
    "                if increment[i-1] >0: \n",
    "                    self.layers[i].weight.data[0:-increment[i],0:-increment[i-1]] = weight\n",
    "                else:\n",
    "                    self.layers[i].weight.data[0:-increment[i],0:] = weight\n",
    "            else:\n",
    "                if increment[i-1] >0:\n",
    "                    self.layers[i].weight.data[0:,0:-increment[i-1]] = weight\n",
    "                else:\n",
    "                    self.layers[i].weight.data[0:,0:] = weight\n",
    "        \n",
    "        weight = self.layers[-1].weight.data\n",
    "        self.layers[-1] = nn.Linear(self.hidden[-1], self.num_actions)\n",
    "        if increment[-1] >0:\n",
    "            self.layers[-1].weight.data[:,0:-increment[-1]] = weight\n",
    "        else:\n",
    "            self.layers[-1].weight.data[:,0:] = weight\n",
    "        \n",
    "    def decrease_capacity(self, increment):\n",
    "        for i in range(len(self.hidden)):\n",
    "            self.hidden[i] -= increment[i]\n",
    "        \n",
    "        weight = self.layers[0].weight.data\n",
    "        self.layers[0] = nn.Linear(self.num_inputs, self.hidden[0])\n",
    "        self.layers[0].weight.data = weight[0:-increment[0],:]\n",
    "        \n",
    "        for i in range(1, len(self.layers) - 1):\n",
    "            weight = self.layers[i].weight.data\n",
    "            self.layers[i] = nn.Linear(self.hidden[i-1], self.hidden[i])\n",
    "            self.layers[i].weight.data = weight[0:-increment[i],0:-increment[i-1]]\n",
    "        \n",
    "        weight = self.layers[-1].weight.data\n",
    "        self.layers[-1] = nn.Linear(self.hidden[-1], self.num_actions)\n",
    "        self.layers[-1].weight.data = weight[:,0:-increment[-1]]\n",
    "    \n",
    "    def act(self, state, epsilon, mask):\n",
    "        if np.random.rand() > epsilon:\n",
    "            state = torch.tensor([state], dtype=torch.float32, device=device)\n",
    "            mask = torch.tensor([mask], dtype=torch.float32, device=device)\n",
    "            q_values = self.forward(state) + mask\n",
    "            action = q_values.max(1)[1].view(1, 1).item()\n",
    "        else:\n",
    "            action =  np.random.randint(self.num_actions)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, num_inputs, hidden, num_actions, non_linearity):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        \n",
    "        self.num_inputs = num_inputs\n",
    "        self.hidden = hidden\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.non_linearity = non_linearity\n",
    "        \n",
    "        self.sharedLayers = nn.ModuleList()\n",
    "        self.sharedLayers.append(nn.Linear(num_inputs, self.hidden[0]))\n",
    "        \n",
    "        previous = self.hidden[0]\n",
    "        for hidden_layer_size in self.hidden[1:-1]:\n",
    "            self.sharedLayers.append(nn.Linear(previous, hidden_layer_size))\n",
    "            previous = hidden_layer_size\n",
    "\n",
    "        self.adv1 = nn.Linear(previous, self.hidden[-1])\n",
    "        self.adv2 = nn.Linear(self.hidden[-1], num_actions)\n",
    "        \n",
    "        self.v1 = nn.Linear(previous, self.hidden[-1])\n",
    "        self.v2 = nn.Linear(self.hidden[-1], 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.sharedLayers)):\n",
    "            \n",
    "            x = self.non_linearity(self.sharedLayers[i](x))\n",
    "\n",
    "        a = self.non_linearity(self.adv1(x))\n",
    "        a = self.adv2(a)\n",
    "        \n",
    "        v = self.non_linearity(self.v1(x))\n",
    "        v = self.v2(v)\n",
    "        \n",
    "        return v + a - a.mean()\n",
    "    \n",
    "    def increase_capacity(self, increment):\n",
    "        for i in range(len(self.hidden)):\n",
    "            self.hidden[i] += increment[i]\n",
    "        \n",
    "        # Check whether the increment isn't zero\n",
    "        if increment[0] > 0:\n",
    "            weight = self.sharedLayers[0].weight.data\n",
    "            self.sharedLayers[0] = nn.Linear(self.num_inputs, self.hidden[0])\n",
    "            self.sharedLayers[0].weight.data[0:-increment[0],:] = weight\n",
    "\n",
    "        for i in range(1, len(self.sharedLayers)):\n",
    "            weight = self.sharedLayers[i].weight.data\n",
    "            self.sharedLayers[i] = nn.Linear(self.hidden[i-1], self.hidden[i])\n",
    "            if increment[i] > 0:\n",
    "                if increment[i-1] > 0:\n",
    "                    self.sharedLayers[i].weight.data[0:-increment[i],0:-increment[i-1]] = weight\n",
    "                else:\n",
    "                    self.sharedLayers[i].weight.data[0:-increment[i],0:] = weight\n",
    "            else:\n",
    "                if increment[i-1] > 0:\n",
    "                        self.sharedLayers[i].weight.data[0:,0:-increment[i-1]] = weight\n",
    "                else:\n",
    "                    self.sharedLayers[i].weight.data[0:,0:] = weight\n",
    "            \n",
    "        weight_adv1 = self.adv1.weight.data\n",
    "        self.adv1 = nn.Linear(self.hidden[-2], self.hidden[-1])\n",
    "        \n",
    "        weight_v1 = self.v1.weight.data\n",
    "        self.v1 = nn.Linear(self.hidden[-2], self.hidden[-1])\n",
    "        if increment[-1] > 0:\n",
    "            if increment[-2] > 0:\n",
    "                self.adv1.weight.data[0:-increment[-1],0:-increment[-2]] = weight_adv1\n",
    "                self.v1.weight.data[0:-increment[-1],0:-increment[-2]] = weight_v1\n",
    "            else:\n",
    "                self.adv1.weight.data[0:-increment[-1],0:] = weight_adv1\n",
    "                self.v1.weight.data[0:-increment[-1],0:] = weight_v1\n",
    "        else:\n",
    "            if increment[-2] > 0:\n",
    "                self.adv1.weight.data[0:,0:-increment[-2]] = weight_adv1\n",
    "                self.v1.weight.data[0:,0:-increment[-2]] = weight_v1\n",
    "            else:\n",
    "                self.adv1.weight.data[0:,0:] = weight_adv1\n",
    "                self.v1.weight.data[0:,0:] = weight_v1\n",
    "            \n",
    "        weight_adv2 = self.adv2.weight.data\n",
    "        self.adv2 = nn.Linear(self.hidden[-1], self.num_actions)\n",
    "        \n",
    "        weight_v2 = self.v2.weight.data\n",
    "        self.v2 = nn.Linear(self.hidden[-1], 1)\n",
    "        \n",
    "        if increment[-1] > 0:\n",
    "            self.adv2.weight.data[:,0:-increment[-1]] = weight_adv2\n",
    "            self.v2.weight.data[:,0:-increment[-1]] = weight_v2\n",
    "        else:\n",
    "            self.adv2.weight.data[:,0:] = weight_adv2\n",
    "            self.v2.weight.data[:,0:] = weight_v2\n",
    "        \n",
    "    def decrease_capacity(self, increment):\n",
    "        for i in range(len(self.hidden)):\n",
    "            self.hidden[i] -= increment[i]\n",
    "\n",
    "        weight = self.sharedLayers[0].weight.data\n",
    "        self.sharedLayers[0] = nn.Linear(self.num_inputs, self.hidden[0])\n",
    "        self.sharedLayers[0].weight.data = weight[0:-increment[0],:]\n",
    "        \n",
    "        for i in range(1, len(self.sharedLayers)):\n",
    "            weight = self.sharedLayers[i].weight.data\n",
    "            self.sharedLayers[i] = nn.Linear(self.hidden[i-1], self.hidden[i])\n",
    "            self.sharedLayers[i].weight.data = weight[0:-increment[i],0:-increment[i-1]]\n",
    "            \n",
    "        weight = self.adv1.weight.data\n",
    "        self.adv1 = nn.Linear(self.hidden[-2], self.hidden[-1])\n",
    "        self.adv1.weight.data = weight[0:-increment[-1],0:-increment[-2]]\n",
    "            \n",
    "        weight = self.adv2.weight.data\n",
    "        self.adv2 = nn.Linear(self.hidden[-1], self.num_actions)\n",
    "        self.adv2.weight.data = weight[:,0:-increment[-1]]\n",
    "        \n",
    "        weight = self.v1.weight.data\n",
    "        self.v1 = nn.Linear(self.hidden[-2], self.hidden[-1])\n",
    "        self.v1.weight.data = weight[0:-increment[-1],0:-increment[-2]]\n",
    "            \n",
    "        weight = self.v2.weight.data\n",
    "        self.v2 = nn.Linear(self.hidden[-1], 1)\n",
    "        self.v2.weight.data = weight[:,0:-increment[-1]]\n",
    "        \n",
    "    def act(self, state, epsilon, mask):\n",
    "        if np.random.rand() > epsilon:\n",
    "            state = torch.tensor([state], dtype=torch.float32, device=device)\n",
    "            mask = torch.tensor([mask], dtype=torch.float32, device=device)\n",
    "            q_values = self.forward(state) + mask\n",
    "            action = q_values.max(1)[1].view(1, 1).item()\n",
    "        else:\n",
    "            action =  np.random.randint(self.num_actions)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQNHER(nn.Module):\n",
    "    def __init__(self, num_inputs, hidden, num_actions, non_linearity):\n",
    "        super(DuelingDQNHER, self).__init__()\n",
    "        \n",
    "        self.num_inputs = num_inputs * 2\n",
    "        self.hidden = hidden\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.non_linearity = non_linearity\n",
    "        \n",
    "        self.sharedLayers = nn.ModuleList()\n",
    "        self.sharedLayers.append(nn.Linear(num_inputs, self.hidden[0]))\n",
    "        \n",
    "        previous = self.hidden[0]\n",
    "        for hidden_layer_size in self.hidden[1:-1]:\n",
    "            self.sharedLayers.append(nn.Linear(previous, hidden_layer_size))\n",
    "            previous = hidden_layer_size\n",
    "\n",
    "        self.adv1 = nn.Linear(previous, self.hidden[-1])\n",
    "        self.adv2 = nn.Linear(self.hidden[-1], num_actions)\n",
    "        \n",
    "        self.v1 = nn.Linear(previous, self.hidden[-1])\n",
    "        self.v2 = nn.Linear(self.hidden[-1], 1)\n",
    "        \n",
    "    def forward(self, state, ):\n",
    "        for i in range(len(self.sharedLayers)):\n",
    "            \n",
    "            x = self.non_linearity(self.sharedLayers[i](x))\n",
    "\n",
    "        a = self.non_linearity(self.adv1(x))\n",
    "        a = self.adv2(a)\n",
    "        \n",
    "        v = self.non_linearity(self.v1(x))\n",
    "        v = self.v2(v)\n",
    "        \n",
    "        return v + a - a.mean()\n",
    "    \n",
    "    def increase_capacity(self, increment):\n",
    "        for i in range(len(self.hidden)):\n",
    "            self.hidden[i] += increment[i]\n",
    "        \n",
    "        # Check whether the increment isn't zero\n",
    "        if increment[0] > 0:\n",
    "            weight = self.sharedLayers[0].weight.data\n",
    "            self.sharedLayers[0] = nn.Linear(self.num_inputs, self.hidden[0])\n",
    "            self.sharedLayers[0].weight.data[0:-increment[0],:] = weight\n",
    "\n",
    "        for i in range(1, len(self.sharedLayers)):\n",
    "            weight = self.sharedLayers[i].weight.data\n",
    "            self.sharedLayers[i] = nn.Linear(self.hidden[i-1], self.hidden[i])\n",
    "            if increment[i] > 0:\n",
    "                if increment[i-1] > 0:\n",
    "                    self.sharedLayers[i].weight.data[0:-increment[i],0:-increment[i-1]] = weight\n",
    "                else:\n",
    "                    self.sharedLayers[i].weight.data[0:-increment[i],0:] = weight\n",
    "            else:\n",
    "                if increment[i-1] > 0:\n",
    "                        self.sharedLayers[i].weight.data[0:,0:-increment[i-1]] = weight\n",
    "                else:\n",
    "                    self.sharedLayers[i].weight.data[0:,0:] = weight\n",
    "            \n",
    "        weight_adv1 = self.adv1.weight.data\n",
    "        self.adv1 = nn.Linear(self.hidden[-2], self.hidden[-1])\n",
    "        \n",
    "        weight_v1 = self.v1.weight.data\n",
    "        self.v1 = nn.Linear(self.hidden[-2], self.hidden[-1])\n",
    "        if increment[-1] > 0:\n",
    "            if increment[-2] > 0:\n",
    "                self.adv1.weight.data[0:-increment[-1],0:-increment[-2]] = weight_adv1\n",
    "                self.v1.weight.data[0:-increment[-1],0:-increment[-2]] = weight_v1\n",
    "            else:\n",
    "                self.adv1.weight.data[0:-increment[-1],0:] = weight_adv1\n",
    "                self.v1.weight.data[0:-increment[-1],0:] = weight_v1\n",
    "        else:\n",
    "            if increment[-2] > 0:\n",
    "                self.adv1.weight.data[0:,0:-increment[-2]] = weight_adv1\n",
    "                self.v1.weight.data[0:,0:-increment[-2]] = weight_v1\n",
    "            else:\n",
    "                self.adv1.weight.data[0:,0:] = weight_adv1\n",
    "                self.v1.weight.data[0:,0:] = weight_v1\n",
    "            \n",
    "        weight_adv2 = self.adv2.weight.data\n",
    "        self.adv2 = nn.Linear(self.hidden[-1], self.num_actions)\n",
    "        \n",
    "        weight_v2 = self.v2.weight.data\n",
    "        self.v2 = nn.Linear(self.hidden[-1], 1)\n",
    "        \n",
    "        if increment[-1] > 0:\n",
    "            self.adv2.weight.data[:,0:-increment[-1]] = weight_adv2\n",
    "            self.v2.weight.data[:,0:-increment[-1]] = weight_v2\n",
    "        else:\n",
    "            self.adv2.weight.data[:,0:] = weight_adv2\n",
    "            self.v2.weight.data[:,0:] = weight_v2\n",
    "        \n",
    "    def decrease_capacity(self, increment):\n",
    "        for i in range(len(self.hidden)):\n",
    "            self.hidden[i] -= increment[i]\n",
    "\n",
    "        weight = self.sharedLayers[0].weight.data\n",
    "        self.sharedLayers[0] = nn.Linear(self.num_inputs, self.hidden[0])\n",
    "        self.sharedLayers[0].weight.data = weight[0:-increment[0],:]\n",
    "        \n",
    "        for i in range(1, len(self.sharedLayers)):\n",
    "            weight = self.sharedLayers[i].weight.data\n",
    "            self.sharedLayers[i] = nn.Linear(self.hidden[i-1], self.hidden[i])\n",
    "            self.sharedLayers[i].weight.data = weight[0:-increment[i],0:-increment[i-1]]\n",
    "            \n",
    "        weight = self.adv1.weight.data\n",
    "        self.adv1 = nn.Linear(self.hidden[-2], self.hidden[-1])\n",
    "        self.adv1.weight.data = weight[0:-increment[-1],0:-increment[-2]]\n",
    "            \n",
    "        weight = self.adv2.weight.data\n",
    "        self.adv2 = nn.Linear(self.hidden[-1], self.num_actions)\n",
    "        self.adv2.weight.data = weight[:,0:-increment[-1]]\n",
    "        \n",
    "        weight = self.v1.weight.data\n",
    "        self.v1 = nn.Linear(self.hidden[-2], self.hidden[-1])\n",
    "        self.v1.weight.data = weight[0:-increment[-1],0:-increment[-2]]\n",
    "            \n",
    "        weight = self.v2.weight.data\n",
    "        self.v2 = nn.Linear(self.hidden[-1], 1)\n",
    "        self.v2.weight.data = weight[:,0:-increment[-1]]\n",
    "        \n",
    "    def act(self, state, epsilon, mask):\n",
    "        if np.random.rand() > epsilon:\n",
    "            state = torch.tensor([state], dtype=torch.float32, device=device)\n",
    "            mask = torch.tensor([mask], dtype=torch.float32, device=device)\n",
    "            q_values = self.forward(state) + mask\n",
    "            action = q_values.max(1)[1].view(1, 1).item()\n",
    "        else:\n",
    "            action =  np.random.randint(self.num_actions)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the q-values of an action in a state\n",
    "def compute_q_val(model, state, action):\n",
    "    qactions = model(state)\n",
    "    return torch.gather(qactions,1,action.view(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6007],\n",
       "        [ 0.0726]], grad_fn=<GatherBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DuelingDQN(2,[2,3,4],3,F.relu)\n",
    "state = torch.tensor([[0.5,0.6],[0.1,0.2]])\n",
    "action = torch.tensor([[0],[1]])\n",
    "compute_q_val(model, state, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the target. When done, 0 is added to the reward as there is no next state.\n",
    "def compute_target_dqn(model, reward, next_state, done, gamma):\n",
    "    return reward + gamma * model(next_state).max(1)[0] * (1-done)\n",
    "\n",
    "# Computes the target. When done, 0 is added to the reward as there is no next state. But now for Double DQN\n",
    "def compute_target_ddqn(model, target_model, reward, next_state, done, gamma):\n",
    "    a = model(next_state)\n",
    "    return reward.view(-1,1) + gamma * torch.gather(target_model(next_state),1,model(next_state).max(1)[1].view(-1,1)) * (1-done).view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(q1, target_network, memory, optimizer, batch_size, gamma, local_steps, doubleDQN):\n",
    "    if len(memory) < batch_size:\n",
    "        return None\n",
    "    \n",
    "    batch, indices, weights = memory.sample(batch_size, local_steps)\n",
    "\n",
    "    state, action, reward, next_state, done = zip(*batch)\n",
    "    \n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "    action = torch.tensor(action, dtype=torch.long, device=device)\n",
    "    next_state = torch.tensor(next_state, dtype=torch.float32, device=device)\n",
    "    reward = torch.tensor(reward, dtype=torch.float32, device=device)\n",
    "    done = torch.tensor(done, dtype=torch.float32, device=device)\n",
    "    \n",
    "    weights.to(device)\n",
    "\n",
    "    q_val = compute_q_val(q1, state, action)\n",
    "\n",
    "    with torch.no_grad():\n",
    "# Vanilla\n",
    "#         target = compute_target_dqn(q1, reward, next_state, done, gamma)\n",
    "        if doubleDQN:\n",
    "            target = compute_target_ddqn(q1, target_network, reward, next_state, done, gamma)\n",
    "        else:\n",
    "            target = compute_target_dqn(target_network, reward, next_state, done, gamma)\n",
    "#     loss = F.mse_loss(q_val, target)\n",
    "    difference = (q_val - target.view(-1,1))\n",
    "    \n",
    "    # Weights is 1 for normal replay buffer so nothing changes\n",
    "    # McAleer divides the loss by the number of moves of the scramble here. Might not make sense in non-MCTS setting\n",
    "    loss = difference.pow(2) * weights\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Also taken from higgsfield\n",
    "    memory.update_priorities(indices, difference.detach().squeeze().abs().cpu().numpy().tolist())\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_settings(architecture, duelingDQN, doubleDQN, prioritizedReplayMemory, alpha, \n",
    "                        memoryCapacity, lr, amsgrad, epochs, batch_size, gamma, capacity_increase, \n",
    "                        threshold, evaluation_frequency, tau, curriculum, non_linearity,\n",
    "                        verbose=False, load_path=None, save_path=None, seed=None):\n",
    "    \n",
    "    # If the directory does not exist, make one\n",
    "    if save_path:\n",
    "        if not os.path.isdir(save_path):\n",
    "            os.mkdir(save_path)\n",
    "    \n",
    "    # If a seed is set, set the seed for all sources of randomness\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "            \n",
    "    # Difficulty the problem starts with\n",
    "    difficulty = 0\n",
    "    # The maximum number of tries the agent gets at the start\n",
    "    max_tries_start = 1\n",
    "    max_tries = max_tries_start\n",
    "    # 3 is chosen because this allows the network to learn the difference between short and long paths from the beginning. Take for example a cube that has been scrambled as follows: U. The solution within 3 steps is eather U' (r=1) or U, U, U (r=-1)\n",
    "    \n",
    "    # Arrays to keep track of losses and accuracies over time\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    # Global steps keeps track of the total number of optimisation steps\n",
    "    global_steps = 0\n",
    "    # Local steps keeps track of number of optimisation steps within a level\n",
    "    local_steps = 0\n",
    "    # Total time keeps track of how long the training process takes\n",
    "    \n",
    "    # Epsilon exponential decay\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_final = 0.01\n",
    "    # Duration of decay dependent on difficulty\n",
    "#     epsilon_decay = 10000*(difficulty//12 + 1)\n",
    "    epsilon_decay = 10000\n",
    "    # Duration independent on dificulty\n",
    "    epsilon_by_step = lambda step_idx: epsilon_final + (epsilon_start - epsilon_final) * np.exp(-1. * step_idx / epsilon_decay)\n",
    "    \n",
    "    # If a path with a model is provided certain variables are loaded\n",
    "    if load_path:\n",
    "        current_model = torch.load(load_path + 'model.pt')\n",
    "        target_model = copy.deepcopy(current_model)\n",
    "        max_tries = torch.load(load_path + 'max_tries')\n",
    "        epsilon_decay = torch.load(load_path + 'epsilon_decay')\n",
    "        global_steps = torch.load(load_path + 'global_steps')\n",
    "        local_steps = torch.load(load_path + 'local_steps')\n",
    "        difficulty = torch.load(load_path + 'difficulty')\n",
    "    else:\n",
    "        # n^2 per face, 6 faces, 6 colours one-hot encoded. 3x3x6x6=324, 2x2x6x6=144\n",
    "        state_size = env.size**2 * 6**2\n",
    "        \n",
    "        # The number of output nodes is different for the 2x2x2. The move L is the same as the move R, except the orientation changes.\n",
    "        output_nodes = 12\n",
    "        if env.size == 2:\n",
    "            output_nodes = 6\n",
    "        \n",
    "        # Initialising either a network with dueling architecture or regular network\n",
    "        if duelingDQN:\n",
    "            current_model = DuelingDQN(state_size, architecture, output_nodes, non_linearity)\n",
    "            target_model = DuelingDQN(state_size, copy.copy(architecture), output_nodes, non_linearity)\n",
    "        else:\n",
    "            current_model = DQN(state_size, architecture, output_nodes, non_linearity)\n",
    "            target_model = DQN(state_size, copy.copy(architecture), output_nodes, non_linearity)\n",
    "            \n",
    "    if torch.cuda.is_available():\n",
    "        current_model.to('cuda')\n",
    "        target_model.to('cuda')\n",
    "    else:\n",
    "        current_model.to('cpu')\n",
    "        target_model.to('cpu')\n",
    "    \n",
    "    # Uses prioritized replay sampling when set to true, otherwise uniform replay sampling is used\n",
    "    if prioritizedReplayMemory:\n",
    "        memory = PrioritizedReplayMemory(memoryCapacity, alpha)\n",
    "    else:\n",
    "        memory = ReplayMemory(memoryCapacity)\n",
    "    \n",
    "    # Initialise optimiser\n",
    "    optimizer = optim.Adam(current_model.parameters(), lr=lr, amsgrad=amsgrad)\n",
    "    \n",
    "    # This allows you to stop the training progress\n",
    "    try:\n",
    "        for epoch in range(epochs):\n",
    "            epoch_losses = []\n",
    "            # Different types of curricula decide what state to show the network next.\n",
    "            if curriculum is 'Naive':\n",
    "                state = env.curriculum_reset(difficulty)\n",
    "            if curriculum is 'Joe':\n",
    "                p = np.random.rand()\n",
    "                if p < 0.2:\n",
    "                    state = env.curriculum_reset(np.random.randint(difficulty + 1, difficulty + 12))\n",
    "                else:\n",
    "                    state = env.curriculum_reset(difficulty)\n",
    "            if curriculum is 'Sutskever':\n",
    "                p = np.random.rand()\n",
    "                if p < 0.2:\n",
    "                    state = env.reset(np.random.randint(difficulty + 1,1000))\n",
    "                else:\n",
    "                    state = env.curriculum_reset(difficulty)\n",
    "            if curriculum is 'Mixed':\n",
    "                state = env.curriculum_reset(np.random.randint(difficulty + 1,1000))\n",
    "\n",
    "            done = 0\n",
    "            tries = 0\n",
    "            while tries < max_tries and not done:\n",
    "                \n",
    "                epsilon = epsilon_by_step(local_steps)\n",
    "                action = current_model.act(state, epsilon, [0]*env.action_space.n)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                memory.push((state, action, reward, next_state, done))\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                tries += 1\n",
    "            \n",
    "            loss = train_dqn(current_model, target_model, memory, optimizer, batch_size, gamma, local_steps, doubleDQN)\n",
    "\n",
    "            if loss:\n",
    "                epoch_losses.append(loss)\n",
    "                \n",
    "            global_steps += 1\n",
    "            local_steps += 1\n",
    "                \n",
    "            if global_steps % tau == 0:\n",
    "                target_model.load_state_dict(current_model.state_dict())\n",
    "                if save_path:\n",
    "                    torch.save(current_model, save_path + \"model.pt\")\n",
    "                    torch.save(max_tries, save_path + \"max_tries\")\n",
    "                    torch.save(epsilon_decay, save_path + \"epsilon_decay\")\n",
    "                    torch.save(global_steps, save_path + \"global_steps\")\n",
    "                    torch.save(local_steps, save_path + \"local_steps\")\n",
    "                    torch.save(local_steps, save_path + \"difficulty\")\n",
    "                    \n",
    "            if global_steps % evaluation_frequency == 0:\n",
    "                total_done = 0\n",
    "                for i in range(difficulty + 1):\n",
    "                    # Here the agent is forced to evaluate its ability to solve certain last moves\n",
    "                    hashes = defaultdict(list)\n",
    "                    state = env.force_last_action_reset(i)\n",
    "                    hashes[hash(state.tostring())] = [0]*env.action_space.n\n",
    "                    done = 0\n",
    "                    tries = 0\n",
    "                    while tries < max_tries and not done:\n",
    "                        mask = hashes[hash(state.tostring())]\n",
    "                        action = current_model.act(state, 0, mask)\n",
    "                        next_state, reward, done, info = env.step(action)\n",
    "                        memory.push((state, action, reward, next_state, done))\n",
    "                        hstate = state.copy()\n",
    "                        state = next_state\n",
    "                        \n",
    "                        h = hash(state.tostring())\n",
    "                        if h in hashes.keys():\n",
    "                            hashes[hash(hstate.tostring())][action] = -999\n",
    "                        else:\n",
    "                            hashes[h] = [0]*env.action_space.n\n",
    "                        \n",
    "                        total_done += done\n",
    "                        tries += 1\n",
    "                        \n",
    "                # Here the agent evaluates the ability to solve puzzles that have the last move from a set of moves with the same difficulty\n",
    "                for i in range(1000):\n",
    "                    hashes = defaultdict(list)\n",
    "                    state = env.curriculum_reset(difficulty)\n",
    "                    hashes[hash(state.tostring())] = [0]*env.action_space.n\n",
    "                    done = 0\n",
    "                    tries = 0\n",
    "                    while tries < max_tries and not done:\n",
    "                        mask = hashes[hash(state.tostring())]\n",
    "                        action = current_model.act(state, 0, mask)\n",
    "                        next_state, reward, done, info = env.step(action)\n",
    "                        memory.push((state, action, reward, next_state, done))\n",
    "                        hstate = state.copy()\n",
    "                        state = next_state\n",
    "                        \n",
    "                        h = hash(state.tostring())\n",
    "                        if h in hashes.keys():\n",
    "                            hashes[hash(hstate.tostring())][action] = -999\n",
    "                        else:\n",
    "                            hashes[h] = [0]*env.action_space.n\n",
    "                            \n",
    "                        \n",
    "                        total_done += done\n",
    "                        tries += 1\n",
    "                \n",
    "                accuracy = total_done/(1000 + difficulty + 1)\n",
    "                accuracies.append(accuracy)\n",
    "                \n",
    "                if accuracy >= threshold or epsilon<0.0105:\n",
    "                    difficulty +=1\n",
    "                    max_tries = difficulty // output_nodes + max_tries_start\n",
    "                    local_steps = 0\n",
    "                    \n",
    "                    epsilon_start = 1.0\n",
    "                    epsilon_final = 0.01\n",
    "#                     epsilon_decay = 10000*(difficulty//output_nodes + 1)\n",
    "                    epsilon_decay = 10000\n",
    "\n",
    "                    epsilon_by_step = lambda step_idx: epsilon_final + (epsilon_start - epsilon_final) * np.exp(-1. * step_idx / epsilon_decay)\n",
    "                    \n",
    "                    if capacity_increase:\n",
    "                        cap = [c(difficulty) for c in capacity_increase]\n",
    "                        current_model.increase_capacity(cap)\n",
    "                        target_model.increase_capacity(cap)\n",
    "                        current_model.to(device)\n",
    "                        target_model.to(device)\n",
    "                        optimizer = optim.Adam(current_model.parameters(), lr=lr, amsgrad=amsgrad)\n",
    "\n",
    "                if verbose:\n",
    "                    clear_output(True)\n",
    "                    print(\"Epoch: \", epoch, \"Global steps: \", global_steps)\n",
    "                    print(\"Difficulty: \", difficulty, \"Max tries:\", max_tries)\n",
    "                    print(\"Memory: \", len(memory), \"epsilon: \", epsilon)\n",
    "                    print(\"Accuracy: \", accuracy)\n",
    "                    print(current_model)\n",
    "                    \n",
    "                losses.append(np.average(epoch_losses))\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    if save_path:\n",
    "        torch.save(current_model, save_path + \"model.pt\")\n",
    "        torch.save(max_tries, save_path + \"max_tries\")\n",
    "        torch.save(epsilon_decay, save_path + \"epsilon_decay\")\n",
    "        torch.save(global_steps, save_path + \"global_steps\")\n",
    "        torch.save(local_steps, save_path + \"local_steps\")\n",
    "        torch.save(local_steps, save_path + \"difficulty\")\n",
    "            \n",
    "    return difficulty, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  26799 Global steps:  26800\n",
      "Difficulty:  61 Max tries: 6\n",
      "Memory:  100000 epsilon:  0.9612766676713423\n",
      "Accuracy:  0.9048964218455744\n",
      "DuelingDQN(\n",
      "  (sharedLayers): ModuleList(\n",
      "    (0): Linear(in_features=324, out_features=4096, bias=True)\n",
      "    (1): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "  )\n",
      "  (adv1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (adv2): Linear(in_features=512, out_features=12, bias=True)\n",
      "  (v1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (v2): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "epochs = 500000\n",
    "experiments = []\n",
    "# experiments.append([\"Baseline3\", [4096, 2048, 512], False, False, False, 0.7, 100000, 1e-3, False, epochs, 128, 0.99, None, 0.95, 100, 1000, 'Naive', F.relu])\n",
    "# experiments.append([\"Mixed3\", [4096, 2048, 512], False, False, False, 0.7, 100000, 1e-3, True, epochs, 128, 0.99, None, 0.95, 100, 1000, 'Mixed', F.relu])\n",
    "# experiments.append([\"Joe3\", [4096, 2048, 512], False, False, False, 0.7, 100000, 1e-3, True, epochs, 128, 0.99, None, 0.95, 100, 1000, 'Joe', F.relu])\n",
    "# experiments.append([\"Sutskever3\", [4096, 2048, 512], False, False, False, 0.7, 100000, 1e-3, True, epochs, 128, 0.99, None, 0.95, 100, 1000, 'Sutskever', F.relu])\n",
    "# experiments.append([\"Prioritised0.53\", [4096, 2048, 512], False, False, True, 0.5, 100000, 1e-3, True, epochs, 128, 0.99, None, 0.95, 100, 1000, 'Sutskever', F.relu])\n",
    "# experiments.append([\"Prioritised0.73\", [4096, 2048, 512], False, False, True, 0.7, 100000, 1e-3, True, epochs, 128, 0.99, None, 0.95, 100, 1000, 'Sutskever', F.relu])\n",
    "# experiments.append([\"Prioritised0.93\", [4096, 2048, 512], False, False, True, 0.9, 100000, 1e-3, True, epochs, 128, 0.99, None, 0.95, 100, 1000, 'Sutskever', F.relu])\n",
    "# experiments.append([\"Dueling3\", [4096, 2048, 512], True, False, True, 0.9, 100000, 1e-3, True, epochs, 128, 0.99, None, 0.95, 100, 1000, 'Sutskever', F.relu])\n",
    "# experiments.append([\"Double3\", [4096, 2048, 512], True, True, True, 0.9, 100000, 1e-3, True, epochs, 128, 0.99, None, 0.95, 100, 1000, 'Sutskever', F.relu])\n",
    "# experiments.append([\"Linear_all3\", [64, 32, 8], True, True, True, 0.9, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: 48, lambda x: 24, lambda x: 6], 0.95, 100, 1000, 'Sutskever', F.relu])\n",
    "# experiments.append([\"Elu3\", [4096, 2048, 512], True, True, True, 0.9, 100000, 1e-3, True, epochs, 128, 0.99, None, 0.95, 100, 1000, 'Sutskever', F.elu])\n",
    "experiments.append([\"Leaky_relu3\", [4096, 2048, 512], True, True, True, 0.9, 100000, 1e-3, True, epochs, 128, 0.99, None, 0.95, 100, 1000, 'Sutskever', F.leaky_relu])\n",
    "\n",
    "\n",
    "for experiment in experiments:\n",
    "    architecture, duelingDQN, doubleDQN, prioritizedReplayMemory, alpha, memoryCapacity, lr, amsgrad, epochs, batch_size, gamma, capacity_increase, threshold, evaluation_frequency, tau, curriculum, non_linearity = experiment[1:]\n",
    "\n",
    "    env = rubiks.RubiksEnv(3, unsolved_reward = -1.0, seed= 42)\n",
    "\n",
    "    train_with_settings(architecture, duelingDQN, doubleDQN, prioritizedReplayMemory, alpha, memoryCapacity, lr, amsgrad, epochs, batch_size, gamma, capacity_increase, threshold, evaluation_frequency, tau, curriculum, non_linearity, save_path='models/'+experiment[0]+'/', seed=42, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQNHER(nn.Module):\n",
    "    def __init__(self, num_inputs, hidden, num_actions, non_linearity):\n",
    "        super(DuelingDQNHER, self).__init__()\n",
    "        \n",
    "        self.num_inputs = num_inputs * 2\n",
    "        self.hidden = hidden\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.non_linearity = non_linearity\n",
    "        \n",
    "        self.sharedLayers = nn.ModuleList()\n",
    "        self.sharedLayers.append(nn.Linear(self.num_inputs, self.hidden[0]))\n",
    "        \n",
    "        previous = self.hidden[0]\n",
    "        for hidden_layer_size in self.hidden[1:-1]:\n",
    "            self.sharedLayers.append(nn.Linear(previous, hidden_layer_size))\n",
    "            previous = hidden_layer_size\n",
    "\n",
    "        self.adv1 = nn.Linear(previous, self.hidden[-1])\n",
    "        self.adv2 = nn.Linear(self.hidden[-1], self.num_actions)\n",
    "        \n",
    "        self.v1 = nn.Linear(previous, self.hidden[-1])\n",
    "        self.v2 = nn.Linear(self.hidden[-1], 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.sharedLayers)):\n",
    "            \n",
    "            x = self.non_linearity(self.sharedLayers[i](x))\n",
    "\n",
    "        a = self.non_linearity(self.adv1(x))\n",
    "        a = self.adv2(a)\n",
    "        \n",
    "        v = self.non_linearity(self.v1(x))\n",
    "        v = self.v2(v)\n",
    "        \n",
    "        return v + a - a.mean()\n",
    "    \n",
    "    def increase_capacity(self, increment):\n",
    "        for i in range(len(self.hidden)):\n",
    "            self.hidden[i] += increment[i]\n",
    "        \n",
    "        # Check whether the increment isn't zero\n",
    "        if increment[0] > 0:\n",
    "            weight = self.sharedLayers[0].weight.data\n",
    "            self.sharedLayers[0] = nn.Linear(self.num_inputs, self.hidden[0])\n",
    "            self.sharedLayers[0].weight.data[0:-increment[0],:] = weight\n",
    "\n",
    "        for i in range(1, len(self.sharedLayers)):\n",
    "            weight = self.sharedLayers[i].weight.data\n",
    "            self.sharedLayers[i] = nn.Linear(self.hidden[i-1], self.hidden[i])\n",
    "            if increment[i] > 0:\n",
    "                if increment[i-1] > 0:\n",
    "                    self.sharedLayers[i].weight.data[0:-increment[i],0:-increment[i-1]] = weight\n",
    "                else:\n",
    "                    self.sharedLayers[i].weight.data[0:-increment[i],0:] = weight\n",
    "            else:\n",
    "                if increment[i-1] > 0:\n",
    "                        self.sharedLayers[i].weight.data[0:,0:-increment[i-1]] = weight\n",
    "                else:\n",
    "                    self.sharedLayers[i].weight.data[0:,0:] = weight\n",
    "            \n",
    "        weight_adv1 = self.adv1.weight.data\n",
    "        self.adv1 = nn.Linear(self.hidden[-2], self.hidden[-1])\n",
    "        \n",
    "        weight_v1 = self.v1.weight.data\n",
    "        self.v1 = nn.Linear(self.hidden[-2], self.hidden[-1])\n",
    "        if increment[-1] > 0:\n",
    "            if increment[-2] > 0:\n",
    "                self.adv1.weight.data[0:-increment[-1],0:-increment[-2]] = weight_adv1\n",
    "                self.v1.weight.data[0:-increment[-1],0:-increment[-2]] = weight_v1\n",
    "            else:\n",
    "                self.adv1.weight.data[0:-increment[-1],0:] = weight_adv1\n",
    "                self.v1.weight.data[0:-increment[-1],0:] = weight_v1\n",
    "        else:\n",
    "            if increment[-2] > 0:\n",
    "                self.adv1.weight.data[0:,0:-increment[-2]] = weight_adv1\n",
    "                self.v1.weight.data[0:,0:-increment[-2]] = weight_v1\n",
    "            else:\n",
    "                self.adv1.weight.data[0:,0:] = weight_adv1\n",
    "                self.v1.weight.data[0:,0:] = weight_v1\n",
    "            \n",
    "        weight_adv2 = self.adv2.weight.data\n",
    "        self.adv2 = nn.Linear(self.hidden[-1], self.num_actions)\n",
    "        \n",
    "        weight_v2 = self.v2.weight.data\n",
    "        self.v2 = nn.Linear(self.hidden[-1], 1)\n",
    "        \n",
    "        if increment[-1] > 0:\n",
    "            self.adv2.weight.data[:,0:-increment[-1]] = weight_adv2\n",
    "            self.v2.weight.data[:,0:-increment[-1]] = weight_v2\n",
    "        else:\n",
    "            self.adv2.weight.data[:,0:] = weight_adv2\n",
    "            self.v2.weight.data[:,0:] = weight_v2\n",
    "        \n",
    "    def decrease_capacity(self, increment):\n",
    "        for i in range(len(self.hidden)):\n",
    "            self.hidden[i] -= increment[i]\n",
    "\n",
    "        weight = self.sharedLayers[0].weight.data\n",
    "        self.sharedLayers[0] = nn.Linear(self.num_inputs, self.hidden[0])\n",
    "        self.sharedLayers[0].weight.data = weight[0:-increment[0],:]\n",
    "        \n",
    "        for i in range(1, len(self.sharedLayers)):\n",
    "            weight = self.sharedLayers[i].weight.data\n",
    "            self.sharedLayers[i] = nn.Linear(self.hidden[i-1], self.hidden[i])\n",
    "            self.sharedLayers[i].weight.data = weight[0:-increment[i],0:-increment[i-1]]\n",
    "            \n",
    "        weight = self.adv1.weight.data\n",
    "        self.adv1 = nn.Linear(self.hidden[-2], self.hidden[-1])\n",
    "        self.adv1.weight.data = weight[0:-increment[-1],0:-increment[-2]]\n",
    "            \n",
    "        weight = self.adv2.weight.data\n",
    "        self.adv2 = nn.Linear(self.hidden[-1], self.num_actions)\n",
    "        self.adv2.weight.data = weight[:,0:-increment[-1]]\n",
    "        \n",
    "        weight = self.v1.weight.data\n",
    "        self.v1 = nn.Linear(self.hidden[-2], self.hidden[-1])\n",
    "        self.v1.weight.data = weight[0:-increment[-1],0:-increment[-2]]\n",
    "            \n",
    "        weight = self.v2.weight.data\n",
    "        self.v2 = nn.Linear(self.hidden[-1], 1)\n",
    "        self.v2.weight.data = weight[:,0:-increment[-1]]\n",
    "        \n",
    "    def act(self, state, goal, epsilon):\n",
    "        if np.random.rand() > epsilon:\n",
    "            state = torch.cat((torch.tensor([state], dtype=torch.float32, device=device),torch.tensor([goal], dtype=torch.float32, device=device)),1)\n",
    "            q_values = self.forward(state)\n",
    "            action = q_values.max(1)[1].view(1, 1).item()\n",
    "        else:\n",
    "            action =  np.random.randint(self.num_actions)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqnHER(q1, target_network, memory, optimizer, batch_size, gamma, local_steps, doubleDQN):\n",
    "    if len(memory) < batch_size:\n",
    "        return None\n",
    "\n",
    "    batch, indices, weights = memory.sample(batch_size, local_steps)\n",
    "\n",
    "    state, action, reward, next_state, done, goal = zip(*batch)\n",
    "    \n",
    "    state = torch.cat((torch.tensor(state, dtype=torch.float32, device=device),torch.tensor(goal, dtype=torch.float32, device=device)) ,1)\n",
    "    \n",
    "                      \n",
    "    action = torch.tensor(action, dtype=torch.long, device=device)\n",
    "    next_state = torch.cat((torch.tensor(next_state, dtype=torch.float32, device=device), torch.tensor(goal, dtype=torch.float32, device=device)),1)\n",
    "    reward = torch.tensor(reward, dtype=torch.float32, device=device)\n",
    "    done = torch.tensor(done, dtype=torch.float32, device=device)\n",
    "\n",
    "    \n",
    "    weights.to(device)\n",
    "\n",
    "    q_val = compute_q_val(q1, state, action)\n",
    "\n",
    "    with torch.no_grad():\n",
    "# Vanilla\n",
    "#         target = compute_target_dqn(q1, reward, next_state, done, gamma)\n",
    "        if doubleDQN:\n",
    "            target = compute_target_ddqn(q1, target_network, reward, next_state, done, gamma)\n",
    "        else:\n",
    "            target = compute_target_dqn(target_network, reward, next_state, done, gamma)\n",
    "#     loss = F.mse_loss(q_val, target)\n",
    "    difference = (q_val - target.view(-1,1))\n",
    "    \n",
    "    # Weights is 1 for normal replay buffer so nothing changes\n",
    "    # McAleer divides the loss by the number of moves of the scramble here. Might not make sense in non-MCTS setting\n",
    "    loss = difference.pow(2) * weights\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Also taken from higgsfield\n",
    "    memory.update_priorities(indices, difference.detach().squeeze().abs().cpu().numpy().tolist())\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_settingsHER(architecture, duelingDQN, doubleDQN, prioritizedReplayMemory, alpha, \n",
    "                        memoryCapacity, lr, amsgrad, epochs, batch_size, gamma, capacity_increase, \n",
    "                        threshold, evaluation_frequency, tau, curriculum, non_linearity, her,\n",
    "                        verbose=False, load_path=None, save_path=None, seed=None):\n",
    "    \n",
    "    goal = rubiks2.RubiksEnv2(2,unsolved_reward=-1.0).get_observation()\n",
    "    \n",
    "    # If the directory does not exist, make one\n",
    "    if save_path:\n",
    "        if not os.path.isdir(save_path):\n",
    "            os.mkdir(save_path)\n",
    "    \n",
    "    # If a seed is set, set the seed for all sources of randomness\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "            \n",
    "    # Difficulty the problem starts with\n",
    "    difficulty = 0\n",
    "    # The maximum number of tries the agent gets at the start\n",
    "    max_tries_start = 1\n",
    "    max_tries = max_tries_start\n",
    "    # 3 is chosen because this allows the network to learn the difference between short and long paths from the beginning. Take for example a cube that has been scrambled as follows: U. The solution within 3 steps is eather U' (r=1) or U, U, U (r=-1)\n",
    "    \n",
    "    # Arrays to keep track of losses and accuracies over time\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    # Global steps keeps track of the total number of optimisation steps\n",
    "    global_steps = 0\n",
    "    # Local steps keeps track of number of optimisation steps within a level\n",
    "    local_steps = 0\n",
    "    # Total time keeps track of how long the training process takes\n",
    "    \n",
    "    # Epsilon exponential decay\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_final = 0.01\n",
    "    # Duration of decay dependent on difficulty\n",
    "#     epsilon_decay = 10000*(difficulty//12 + 1)\n",
    "    epsilon_decay = 10000\n",
    "    # Duration independent on dificulty\n",
    "    epsilon_by_step = lambda step_idx: epsilon_final + (epsilon_start - epsilon_final) * np.exp(-1. * step_idx / epsilon_decay)\n",
    "    \n",
    "    # If a path with a model is provided certain variables are loaded\n",
    "    if load_path:\n",
    "        current_model = torch.load(load_path + 'model.pt')\n",
    "        target_model = copy.deepcopy(current_model)\n",
    "        max_tries = torch.load(load_path + 'max_tries')\n",
    "        epsilon_decay = torch.load(load_path + 'epsilon_decay')\n",
    "        global_steps = torch.load(load_path + 'global_steps')\n",
    "        local_steps = torch.load(load_path + 'local_steps')\n",
    "        difficulty = torch.load(load_path + 'difficulty')\n",
    "    else:\n",
    "        # n^2 per face, 6 faces, 6 colours one-hot encoded. 3x3x6x6=324, 2x2x6x6=144\n",
    "        state_size = env.size**2 * 6**2\n",
    "        \n",
    "        # The number of output nodes is different for the 2x2x2. The move L is the same as the move R, except the orientation changes.\n",
    "        output_nodes = 12\n",
    "        if env.size == 2:\n",
    "            output_nodes = 6\n",
    "        \n",
    "        # Initialising either a network with dueling architecture or regular network\n",
    "        if duelingDQN:\n",
    "            current_model = DuelingDQNHER(state_size, architecture, output_nodes, non_linearity)\n",
    "            target_model = DuelingDQNHER(state_size, copy.copy(architecture), output_nodes, non_linearity)\n",
    "        else:\n",
    "            current_model = DQN(state_size, architecture, output_nodes, non_linearity)\n",
    "            target_model = DQN(state_size, copy.copy(architecture), output_nodes, non_linearity)\n",
    "            \n",
    "    if torch.cuda.is_available():\n",
    "        current_model.to('cuda')\n",
    "        target_model.to('cuda')\n",
    "    else:\n",
    "        current_model.to('cpu')\n",
    "        target_model.to('cpu')\n",
    "    \n",
    "    # Uses prioritized replay sampling when set to true, otherwise uniform replay sampling is used\n",
    "    if prioritizedReplayMemory:\n",
    "        memory = PrioritizedReplayMemory(memoryCapacity, alpha)\n",
    "    else:\n",
    "        memory = ReplayMemory(memoryCapacity)\n",
    "    \n",
    "    # Initialise optimiser\n",
    "    optimizer = optim.Adam(current_model.parameters(), lr=lr, amsgrad=amsgrad)\n",
    "    \n",
    "    # This allows you to stop the training progress\n",
    "    try:\n",
    "        for epoch in range(epochs):\n",
    "            epoch_losses = []\n",
    "            \n",
    "            # Different types of curricula decide what state to show the network next.\n",
    "            if curriculum is 'Naive':\n",
    "                state = env.curriculum_reset(difficulty)\n",
    "            if curriculum is 'Joe':\n",
    "                p = np.random.rand()\n",
    "                if p < 0.2:\n",
    "                    state = env.curriculum_reset(np.random.randint(difficulty + 1, difficulty + 12))\n",
    "                else:\n",
    "                    state = env.curriculum_reset(difficulty)\n",
    "            if curriculum is 'Sutskever':\n",
    "                p = np.random.rand()\n",
    "                if p < 0.2:\n",
    "                    state = env.reset(np.random.randint(difficulty + 1,1000))\n",
    "                else:\n",
    "                    state = env.curriculum_reset(difficulty)\n",
    "            if curriculum is 'Mixed':\n",
    "                state = env.curriculum_reset(np.random.randint(difficulty + 1,1000))\n",
    "                \n",
    "            done = 0\n",
    "            tries = 0\n",
    "            transitions = []\n",
    "            while tries < max_tries and not done:\n",
    "                epsilon = epsilon_by_step(local_steps)\n",
    "                action = current_model.act(state,goal, epsilon)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                memory.push((state, action, reward, next_state, done, goal))\n",
    "                transitions.append((state, action, reward, next_state, done, goal))\n",
    "                state = next_state\n",
    "\n",
    "                \n",
    "\n",
    "                tries += 1\n",
    "                \n",
    "            loss = train_dqnHER(current_model, target_model, memory, optimizer, batch_size, gamma, local_steps, doubleDQN)\n",
    "\n",
    "            if loss:\n",
    "                epoch_losses.append(loss)\n",
    "                \n",
    "            if not done:\n",
    "                new_goal = copy.copy(state)\n",
    "\n",
    "                for trans in transitions:\n",
    "                    if np.array_equal(trans[3],new_goal):\n",
    "                        memory.push((trans[0],trans[1],0,trans[3],trans[4],new_goal))\n",
    "                    else:\n",
    "                        memory.push((trans[0],trans[1],trans[2],trans[3],trans[4],new_goal))\n",
    "            \n",
    "            global_steps += 1\n",
    "            local_steps += 1\n",
    "                \n",
    "            if global_steps % tau == 0:\n",
    "                target_model.load_state_dict(current_model.state_dict())\n",
    "                if save_path:\n",
    "                    torch.save(current_model, save_path + \"model.pt\")\n",
    "                    torch.save(max_tries, save_path + \"max_tries\")\n",
    "                    torch.save(epsilon_decay, save_path + \"epsilon_decay\")\n",
    "                    torch.save(global_steps, save_path + \"global_steps\")\n",
    "                    torch.save(local_steps, save_path + \"local_steps\")\n",
    "                    torch.save(local_steps, save_path + \"difficulty\")\n",
    "                    \n",
    "            if global_steps % evaluation_frequency == 0:\n",
    "                total_done = 0\n",
    "                for i in range(500):\n",
    "                    # Here the agent is forced to evaluate its ability to solve certain last moves\n",
    "                    state = env.force_last_action_reset(difficulty)\n",
    "                    done = 0\n",
    "                    tries = 0\n",
    "                    while tries < max_tries and not done:\n",
    "                        action = current_model.act(state,goal, 0)\n",
    "                        next_state, reward, done, info = env.step(action)\n",
    "                        memory.push((state, action, reward, next_state, done, goal))\n",
    "                        state = next_state\n",
    "                        total_done += done\n",
    "                        tries += 1\n",
    "                        \n",
    "                # Here the agent evaluates the ability to solve puzzles that have the last move from a set of moves with the same difficulty\n",
    "                for i in range(500):\n",
    "                    state = env.curriculum_reset(difficulty)\n",
    "                    done = 0\n",
    "                    tries = 0\n",
    "                    while tries < max_tries and not done:\n",
    "                        action = current_model.act(state,goal, 0)\n",
    "                        next_state, reward, done, info = env.step(action)\n",
    "                        memory.push((state, action, reward, next_state, done, goal))\n",
    "                        state = next_state\n",
    "                        total_done += done\n",
    "                        tries += 1\n",
    "                \n",
    "                accuracy = total_done/(1000)\n",
    "                accuracies.append(accuracy)\n",
    "                \n",
    "                if accuracy >= threshold or epsilon<0.0105:\n",
    "                    difficulty += 1\n",
    "                    max_tries = difficulty // output_nodes + max_tries_start\n",
    "                    local_steps = 0\n",
    "                    \n",
    "                    epsilon_start = 1.0\n",
    "                    epsilon_final = 0.01\n",
    "#                     epsilon_decay = 10000*(difficulty//output_nodes + 1)\n",
    "                    epsilon_decay = 10000\n",
    "\n",
    "                    epsilon_by_step = lambda step_idx: epsilon_final + (epsilon_start - epsilon_final) * np.exp(-1. * step_idx / epsilon_decay)\n",
    "                    \n",
    "                    if capacity_increase:\n",
    "        #                         memory.beta_start = 0.5\n",
    "                        cap = [c(difficulty) for c in capacity_increase]\n",
    "                        current_model.increase_capacity(cap)\n",
    "                        target_model.increase_capacity(cap)\n",
    "                        current_model.to(device)\n",
    "                        target_model.to(device)\n",
    "                        optimizer = optim.Adam(current_model.parameters(), lr=lr, amsgrad=amsgrad)\n",
    "\n",
    "                if verbose:\n",
    "                    clear_output(True)\n",
    "                    print(\"Epoch: \", epoch, \"Global steps: \", global_steps)\n",
    "                    print(\"Difficulty: \", difficulty, \"Max tries:\", max_tries)\n",
    "                    print(\"Memory: \", len(memory), \"epsilon: \", epsilon)\n",
    "                    print(\"Accuracy: \", accuracy)\n",
    "                    print(current_model)\n",
    "                    \n",
    "                losses.append(np.average(epoch_losses))\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        if save_path:\n",
    "            torch.save(current_model, save_path + \"model.pt\")\n",
    "            torch.save(max_tries, save_path + \"max_tries\")\n",
    "            torch.save(epsilon_decay, save_path + \"epsilon_decay\")\n",
    "            torch.save(global_steps, save_path + \"global_steps\")\n",
    "            torch.save(local_steps, save_path + \"local_steps\")\n",
    "            torch.save(local_steps, save_path + \"difficulty\")\n",
    "            \n",
    "    return difficulty, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: all CUDA-capable devices are busy or unavailable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-4b693add3590>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrubiks2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRubiksEnv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsolved_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtrain_with_settingsHER\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchitecture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduelingDQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoubleDQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprioritizedReplayMemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemoryCapacity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapacity_increase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluation_frequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurriculum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_linearity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'models/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-1035d8492b8b>\u001b[0m in \u001b[0;36mtrain_with_settingsHER\u001b[0;34m(architecture, duelingDQN, doubleDQN, prioritizedReplayMemory, alpha, memoryCapacity, lr, amsgrad, epochs, batch_size, gamma, capacity_increase, threshold, evaluation_frequency, tau, curriculum, non_linearity, her, verbose, load_path, save_path, seed)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mcurrent_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mtarget_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;31m# Tensors stored in modules are graph leaves, and we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                 \u001b[0;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: all CUDA-capable devices are busy or unavailable"
     ]
    }
   ],
   "source": [
    "epochs = 50000\n",
    "experiments = []\n",
    "experiments.append([\"Baseline_HER\", [4096, 2048, 512], True, False, False, 0.7, 100000, 1e-3, False, epochs, 128, 0.99, None, 0.95, 100, 1000, 'Naive', F.relu])\n",
    "experiments.append([\"Mixed_HER\", [4096, 2048, 512], True, False, False, 0.7, 100000, 1e-3, True, epochs, 128, 0.99, None, 0.95, 100, 1000, 'Mixed', F.relu])\n",
    "experiments.append([\"Joe_HER\", [4096, 2048, 512], True, False, False, 0.7, 100000, 1e-3, True, epochs, 128, 0.99, None, 0.95, 100, 1000, 'Joe', F.relu])\n",
    "experiments.append([\"Sutskever_HER\", [4096, 2048, 512], True, False, False, 0.7, 100000, 1e-3, True, epochs, 128, 0.99, None, 0.95, 100, 1000, 'Sutskever', F.relu])\n",
    "experiments.append([\"Prioritised0.5_HER\", [4096, 2048, 512], True, False, True, 0.5, 100000, 1e-3, True, epochs, 128, 0.99, None, 0.95, 100, 1000, 'Sutskever', F.relu])\n",
    "experiments.append([\"Prioritised0.7_HER\", [4096, 2048, 512], True, False, True, 0.7, 100000, 1e-3, True, epochs, 128, 0.99, None, 0.95, 100, 1000, 'Sutskever', F.relu])\n",
    "experiments.append([\"Prioritised0.9_HER\", [4096, 2048, 512], True, False, True, 0.9, 100000, 1e-3, True, epochs, 128, 0.99, None, 0.95, 100, 1000, 'Sutskever', F.relu])\n",
    "experiments.append([\"Dueling_HER\", [4096, 2048, 512], True, False, True, 0.9, 100000, 1e-3, True, epochs, 128, 0.99, None, 0.95, 100, 1000, 'Sutskever', F.relu])\n",
    "experiments.append([\"Double_HER\", [4096, 2048, 512], True, True, True, 0.9, 100000, 1e-3, True, epochs, 128, 0.99, None, 0.95, 100, 1000, 'Sutskever', F.relu])\n",
    "experiments.append([\"Linear_all_HER\", [64, 32, 8], True, True, True, 0.9, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: 48, lambda x: 24, lambda x: 6], 0.95, 100, 1000, 'Sutskever', F.relu])\n",
    "experiments.append([\"Elu_HER\", [4096, 2048, 512], True, True, True, 0.9, 100000, 1e-3, True, epochs, 128, 0.99, None, 0.95, 100, 1000, 'Sutskever', F.elu])\n",
    "experiments.append([\"Leaky_relu_HER\", [4096, 2048, 512], True, True, True, 0.9, 100000, 1e-3, True, epochs, 128, 0.99, None, 0.95, 100, 1000, 'Sutskever', F.leaky_relu])\n",
    "\n",
    "\n",
    "for experiment in experiments:\n",
    "    architecture, duelingDQN, doubleDQN, prioritizedReplayMemory, alpha, memoryCapacity, lr, amsgrad, epochs, batch_size, gamma, capacity_increase, threshold, evaluation_frequency, tau, curriculum, non_linearity = experiment[1:]\n",
    "\n",
    "    env = rubiks2.RubiksEnv2(2, unsolved_reward = -1.0, seed= 42)\n",
    "\n",
    "    train_with_settingsHER(architecture, duelingDQN, doubleDQN, prioritizedReplayMemory, alpha, memoryCapacity, lr, amsgrad, epochs, batch_size, gamma, capacity_increase, threshold, evaluation_frequency, tau, curriculum, non_linearity, True, save_path='models/'+experiment[0]+'/', seed=42, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
