{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import rubiks\n",
    "import rubiks2\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os \n",
    "import copy\n",
    "import time\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from data_structures import SegmentTree, MinSegmentTree, SumSegmentTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n",
      "Torch Version:  1.0.1.post2\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)\n",
    "print(\"Torch Version: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "        \n",
    "    def push(self, batch):\n",
    "        self.memory.append((batch))\n",
    "        \n",
    "    def sample(self, batch_size, _):\n",
    "        # Lazy programming. Returns 1, 1 so that I don't have write additional code that discerns between replay memory and prioritized replay memory\n",
    "        return random.sample(self.memory, batch_size), 1, 1\n",
    "    \n",
    "    def update_priorities(self, idxes, priorities):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even more laziness. Stole this code from https://github.com/higgsfield/\n",
    "class PrioritizedReplayMemory(object):\n",
    "    def __init__(self, size, alpha=0.7, beta_start=0.5 , beta_frames=10000):\n",
    "        \"\"\"Create Prioritized Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        alpha: float\n",
    "            how much prioritization is used\n",
    "            (0 - no prioritization, 1 - full prioritization)\n",
    "        See Also\n",
    "        --------\n",
    "        ReplayBuffer.__init__\n",
    "        \"\"\"\n",
    "        super(PrioritizedReplayMemory, self).__init__()\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "        self._next_idx = 0\n",
    "\n",
    "        assert alpha >= 0\n",
    "        self._alpha = alpha\n",
    "\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_frames = beta_frames\n",
    "\n",
    "        it_capacity = 1\n",
    "        while it_capacity < size:\n",
    "            it_capacity *= 2\n",
    "\n",
    "        self._it_sum = SumSegmentTree(it_capacity)\n",
    "        self._it_min = MinSegmentTree(it_capacity)\n",
    "        self._max_priority = 1.0\n",
    "\n",
    "    def beta_by_frame(self, frame_idx):\n",
    "        return min(1.0, self.beta_start + frame_idx * (1.0 - self.beta_start) / self.beta_frames)\n",
    "\n",
    "    def push(self, data):\n",
    "        \"\"\"See ReplayBuffer.store_effect\"\"\"\n",
    "        idx = self._next_idx\n",
    "\n",
    "        if self._next_idx >= len(self._storage):\n",
    "            self._storage.append(data)\n",
    "        else:\n",
    "            self._storage[self._next_idx] = data\n",
    "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
    "\n",
    "\n",
    "        self._it_sum[idx] = self._max_priority ** self._alpha\n",
    "        self._it_min[idx] = self._max_priority ** self._alpha\n",
    "\n",
    "    def _encode_sample(self, idxes):\n",
    "        return [self._storage[i] for i in idxes]\n",
    "\n",
    "    def _sample_proportional(self, batch_size):\n",
    "        res = []\n",
    "        for _ in range(batch_size):\n",
    "            # TODO(szymon): should we ensure no repeats?\n",
    "            mass = random.random() * self._it_sum.sum(0, len(self._storage) - 1)\n",
    "            idx = self._it_sum.find_prefixsum_idx(mass)\n",
    "            res.append(idx)\n",
    "        return res\n",
    "\n",
    "    def sample(self, batch_size, global_steps):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "        compared to ReplayBuffer.sample\n",
    "        it also returns importance weights and idxes\n",
    "        of sampled experiences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        beta: float\n",
    "            To what degree to use importance weights\n",
    "            (0 - no corrections, 1 - full correction)\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            batch of observations\n",
    "        act_batch: np.array\n",
    "            batch of actions executed given obs_batch\n",
    "        rew_batch: np.array\n",
    "            rewards received as results of executing act_batch\n",
    "        next_obs_batch: np.array\n",
    "            next set of observations seen after executing act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        weights: np.array\n",
    "            Array of shape (batch_size,) and dtype np.float32\n",
    "            denoting importance weight of each sampled transition\n",
    "        idxes: np.array\n",
    "            Array of shape (batch_size,) and dtype np.int32\n",
    "            idexes in buffer of sampled experiences\n",
    "        \"\"\"\n",
    "\n",
    "        idxes = self._sample_proportional(batch_size)\n",
    "\n",
    "        weights = []\n",
    "\n",
    "        #find smallest sampling prob: p_min = smallest priority^alpha / sum of priorities^alpha\n",
    "        p_min = self._it_min.min() / self._it_sum.sum()\n",
    "\n",
    "        beta = self.beta_by_frame(global_steps)\n",
    "        \n",
    "        #max_weight given to smallest prob\n",
    "        max_weight = (p_min * len(self._storage)) ** (-beta)\n",
    "\n",
    "        for idx in idxes:\n",
    "            p_sample = self._it_sum[idx] / self._it_sum.sum()\n",
    "            weight = (p_sample * len(self._storage)) ** (-beta)\n",
    "            weights.append(weight / max_weight)\n",
    "        weights = torch.tensor(weights, device=device, dtype=torch.float) \n",
    "        encoded_sample = self._encode_sample(idxes)\n",
    "        return encoded_sample, idxes, weights\n",
    "\n",
    "    def update_priorities(self, idxes, priorities):\n",
    "        \"\"\"Update priorities of sampled transitions.\n",
    "        sets priority of transition at index idxes[i] in buffer\n",
    "        to priorities[i].\n",
    "        Parameters\n",
    "        ----------\n",
    "        idxes: [int]\n",
    "            List of idxes of sampled transitions\n",
    "        priorities: [float]\n",
    "            List of updated priorities corresponding to\n",
    "            transitions at the sampled idxes denoted by\n",
    "            variable `idxes`.\n",
    "        \"\"\"\n",
    "        assert len(idxes) == len(priorities)\n",
    "#         print(idxes, priorities)\n",
    "#         print(list(zip(idxes, priorities)))\n",
    "        for idx, priority in zip(idxes, priorities):\n",
    "            assert 0 <= idx < len(self._storage)\n",
    "            self._it_sum[idx] = (priority+1e-5) ** self._alpha\n",
    "\n",
    "            self._it_min[idx] = (priority+1e-5) ** self._alpha\n",
    "\n",
    "            self._max_priority = max(self._max_priority, (priority+1e-5))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self._storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise rubiks cube environment. First argument determines the size of the cube. \n",
    "# The unsolved_reward parameter determines the penalty the reinforcementagent incurs of each move that does not lead to the solved state.\n",
    "# This forces the agent to favour shorter paths. E.g. cube that is scrambled with U can be solved by the sequence U,U,U-> r=-1 or by U'->r=1\n",
    "env = rubiks2.RubiksEnv2(2, unsolved_reward = -1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon decay\n",
    "\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 10000\n",
    "\n",
    "epsilon_by_step = lambda step_idx: epsilon_final + (epsilon_start - epsilon_final) * np.exp(-1. * step_idx / epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f590d8e2d68>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Epsilon decay')\n",
    "plt.xlabel('Global steps')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.plot([epsilon_by_step(i) for i in range(10000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classic DQN. Increase_capacity method adds new nodes to layers according to increment\n",
    "# TODO: decrease capacity does not work as of yt\n",
    "# TODO: Non-linearity defined in init\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_inputs, hidden, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.num_inputs = num_inputs\n",
    "        self.hidden = hidden\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(num_inputs, self.hidden[0]))\n",
    "        \n",
    "        previous = self.hidden[0]\n",
    "        for hidden_layer_size in self.hidden[1:]:\n",
    "            self.layers.append(nn.Linear(previous, hidden_layer_size))\n",
    "            previous = hidden_layer_size\n",
    "            \n",
    "        self.layers.append(nn.Linear(previous, num_actions))        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            x = F.elu(self.layers[i](x))\n",
    "#             x = F.relu(self.layers[i](x))\n",
    "        return self.layers[-1](x)\n",
    "    \n",
    "    def increase_capacity(self, increment):\n",
    "        for i in range(len(self.hidden)):\n",
    "            self.hidden[i] += increment[i]\n",
    "        \n",
    "        weight = self.layers[0].weight.data\n",
    "        self.layers[0] = nn.Linear(self.num_inputs, self.hidden[0])\n",
    "        if increment[0]>0:\n",
    "            self.layers[0].weight.data[0:-increment[0],:] = weight\n",
    "        else:\n",
    "            self.layers[0].weight.data[0:,:] = weight\n",
    "        \n",
    "        for i in range(1, len(self.layers) - 1):\n",
    "            weight = self.layers[i].weight.data\n",
    "            self.layers[i] = nn.Linear(self.hidden[i-1], self.hidden[i])\n",
    "            if increment[i] > 0:\n",
    "                if increment[i-1] >0: \n",
    "                    self.layers[i].weight.data[0:-increment[i],0:-increment[i-1]] = weight\n",
    "                else:\n",
    "                    self.layers[i].weight.data[0:-increment[i],0:] = weight\n",
    "            else:\n",
    "                if increment[i-1] >0:\n",
    "                    self.layers[i].weight.data[0:,0:-increment[i-1]] = weight\n",
    "                else:\n",
    "                    self.layers[i].weight.data[0:,0:] = weight\n",
    "        \n",
    "        weight = self.layers[-1].weight.data\n",
    "        self.layers[-1] = nn.Linear(self.hidden[-1], self.num_actions)\n",
    "        if increment[-1] >0:\n",
    "            self.layers[-1].weight.data[:,0:-increment[-1]] = weight\n",
    "        else:\n",
    "            self.layers[-1].weight.data[:,0:] = weight\n",
    "        \n",
    "    def decrease_capacity(self, increment):\n",
    "        for i in range(len(self.hidden)):\n",
    "            self.hidden[i] -= increment[i]\n",
    "        \n",
    "        weight = self.layers[0].weight.data\n",
    "        self.layers[0] = nn.Linear(self.num_inputs, self.hidden[0])\n",
    "        self.layers[0].weight.data = weight[0:-increment[0],:]\n",
    "        \n",
    "        for i in range(1, len(self.layers) - 1):\n",
    "            weight = self.layers[i].weight.data\n",
    "            self.layers[i] = nn.Linear(self.hidden[i-1], self.hidden[i])\n",
    "            self.layers[i].weight.data = weight[0:-increment[i],0:-increment[i-1]]\n",
    "        \n",
    "        weight = self.layers[-1].weight.data\n",
    "        self.layers[-1] = nn.Linear(self.hidden[-1], self.num_actions)\n",
    "        self.layers[-1].weight.data = weight[:,0:-increment[-1]]\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        if np.random.rand() > epsilon:\n",
    "            state = torch.tensor([state], dtype=torch.float32, device=device)\n",
    "            q_values = self.forward(state)\n",
    "            action = q_values.max(1)[1].view(1, 1).item()\n",
    "        else:\n",
    "            action =  np.random.randint(self.num_actions)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, num_inputs, hidden, num_actions):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        \n",
    "        self.num_inputs = num_inputs\n",
    "        self.hidden = hidden\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.sharedLayers = nn.ModuleList()\n",
    "        self.sharedLayers.append(nn.Linear(num_inputs, self.hidden[0]))\n",
    "        \n",
    "        previous = self.hidden[0]\n",
    "        for hidden_layer_size in self.hidden[1:-1]:\n",
    "            self.sharedLayers.append(nn.Linear(previous, hidden_layer_size))\n",
    "            previous = hidden_layer_size\n",
    "\n",
    "        self.adv1 = nn.Linear(previous, self.hidden[-1])\n",
    "        self.adv2 = nn.Linear(self.hidden[-1], num_actions)\n",
    "        \n",
    "        self.v1 = nn.Linear(previous, self.hidden[-1])\n",
    "        self.v2 = nn.Linear(self.hidden[-1], 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.sharedLayers)):\n",
    "            \n",
    "            x = F.elu(self.sharedLayers[i](x))\n",
    "#             x = F.relu(self.sharedLayers[i](x))\n",
    "            \n",
    "#         a = F.relu(self.advenv.action_space.n1(x))\n",
    "        a = F.elu(self.adv1(x))\n",
    "        a = self.adv2(a)\n",
    "        \n",
    "        v = F.elu(self.v1(x))\n",
    "#         v = F.relu(self.v1(x))\n",
    "        v = self.v2(v)\n",
    "        \n",
    "        return v + a - a.mean()\n",
    "    \n",
    "    def increase_capacity(self, increment):\n",
    "        for i in range(len(self.hidden)):\n",
    "            self.hidden[i] += increment[i]\n",
    "        \n",
    "        # Check whether the increment isn't zero\n",
    "        if increment[0] > 0:\n",
    "            weight = self.sharedLayers[0].weight.data\n",
    "            self.sharedLayers[0] = nn.Linear(self.num_inputs, self.hidden[0])\n",
    "            self.sharedLayers[0].weight.data[0:-increment[0],:] = weight\n",
    "\n",
    "        for i in range(1, len(self.sharedLayers)):\n",
    "            weight = self.sharedLayers[i].weight.data\n",
    "            self.sharedLayers[i] = nn.Linear(self.hidden[i-1], self.hidden[i])\n",
    "            if increment[i] > 0:\n",
    "                if increment[i-1] > 0:\n",
    "                    self.sharedLayers[i].weight.data[0:-increment[i],0:-increment[i-1]] = weight\n",
    "                else:\n",
    "                    self.sharedLayers[i].weight.data[0:-increment[i],0:] = weight\n",
    "            else:\n",
    "                if increment[i-1] > 0:\n",
    "                        self.sharedLayers[i].weight.data[0:,0:-increment[i-1]] = weight\n",
    "                else:\n",
    "                    self.sharedLayers[i].weight.data[0:,0:] = weight\n",
    "            \n",
    "        weight_adv1 = self.adv1.weight.data\n",
    "        self.adv1 = nn.Linear(self.hidden[-2], self.hidden[-1])\n",
    "        \n",
    "        weight_v1 = self.v1.weight.data\n",
    "        self.v1 = nn.Linear(self.hidden[-2], self.hidden[-1])\n",
    "        if increment[-1] > 0:\n",
    "            if increment[-2] > 0:\n",
    "                self.adv1.weight.data[0:-increment[-1],0:-increment[-2]] = weight_adv1\n",
    "                self.v1.weight.data[0:-increment[-1],0:-increment[-2]] = weight_v1\n",
    "            else:\n",
    "                self.adv1.weight.data[0:-increment[-1],0:] = weight_adv1\n",
    "                self.v1.weight.data[0:-increment[-1],0:] = weight_v1\n",
    "        else:\n",
    "            if increment[-2] > 0:\n",
    "                self.adv1.weight.data[0:,0:-increment[-2]] = weight_adv1\n",
    "                self.v1.weight.data[0:,0:-increment[-2]] = weight_v1\n",
    "            else:\n",
    "                self.adv1.weight.data[0:,0:] = weight_adv1\n",
    "                self.v1.weight.data[0:,0:] = weight_v1\n",
    "            \n",
    "        weight_adv2 = self.adv2.weight.data\n",
    "        self.adv2 = nn.Linear(self.hidden[-1], self.num_actions)\n",
    "        \n",
    "        weight_v2 = self.v2.weight.data\n",
    "        self.v2 = nn.Linear(self.hidden[-1], 1)\n",
    "        \n",
    "        if increment[-1] > 0:\n",
    "            self.adv2.weight.data[:,0:-increment[-1]] = weight_adv2\n",
    "            self.v2.weight.data[:,0:-increment[-1]] = weight_v2\n",
    "        else:\n",
    "            self.adv2.weight.data[:,0:] = weight_adv2\n",
    "            self.v2.weight.data[:,0:] = weight_v2\n",
    "        \n",
    "    def decrease_capacity(self, increment):\n",
    "        for i in range(len(self.hidden)):\n",
    "            self.hidden[i] -= increment[i]\n",
    "\n",
    "        weight = self.sharedLayers[0].weight.data\n",
    "        self.sharedLayers[0] = nn.Linear(self.num_inputs, self.hidden[0])\n",
    "        self.sharedLayers[0].weight.data = weight[0:-increment[0],:]\n",
    "        \n",
    "        for i in range(1, len(self.sharedLayers)):\n",
    "            weight = self.sharedLayers[i].weight.data\n",
    "            self.sharedLayers[i] = nn.Linear(self.hidden[i-1], self.hidden[i])\n",
    "            self.sharedLayers[i].weight.data = weight[0:-increment[i],0:-increment[i-1]]\n",
    "            \n",
    "        weight = self.adv1.weight.data\n",
    "        self.adv1 = nn.Linear(self.hidden[-2], self.hidden[-1])\n",
    "        self.adv1.weight.data = weight[0:-increment[-1],0:-increment[-2]]\n",
    "            \n",
    "        weight = self.adv2.weight.data\n",
    "        self.adv2 = nn.Linear(self.hidden[-1], self.num_actions)\n",
    "        self.adv2.weight.data = weight[:,0:-increment[-1]]\n",
    "        \n",
    "        weight = self.v1.weight.data\n",
    "        self.v1 = nn.Linear(self.hidden[-2], self.hidden[-1])\n",
    "        self.v1.weight.data = weight[0:-increment[-1],0:-increment[-2]]\n",
    "            \n",
    "        weight = self.v2.weight.data\n",
    "        self.v2 = nn.Linear(self.hidden[-1], 1)\n",
    "        self.v2.weight.data = weight[:,0:-increment[-1]]\n",
    "        \n",
    "    def act(self, state, epsilon):\n",
    "        if np.random.rand() > epsilon:\n",
    "            state = torch.tensor([state], dtype=torch.float32, device=device)\n",
    "            q_values = self.forward(state)\n",
    "            action = q_values.max(1)[1].view(1, 1).item()\n",
    "        else:\n",
    "            action =  np.random.randint(self.num_actions)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the q-values of an action in a state\n",
    "def compute_q_val(model, state, action):\n",
    "    qactions = model(state)\n",
    "    return torch.gather(qactions,1,action.view(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the target. When done, 0 is added to the reward as there is no next state.\n",
    "def compute_target_dqn(model, reward, next_state, done, gamma):\n",
    "    return reward + gamma * model(next_state).max(1)[0] * (1-done)\n",
    "\n",
    "# Computes the target. When done, 0 is added to the reward as there is no next state. But now for Double DQN\n",
    "def compute_target_ddqn(model, target_model, reward, next_state, done, gamma):\n",
    "    a = model(next_state)\n",
    "    return reward.view(-1,1) + gamma * torch.gather(target_model(next_state),1,model(next_state).max(1)[1].view(-1,1)) * (1-done).view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(q1, target_network, memory, optimizer, batch_size, gamma, local_steps, doubleDQN):\n",
    "    if len(memory) < batch_size:\n",
    "        return None\n",
    "\n",
    "# Regular buffer\n",
    "#     state, action, reward, next_state, done = memory.sample(batch_size)\n",
    "    \n",
    "    batch, indices, weights = memory.sample(batch_size, local_steps)\n",
    "\n",
    "    state, action, reward, next_state, done = zip(*batch)\n",
    "    \n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "    action = torch.tensor(action, dtype=torch.long, device=device)\n",
    "    next_state = torch.tensor(next_state, dtype=torch.float32, device=device)\n",
    "    reward = torch.tensor(reward, dtype=torch.float32, device=device)\n",
    "    done = torch.tensor(done, dtype=torch.float32, device=device)\n",
    "    \n",
    "    weights.to(device)\n",
    "\n",
    "    q_val = compute_q_val(q1, state, action)\n",
    "\n",
    "    with torch.no_grad():\n",
    "# Vanilla\n",
    "#         target = compute_target_dqn(q1, reward, next_state, done, gamma)\n",
    "        if doubleDQN:\n",
    "            target = compute_target_ddqn(q1, target_network, reward, next_state, done, gamma)\n",
    "        else:\n",
    "            target = compute_target_dqn(target_network, reward, next_state, done, gamma)\n",
    "#     loss = F.mse_loss(q_val, target)\n",
    "    difference = (q_val - target.view(-1,1))\n",
    "    \n",
    "    # Weights is 1 for normal replay buffer so nothing changes\n",
    "    # McAleer divides the loss by the number of moves of the scramble here. Might not make sense in non-MCTS setting\n",
    "    loss = difference.pow(2) * weights\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # Also taken from higgsfield\n",
    "    memory.update_priorities(indices, difference.detach().squeeze().abs().cpu().numpy().tolist())\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_settings(architecture, duelingDQN, doubleDQN, prioritizedReplayMemory, alpha, \n",
    "                        memoryCapacity, lr, amsgrad, epochs, batch_size, gamma, capacity_increase, \n",
    "                        threshold, evaluation_frequency, tau, curriculum, verbose=False,\n",
    "                        load_path=None, save_path=None, seed=None):\n",
    "    if save_path:\n",
    "        if not os.path.isdir(save_path):\n",
    "            os.mkdir(save_path)\n",
    "    \n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "    \n",
    "    difficulty = 0\n",
    "    max_tries = 15\n",
    "\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "\n",
    "    global_steps = 0\n",
    "    local_steps = 0\n",
    "    \n",
    "    epsilon_start = 1.0\n",
    "    epsilon_final = 0.01\n",
    "    epsilon_decay = 10000*(difficulty//6 + 1)\n",
    "    epsilon_by_step = lambda step_idx: epsilon_final + (epsilon_start - epsilon_final) * np.exp(-1. * step_idx / epsilon_decay)\n",
    "    \n",
    "    if load_path:\n",
    "        current_model = torch.load(load_path + 'model.pt')\n",
    "        target_model = copy.deepcopy(current_model)\n",
    "        max_tries = torch.load(load_path + 'max_tries')\n",
    "        epsilon_decay = torch.load(load_path + 'epsilon_decay')\n",
    "        global_steps = torch.load(load_path + 'global_steps')\n",
    "        local_steps = torch.load(load_path + 'local_steps')\n",
    "        difficulty = torch.load(load_path + 'difficulty')\n",
    "    else:\n",
    "        state_size = env.size**2 * 6**2\n",
    "        \n",
    "        # Uses a Dueling DQN architecture when set to true, otherwise a vanilla DQN is used\n",
    "        if duelingDQN:\n",
    "            current_model = DuelingDQN(state_size, architecture, 6)\n",
    "            target_model = DuelingDQN(state_size, copy.copy(architecture), 6)\n",
    "        else:\n",
    "            current_model = DQN(state_size, architecture, 6)\n",
    "            target_model = DQN(state_size, copy.copy(architecture), 6)\n",
    "\n",
    "        # Puts models on device if possible\n",
    "        if torch.cuda.is_available():\n",
    "            current_model.to(device)\n",
    "            target_model.to(device)\n",
    "    \n",
    "    # Uses prioritized replay sampling when set to true, otherwise uniform replay sampling is used\n",
    "    if prioritizedReplayMemory:\n",
    "        memory = PrioritizedReplayMemory(memoryCapacity, alpha)\n",
    "    else:\n",
    "        memory = ReplayMemory(memoryCapacity)\n",
    "    \n",
    "    optimizer = optim.Adam(current_model.parameters(), lr=lr, amsgrad=amsgrad)\n",
    "\n",
    "    try:\n",
    "        for epoch in range(epochs):\n",
    "            t = time.time()\n",
    "            epoch_losses = []\n",
    "            p = np.random.rand()\n",
    "            \n",
    "            # Zaremba & Sutskever: combining naive curriculum with mixed curriculum\n",
    "            if curriculum is 'naive':\n",
    "                state = env.curriculum_reset(difficulty)\n",
    "            if curriculum is 'Joe':\n",
    "                if p < 0.2:\n",
    "                    # Mixing between 0 and max might not work as well.\n",
    "                    # Agents at level 6 can already solve every puzzle of 1 move.\n",
    "                    # An agent that is at level 1 might not learn much from a puzzle that requires 20 moves as it is unlikely that a state will be reached that the agent knows how to solve and therefore the q-values don't mean anything.\n",
    "    #                 state = env.curriculum_reset(np.random.randint(0,6*14))\n",
    "                    state = env.curriculum_reset(np.random.randint(difficulty, difficulty + 6))\n",
    "                else:\n",
    "                    state = env.curriculum_reset(difficulty)\n",
    "            if curriculum is 'Sutskever':\n",
    "                if p < 0.2:\n",
    "                    # Mixing between 0 and max might not work as well.\n",
    "                    # Agents at level 6 can already solve every puzzle of 1 move.\n",
    "                    # An agent that is at level 1 might not learn much from a puzzle that requires 20 moves as it is unlikely that a state will be reached that the agent knows how to solve and therefore the q-values don't mean anything.\n",
    "                    state = env.curriculum_reset(np.random.randint(0,1000))\n",
    "                else:\n",
    "                    state = env.curriculum_reset(difficulty)\n",
    "            if curriculum is 'mixed':\n",
    "                state = env.curriculum_reset(np.random.randint(0,1000))\n",
    "                \n",
    "            done = 0\n",
    "            tries = 0\n",
    "            while tries < max_tries and not done:\n",
    "                epsilon = epsilon_by_step(local_steps)\n",
    "                action = current_model.act(state, epsilon)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                memory.push((state, action, reward, next_state, done))\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                loss = train_dqn(current_model, target_model, memory, optimizer, batch_size, gamma, local_steps, doubleDQN)\n",
    "\n",
    "                if loss:\n",
    "                    epoch_losses.append(loss)\n",
    "\n",
    "                tries += 1\n",
    "                global_steps += 1\n",
    "                local_steps += 1\n",
    "\n",
    "            # Update target model every x global_steps and save current model\n",
    "            if global_steps % 100 == 0:\n",
    "                target_model.load_state_dict(current_model.state_dict())\n",
    "                if save_path:\n",
    "                    torch.save(current_model, save_path + \"model.pt\")\n",
    "                    torch.save(max_tries, save_path + \"max_tries\")\n",
    "                    torch.save(epsilon_decay, save_path + \"epsilon_decay\")\n",
    "                    torch.save(global_steps, save_path + \"global_steps\")\n",
    "                    torch.save(local_steps, save_path + \"local_steps\")\n",
    "                    torch.save(local_steps, save_path + \"difficulty\")\n",
    "                \n",
    "            # Evaluate every 10 global_steps\n",
    "            if global_steps % 10 == 0:\n",
    "                total_done = 0\n",
    "                for i in range(difficulty + 1):\n",
    "                    # Here the agent is forced to evaluate its ability to solve certain last moves\n",
    "                    state = env.force_last_action_reset(i)\n",
    "                    done = 0\n",
    "                    tries = 0\n",
    "                    while tries < max_tries and not done:\n",
    "                        action = current_model.act(state, 0)\n",
    "                        next_state, reward, done, info = env.step(action)\n",
    "                        memory.push((state, action, reward, next_state, done))\n",
    "                        state = next_state\n",
    "                        total_done += done\n",
    "                        tries += 1\n",
    "                \n",
    "                # Here the agent evaluates the ability to solve puzzles that have the last move from a set of moves with the same difficulty\n",
    "                for i in range(1000):\n",
    "                    state = env.curriculum_reset(difficulty)\n",
    "                    done = 0\n",
    "                    tries = 0\n",
    "                    while tries < max_tries and not done:\n",
    "                        action = current_model.act(state, 0)\n",
    "                        next_state, reward, done, info = env.step(action)\n",
    "                        memory.push((state, action, reward, next_state, done))\n",
    "                        state = next_state\n",
    "                        total_done += done\n",
    "                        tries += 1\n",
    "\n",
    "                accuracy = total_done/(1000 + difficulty + 1)\n",
    "                accuracies.append(accuracy)\n",
    "                \n",
    "                # When the accuracy surpasses or equals the threshold the number of tries is incremented, the epsilon is reset and the capacity of the network is increased.\n",
    "                if accuracy >= threshold:\n",
    "                    difficulty += 1\n",
    "                    max_tries = difficulty//6 + 1\n",
    "                    local_steps = 0\n",
    "                    \n",
    "                    epsilon_start = 1.0\n",
    "                    epsilon_final = 0.01\n",
    "                    epsilon_decay = 10000*(difficulty//6 + 1)\n",
    "\n",
    "                    epsilon_by_step = lambda step_idx: epsilon_final + (epsilon_start - epsilon_final) * np.exp(-1. * step_idx / epsilon_decay)\n",
    "\n",
    "\n",
    "                    if capacity_increase:\n",
    "#                         memory.beta_start = 0.5\n",
    "                        cap = [c(difficulty) for c in capacity_increase]\n",
    "                        current_model.increase_capacity(cap)\n",
    "                        target_model.increase_capacity(cap)\n",
    "                        current_model.to(device)\n",
    "                        target_model.to(device)\n",
    "                        optimizer = optim.Adam(current_model.parameters(), lr=lr, amsgrad=amsgrad)\n",
    "                \n",
    "                if verbose:\n",
    "                    clear_output(True)\n",
    "                    print(\"Epoch: \", epoch, \"Global steps: \", global_steps)\n",
    "                    print(\"Difficulty: \", difficulty, \"Max tries:\", max_tries)\n",
    "                    print(\"Memory: \", len(memory), \"epsilon: \", epsilon)\n",
    "                    print(\"Accuracy: \", accuracy)\n",
    "                    print(\"Time: \", time.time()-t)\n",
    "                    print(current_model)\n",
    "\n",
    "            losses.append(np.average(epoch_losses))\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        if save_path:\n",
    "            torch.save(current_model, save_path + \"model.pt\")\n",
    "\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.title(\"Average loss per epoch\")\n",
    "#     plt.ylabel(\"Loss\")\n",
    "#     plt.xlabel(\"Epochs\")\n",
    "#     plt.plot(losses)\n",
    "#     plt.show()\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.title(\"Accuracy\")\n",
    "#     plt.plot(accuracies)\n",
    "#     plt.show()\n",
    "    \n",
    "    return difficulty, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  101 Global steps:  1280\n",
      "Difficulty:  4 Max tries: 15\n",
      "Memory:  1280 epsilon:  0.9971331589287308\n",
      "Accuracy:  0.7810945273631841\n",
      "Time:  0.7226462364196777\n",
      "DuelingDQN(\n",
      "  (sharedLayers): ModuleList(\n",
      "    (0): Linear(in_features=144, out_features=68, bias=True)\n",
      "    (1): Linear(in_features=68, out_features=36, bias=True)\n",
      "  )\n",
      "  (adv1): Linear(in_features=36, out_features=516, bias=True)\n",
      "  (adv2): Linear(in_features=516, out_features=6, bias=True)\n",
      "  (v1): Linear(in_features=36, out_features=516, bias=True)\n",
      "  (v2): Linear(in_features=516, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000000\n",
    "experiments = []\n",
    "experiments.append([\"Final\", [64, 32, 512], True, True, True, 0.7, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: int(np.floor(1.0685518383979797**(x+1))), lambda x: int(np.floor(1.0580005645751953**(x+1))), lambda x: int(np.floor(1.0363082885742188**(x+1)))], 1.0, 10, 100, 'Sutskever'])\n",
    "\n",
    "for experiment in experiments:\n",
    "    t = time.time()\n",
    "    for i in range(1):\n",
    "        print(\"---\",experiment[0],\"---\")\n",
    "        print(experiment[1:])\n",
    "        architecture, duelingDQN, doubleDQN, prioritizedReplayMemory, alpha, memoryCapacity, lr, amsgrad, epochs, batch_size, gamma, capacity_increase, threshold, evaluation_frequency, tau, curriculum = experiment[1:]\n",
    "        difficulties = []\n",
    "        accuracies = []\n",
    "        result = []\n",
    "    \n",
    "        env = rubiks2.RubiksEnv2(2, unsolved_reward = -1.0, seed=42)\n",
    "        \n",
    "        difficulty, accuracy = train_with_settings(architecture, duelingDQN, doubleDQN, prioritizedReplayMemory, alpha, memoryCapacity, lr, amsgrad, epochs, batch_size, gamma, capacity_increase, threshold, evaluation_frequency, tau, curriculum, save_path='models/'+experiment[0]+str(i)+'/', seed=i, verbose=True)\n",
    "        difficulties.append(difficulty)\n",
    "        accuracies.append(accuracy)\n",
    "        print(\"Difficulty reached:\", difficulty, \"with accuracy: \", accuracy)\n",
    "        diffs = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,1000]\n",
    "        for diff in diffs:\n",
    "            scrambles, ratio, avgtries = test('models/'+experiment[0]+str(i)+'/model.pt', diff)\n",
    "            result.append((scrambles, ratio, avgtries))\n",
    "        results.append(result)\n",
    "    print(time.time() - t)\n",
    "    print(results)\n",
    "    print(difficulties)\n",
    "    print(accuracies)\n",
    "    # 43 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture\n",
    "epochs = 10000\n",
    "experiments = []\n",
    "experiments.append([\"Baseline\", [4096, 2048, 512], True, True, True, 0.7, 100000, 1e-3, True, epochs, 128, 0.99, None, 0.95, 10, 100, 'Joe'])\n",
    "experiments.append([\"Exponential_all_layers\", [64, 32, 8], True, True, True, 0.7, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: int(np.floor(1.0286808013916016**(x+1))), lambda x: int(np.floor(1.0234394073486328**(x+1))), lambda x: int(np.floor(1.012664794921875**(x+1)))], 0.95, 10, 100, 'Joe'])\n",
    "experiments.append([\"Exponential_bottom_layers\", [64, 32, 512], True, True, True, 0.7, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: int(np.floor(1.0286808013916016**(x+1))), lambda x: int(np.floor(1.0234394073486328**(x+1))), lambda x: int(0)], 0.95, 10, 100, 'Joe'])\n",
    "experiments.append([\"Exponential_top_layer\", [4096, 2048, 512], True, True, True, 0.7, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: int(0), lambda x: int(0), lambda x: int(np.floor(1.012664794921875**(x+1)))], 0.95, 10, 100, 'Joe'])\n",
    "experiments.append([\"Linear_all_layers\", [64, 32, 8], True, True, True, 0.7, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: 24, lambda x: 12, lambda x: 3], 0.95, 10, 100, 'Joe'])\n",
    "experiments.append([\"Linear_bottom_layers\", [64, 32, 512], True, True, True, 0.7, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: 24, lambda x: 12, lambda x: 0], 0.95, 10, 100, 'Joe'])\n",
    "experiments.append([\"Linear_top_layer\", [4096, 2048, 8], True, True, True, 0.7, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: 0, lambda x: 0, lambda x: 3], 0.95, 10, 100, 'Joe'])\n",
    "results = []\n",
    "\n",
    "for experiment in experiments:\n",
    "    t = time.time()\n",
    "    for i in range(5):\n",
    "        print(\"---\",experiment[0],\"---\")\n",
    "        print(experiment[1:])\n",
    "        architecture, duelingDQN, doubleDQN, prioritizedReplayMemory, alpha, memoryCapacity, lr, amsgrad, epochs, batch_size, gamma, capacity_increase, threshold, evaluation_frequency, tau, curriculum = experiment[1:]\n",
    "        difficulties = []\n",
    "        accuracies = []\n",
    "        result = []\n",
    "    \n",
    "        env = rubiks2.RubiksEnv2(2, unsolved_reward = -1.0, seed=i)\n",
    "        \n",
    "        difficulty, accuracy = train_with_settings(architecture, duelingDQN, doubleDQN, prioritizedReplayMemory, alpha, memoryCapacity, lr, amsgrad, epochs, batch_size, gamma, capacity_increase, threshold, evaluation_frequency, tau, curriculum, save_path='models/'+experiment[0]+str(i)+'/', seed=i)\n",
    "        difficulties.append(difficulty)\n",
    "        accuracies.append(accuracy)\n",
    "        print(\"Difficulty reached:\", difficulty, \"with accuracy: \", accuracy)\n",
    "        diffs = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,1000]\n",
    "        for diff in diffs:\n",
    "            scrambles, ratio, avgtries = test('models/'+experiment[0]+str(i)+'/model.pt', diff)\n",
    "            result.append((scrambles, ratio, avgtries))\n",
    "        results.append(result)\n",
    "    print(time.time() - t)\n",
    "    print(results)\n",
    "    print(difficulties)\n",
    "    print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = []\n",
    "experiments.append([\"init_final_little_smaller\", [32, 16, 384], True, True, True, 0.7, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: int(np.floor(1.0265579223632812**(x+1))), lambda x: int(np.floor(1.0212650299072266**(x+1))), lambda x: 0], 0.95, 10, 100, 'Joe'])\n",
    "experiments.append([\"final_little_smaller\", [64, 32, 384], True, True, True, 0.7, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: int(np.floor(1.0264892578125**(x+1))), lambda x: int(np.floor(1.0212106704711914**(x+1))), lambda x: 0], 0.95, 10, 100, 'Joe'])\n",
    "experiments.append([\"final_smaller\", [64, 32, 256], True, True, True, 0.7, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: int(np.floor(1.0233228206634521**(x+1))), lambda x: int(np.floor(1.017974853515625**(x+1))), lambda x: 0], 0.95, 10, 100, 'Joe'])\n",
    "experiments.append([\"init_final_smaller\", [32, 16, 256], True, True, True, 0.7, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: int(np.floor(1.0234394073486328**(x+1))), lambda x: int(np.floor(1.0180950164794922**(x+1))), lambda x: 0], 0.95, 10, 100, 'Joe'])\n",
    "experiments.append([\"Init_smaller\", [32, 16, 256], True, True, True, 0.7, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: int(np.floor(1.028738021850586**(x+1))), lambda x: int(np.floor(1.0234909057617188**(x+1))), lambda x: 0], 0.95, 10, 100, 'Joe'])\n",
    "experiments.append([\"Final\", [64, 32, 512], True, True, True, 0.7, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: int(np.floor(1.0286808013916016**(x+1))), lambda x: int(np.floor(1.0234394073486328**(x+1))), lambda x: 0], 0.95, 10, 100, 'Joe'])\n",
    "\n",
    "results = []\n",
    "\n",
    "for experiment in experiments:\n",
    "    t = time.time()\n",
    "    for i in range(5):\n",
    "        print(\"---\",experiment[0],\"---\")\n",
    "        print(experiment[1:])\n",
    "        architecture, duelingDQN, doubleDQN, prioritizedReplayMemory, alpha, memoryCapacity, lr, amsgrad, epochs, batch_size, gamma, capacity_increase, threshold, evaluation_frequency, tau, curriculum = experiment[1:]\n",
    "        difficulties = []\n",
    "        accuracies = []\n",
    "        result = []\n",
    "    \n",
    "        env = rubiks.RubiksEnv(2, unsolved_reward = -1.0, seed=i)\n",
    "        \n",
    "        difficulty, accuracy = train_with_settings(architecture, duelingDQN, doubleDQN, prioritizedReplayMemory, alpha, memoryCapacity, lr, amsgrad, epochs, batch_size, gamma, capacity_increase, threshold, evaluation_frequency, tau, curriculum, save_path='models/'+experiment[0]+str(i)+'/', seed=i)\n",
    "        difficulties.append(difficulty)\n",
    "        accuracies.append(accuracy)\n",
    "        print(\"Difficulty reached:\", difficulty, \"with accuracy: \", accuracy)\n",
    "        diffs = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,1000]\n",
    "        for diff in diffs:\n",
    "            scrambles, ratio, avgtries = test('models/'+experiment[0]+str(i)+'/model.pt', diff)\n",
    "            result.append((scrambles, ratio, avgtries))\n",
    "        results.append(result)\n",
    "    print(time.time() - t)\n",
    "    print(results)\n",
    "    print(difficulties)\n",
    "    print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curriculum\n",
    "epochs = 5000\n",
    "experiments = []\n",
    "experiments.append([\"Joe_Method\", [64, 32, 8], True, True, True, 0.7, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: int(np.floor(1.0286808013916016**(x+1))), lambda x: int(np.floor(1.0234394073486328**(x+1))), lambda x: int(np.floor(1.012664794921875**(x+1)))], 0.95, 10, 100, 'Joe'])\n",
    "experiments.append([\"Naive_Method\", [64, 32, 8], True, True, True, 0.7, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: int(np.floor(1.0286808013916016**(x+1))), lambda x: int(np.floor(1.0234394073486328**(x+1))), lambda x: int(np.floor(1.012664794921875**(x+1)))], 0.95, 10, 100, 'naive'])\n",
    "experiments.append([\"Mixed_Method\", [64, 32, 8], True, True, True, 0.7, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: int(np.floor(1.0286808013916016**(x+1))), lambda x: int(np.floor(1.0234394073486328**(x+1))), lambda x: int(np.floor(1.012664794921875**(x+1)))], 0.95, 10, 100, 'mixed'])\n",
    "experiments.append([\"Sutskever_Method\", [64, 32, 8], True, True, True, 0.7, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: int(np.floor(1.0286808013916016**(x+1))), lambda x: int(np.floor(1.0234394073486328**(x+1))), lambda x: int(np.floor(1.012664794921875**(x+1)))], 0.95, 10, 100, 'Sutskever'])\n",
    "\n",
    "for experiment in experiments:\n",
    "    t = time.time()\n",
    "    print(\"---\",experiment[0],\"---\")\n",
    "    print(experiment[1:])\n",
    "    architecture, duelingDQN, doubleDQN, prioritizedReplayMemory, alpha, memoryCapacity, lr, amsgrad, epochs, batch_size, gamma, capacity_increase, threshold, evaluation_frequency, tau, curriculum = experiment[1:]\n",
    "    difficulties = []\n",
    "    accuracies = []\n",
    "    for i in range(10):\n",
    "        env = rubiks.RubiksEnv(2, unsolved_reward = -1.0, seed=i)\n",
    "        \n",
    "        difficulty, accuracy = train_with_settings(architecture, duelingDQN, doubleDQN, prioritizedReplayMemory, alpha, memoryCapacity, lr, amsgrad, epochs, batch_size, gamma, capacity_increase, threshold, evaluation_frequency, tau, curriculum, save_path='models/'+experiment[0]+str(i)+'/', seed=i)\n",
    "        difficulties.append(difficulty)\n",
    "        accuracies.append(accuracy)\n",
    "        print(\"Difficulty reached:\", difficulty, \"with accuracy: \", accuracy)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 1)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 2)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 3)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 4)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 5)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 6)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 7)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 8)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 9)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 10)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 11)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 12)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 13)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 14)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treshold\n",
    "experiments = []\n",
    "experiments.append([\"Threshold_1.0\", [64, 32, 8], True, True, True, 0.7, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: int(np.floor(1.0286808013916016**(x+1))), lambda x: int(np.floor(1.0234394073486328**(x+1))), lambda x: int(np.floor(1.012664794921875**(x+1)))], 1.0, 10, 100, 'Joe'])\n",
    "experiments.append([\"Threshold_0.95\", [64, 32, 8], True, True, True, 0.7, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: int(np.floor(1.0286808013916016**(x+1))), lambda x: int(np.floor(1.0234394073486328**(x+1))), lambda x: int(np.floor(1.012664794921875**(x+1)))], 0.95, 10, 100, 'Joe'])\n",
    "experiments.append([\"Threshold_0.9\", [64, 32, 8], True, True, True, 0.7, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: int(np.floor(1.0286808013916016**(x+1))), lambda x: int(np.floor(1.0234394073486328**(x+1))), lambda x: int(np.floor(1.012664794921875**(x+1)))], 0.9, 10, 100, 'Joe'])\n",
    "\n",
    "for experiment in experiments:\n",
    "    print(\"---\",experiment[0],\"---\")\n",
    "    print(experiment[1:])\n",
    "    architecture, duelingDQN, doubleDQN, prioritizedReplayMemory, alpha, memoryCapacity, lr, amsgrad, epochs, batch_size, gamma, capacity_increase, threshold, evaluation_frequency, tau, curriculum = experiment[1:]\n",
    "    difficulties = []\n",
    "    accuracies = []\n",
    "    for i in range(1):\n",
    "        env = rubiks.RubiksEnv(2, unsolved_reward = -1.0, seed=i)\n",
    "        \n",
    "        difficulty, accuracy = train_with_settings(architecture, duelingDQN, doubleDQN, prioritizedReplayMemory, alpha, memoryCapacity, lr, amsgrad, epochs, batch_size, gamma, capacity_increase, threshold, evaluation_frequency, tau, curriculum, save_path='models/'+experiment[0]+str(i)+'/', seed=i)\n",
    "        difficulties.append(difficulty)\n",
    "        accuracies.append(accuracy)\n",
    "        print(\"Difficulty reached:\", difficulty, \"with accuracy: \", accuracy)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 1)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 2)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 3)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 4)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 5)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 6)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 7)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 8)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 9)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 10)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 11)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 12)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 13)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 14)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Priority\n",
    "epochs = 5000\n",
    "experiments = []\n",
    "experiments.append([\"Priority_0.5\", [64, 32, 8], True, True, True, 0.5, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: int(np.floor(1.0286808013916016**(x+1))), lambda x: int(np.floor(1.0234394073486328**(x+1))), lambda x: int(np.floor(1.012664794921875**(x+1)))], 0.95, 10, 100, 'Joe'])\n",
    "experiments.append([\"Priority_0.7\", [64, 32, 8], True, True, True, 0.7, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: int(np.floor(1.0286808013916016**(x+1))), lambda x: int(np.floor(1.0234394073486328**(x+1))), lambda x: int(np.floor(1.012664794921875**(x+1)))], 0.95, 10, 100, 'Joe'])\n",
    "experiments.append([\"Priority_0.9\", [64, 32, 8], True, True, True, 0.9, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: int(np.floor(1.0286808013916016**(x+1))), lambda x: int(np.floor(1.0234394073486328**(x+1))), lambda x: int(np.floor(1.012664794921875**(x+1)))], 0.95, 10, 100, 'Joe'])\n",
    "experiments.append([\"Priority_0.99\", [64, 32, 8], True, True, True, 0.99, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: int(np.floor(1.0286808013916016**(x+1))), lambda x: int(np.floor(1.0234394073486328**(x+1))), lambda x: int(np.floor(1.012664794921875**(x+1)))], 0.95, 10, 100, 'Joe'])\n",
    "\n",
    "for experiment in experiments:\n",
    "    print(\"---\",experiment[0],\"---\")\n",
    "    print(experiment[1:])\n",
    "    architecture, duelingDQN, doubleDQN, prioritizedReplayMemory, alpha, memoryCapacity, lr, amsgrad, epochs, batch_size, gamma, capacity_increase, threshold, evaluation_frequency, tau, curriculum = experiment[1:]\n",
    "    difficulties = []\n",
    "    accuracies = []\n",
    "    for i in range(10):\n",
    "        env = rubiks.RubiksEnv(2, unsolved_reward = -1.0, seed=i)\n",
    "        \n",
    "        difficulty, accuracy = train_with_settings(architecture, duelingDQN, doubleDQN, prioritizedReplayMemory, alpha, memoryCapacity, lr, amsgrad, epochs, batch_size, gamma, capacity_increase, threshold, evaluation_frequency, tau, curriculum, save_path='models/'+experiment[0]+str(i)+'/', seed=i)\n",
    "        difficulties.append(difficulty)\n",
    "        accuracies.append(accuracy)\n",
    "        print(\"Difficulty reached:\", difficulty, \"with accuracy: \", accuracy)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 1)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 2)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 3)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 4)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 5)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 6)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 7)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 8)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 9)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 10)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 11)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 12)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 13)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 14)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double Dueling ablations\n",
    "epochs = 5000\n",
    "experiments = []\n",
    "experiments.append([\"Dueling_off\", [64, 32, 8], False, True, True, 0.5, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: int(np.floor(1.0286808013916016**(x+1))), lambda x: int(np.floor(1.0234394073486328**(x+1))), lambda x: int(np.floor(1.012664794921875**(x+1)))], 0.95, 10, 100, 'Joe'])\n",
    "experiments.append([\"Double_off\", [64, 32, 8], True, False, True, 0.5, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: int(np.floor(1.0286808013916016**(x+1))), lambda x: int(np.floor(1.0234394073486328**(x+1))), lambda x: int(np.floor(1.012664794921875**(x+1)))], 0.95, 10, 100, 'Joe'])\n",
    "experiments.append([\"DuelingDouble_off\", [64, 32, 8], False, False, True, 0.5, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: int(np.floor(1.0286808013916016**(x+1))), lambda x: int(np.floor(1.0234394073486328**(x+1))), lambda x: int(np.floor(1.012664794921875**(x+1)))], 0.95, 10, 100, 'Joe'])\n",
    "\n",
    "\n",
    "for experiment in experiments:\n",
    "    print(\"---\",experiment[0],\"---\")\n",
    "    print(experiment[1:])\n",
    "    architecture, duelingDQN, doubleDQN, prioritizedReplayMemory, alpha, memoryCapacity, lr, amsgrad, epochs, batch_size, gamma, capacity_increase, threshold, evaluation_frequency, tau, curriculum = experiment[1:]\n",
    "    difficulties = []\n",
    "    accuracies = []\n",
    "    for i in range(10):\n",
    "        env = rubiks.RubiksEnv(2, unsolved_reward = -1.0, seed=i)\n",
    "        \n",
    "        difficulty, accuracy = train_with_settings(architecture, duelingDQN, doubleDQN, prioritizedReplayMemory, alpha, memoryCapacity, lr, amsgrad, epochs, batch_size, gamma, capacity_increase, threshold, evaluation_frequency, tau, curriculum, save_path='models/'+experiment[0]+str(i)+'/', seed=i)\n",
    "        difficulties.append(difficulty)\n",
    "        accuracies.append(accuracy)\n",
    "        print(\"Difficulty reached:\", difficulty, \"with accuracy: \", accuracy)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 1)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 2)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 3)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 4)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 5)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 6)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 7)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 8)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 9)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 10)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 11)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 12)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 13)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 14)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gamma\n",
    "epochs = 5000\n",
    "experiments = []\n",
    "experiments.append([\"Gamma_0.99\", [64, 32, 8], True, False, True, 0.5, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: int(np.floor(1.0286808013916016**(x+1))), lambda x: int(np.floor(1.0234394073486328**(x+1))), lambda x: int(np.floor(1.012664794921875**(x+1)))], 0.95, 10, 100, 'Joe'])\n",
    "experiments.append([\"Gamma_0.95\", [64, 32, 8], True, False, True, 0.5, 100000, 1e-3, True, epochs, 128, 0.95, [lambda x: int(np.floor(1.0286808013916016**(x+1))), lambda x: int(np.floor(1.0234394073486328**(x+1))), lambda x: int(np.floor(1.012664794921875**(x+1)))], 0.95, 10, 100, 'Joe'])\n",
    "experiments.append([\"Gamma_0.9\", [64, 32, 8], True, False, True, 0.5, 100000, 1e-3, True, epochs, 128, 0.9, [lambda x: int(np.floor(1.0286808013916016**(x+1))), lambda x: int(np.floor(1.0234394073486328**(x+1))), lambda x: int(np.floor(1.012664794921875**(x+1)))], 0.95, 10, 100, 'Joe'])\n",
    "experiments.append([\"Gamma_0.8\", [64, 32, 8], True, False, True, 0.5, 100000, 1e-3, True, epochs, 128, 0.8, [lambda x: int(np.floor(1.0286808013916016**(x+1))), lambda x: int(np.floor(1.0234394073486328**(x+1))), lambda x: int(np.floor(1.012664794921875**(x+1)))], 0.95, 10, 100, 'Joe'])\n",
    "\n",
    "for experiment in experiments:\n",
    "    print(\"---\",experiment[0],\"---\")\n",
    "    print(experiment[1:])\n",
    "    architecture, duelingDQN, doubleDQN, prioritizedReplayMemory, alpha, memoryCapacity, lr, amsgrad, epochs, batch_size, gamma, capacity_increase, threshold, evaluation_frequency, tau, curriculum = experiment[1:]\n",
    "    difficulties = []\n",
    "    accuracies = []\n",
    "    for i in range(10):\n",
    "        env = rubiks.RubiksEnv(2, unsolved_reward = -1.0, seed=i)\n",
    "        \n",
    "        difficulty, accuracy = train_with_settings(architecture, duelingDQN, doubleDQN, prioritizedReplayMemory, alpha, memoryCapacity, lr, amsgrad, epochs, batch_size, gamma, capacity_increase, threshold, evaluation_frequency, tau, curriculum, save_path='models/'+experiment[0]+str(i)+'/', seed=i)\n",
    "        difficulties.append(difficulty)\n",
    "        accuracies.append(accuracy)\n",
    "        print(\"Difficulty reached:\", difficulty, \"with accuracy: \", accuracy)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 1)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 2)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 3)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 4)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 5)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 6)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 7)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 8)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 9)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 10)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 11)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 12)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 13)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 14)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update time\n",
    "epochs = 5000\n",
    "experiments = []\n",
    "experiments.append([\"10_10\", [64, 32, 8], True, False, True, 0.5, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: int(np.floor(1.0286808013916016**(x+1))), lambda x: int(np.floor(1.0234394073486328**(x+1))), lambda x: int(np.floor(1.012664794921875**(x+1)))], 0.95, 10, 10, 'Joe'])\n",
    "\n",
    "experiments.append([\"100_100\", [64, 32, 8], True, False, True, 0.5, 100000, 1e-3, True, epochs, 128, 0.95, [lambda x: int(np.floor(1.0286808013916016**(x+1))), lambda x: int(np.floor(1.0234394073486328**(x+1))), lambda x: int(np.floor(1.012664794921875**(x+1)))], 0.95, 100, 100, 'Joe'])\n",
    "experiments.append([\"100_1000\", [64, 32, 8], True, False, True, 0.5, 100000, 1e-3, True, epochs, 128, 0.9, [lambda x: int(np.floor(1.0286808013916016**(x+1))), lambda x: int(np.floor(1.0234394073486328**(x+1))), lambda x: int(np.floor(1.012664794921875**(x+1)))], 0.95, 100, 1000, 'Joe'])\n",
    "experiments.append([\"10_1000\", [64, 32, 8], True, False, True, 0.5, 100000, 1e-3, True, epochs, 128, 0.8, [lambda x: int(np.floor(1.0286808013916016**(x+1))), lambda x: int(np.floor(1.0234394073486328**(x+1))), lambda x: int(np.floor(1.012664794921875**(x+1)))], 0.95, 10, 1000, 'Joe'])\n",
    "experiments.append([\"b10_100\", [64, 32, 8], True, False, True, 0.5, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: int(np.floor(1.0286808013916016**(x+1))), lambda x: int(np.floor(1.0234394073486328**(x+1))), lambda x: int(np.floor(1.012664794921875**(x+1)))], 0.95, 10, 100, 'Joe'])\n",
    "\n",
    "for experiment in experiments:\n",
    "    print(\"---\",experiment[0],\"---\")\n",
    "    print(experiment[1:])\n",
    "    architecture, duelingDQN, doubleDQN, prioritizedReplayMemory, alpha, memoryCapacity, lr, amsgrad, epochs, batch_size, gamma, capacity_increase, threshold, evaluation_frequency, tau, curriculum = experiment[1:]\n",
    "    difficulties = []\n",
    "    accuracies = []\n",
    "    for i in range(10):\n",
    "        env = rubiks.RubiksEnv(2, unsolved_reward = -1.0, seed=i)\n",
    "        \n",
    "        difficulty, accuracy = train_with_settings(architecture, duelingDQN, doubleDQN, prioritizedReplayMemory, alpha, memoryCapacity, lr, amsgrad, epochs, batch_size, gamma, capacity_increase, threshold, evaluation_frequency, tau, curriculum, save_path='models/'+experiment[0]+str(i)+'/', seed=i)\n",
    "        difficulties.append(difficulty)\n",
    "        accuracies.append(accuracy)\n",
    "        print(\"Difficulty reached:\", difficulty, \"with accuracy: \", accuracy)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 1)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 2)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 3)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 4)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 5)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 6)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 7)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 8)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 9)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 10)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 11)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 12)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 13)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 14)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5000\n",
    "experiments = []\n",
    "experiments.append([\"init_final_little_smaller\", [32, 16, 4], True, False, True, 0.5, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: int(np.floor(1.0265579223632812**(x+1))), lambda x: int(np.floor(1.0212650299072266**(x+1))), lambda x: int(np.floor(1.0104103088378906**(x+1)))], 0.95, 10, 100, 'Joe'])\n",
    "experiments.append([\"final_little_smaller\", [64, 32, 8], True, False, True, 0.5, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: int(np.floor(1.0264892578125**(x+1))), lambda x: int(np.floor(1.0212106704711914**(x+1))), lambda x: int(np.floor(1.0103216171264648**(x+1)))], 0.95, 10, 100, 'Joe'])\n",
    "experiments.append([\"final_smaller\", [64, 32, 8], True, False, True, 0.5, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: int(np.floor(1.0233228206634521**(x+1))), lambda x: int(np.floor(1.017974853515625**(x+1))), lambda x: int(np.floor(1.006988525390625**(x+1)))], 0.95, 10, 100, 'Joe'])\n",
    "experiments.append([\"init_final_smaller\", [32, 16, 4], True, False, True, 0.5, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: int(np.floor(1.0234394073486328**(x+1))), lambda x: int(np.floor(1.0180950164794922**(x+1))), lambda x: int(np.floor(1.0071029663085938**(x+1)))], 0.95, 10, 100, 'Joe'])\n",
    "experiments.append([\"Init_smaller\", [32, 16, 4], True, False, True, 0.5, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: int(np.floor(1.028738021850586**(x+1))), lambda x: int(np.floor(1.0234909057617188**(x+1))), lambda x: int(np.floor(1.0126991271972656**(x+1)))], 0.95, 10, 100, 'Joe'])\n",
    "experiments.append([\"Final\", [64, 32, 8], True, False, True, 0.5, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: int(np.floor(1.0286808013916016**(x+1))), lambda x: int(np.floor(1.0234394073486328**(x+1))), lambda x: int(np.floor(1.012664794921875**(x+1)))], 0.95, 10, 100, 'Joe'])\n",
    "\n",
    "for experiment in experiments:\n",
    "    print(\"---\",experiment[0],\"---\")\n",
    "    print(experiment[1:])\n",
    "    architecture, duelingDQN, doubleDQN, prioritizedReplayMemory, alpha, memoryCapacity, lr, amsgrad, epochs, batch_size, gamma, capacity_increase, threshold, evaluation_frequency, tau, curriculum = experiment[1:]\n",
    "    difficulties = []\n",
    "    accuracies = []\n",
    "    for i in range(10):\n",
    "        env = rubiks.RubiksEnv(2, unsolved_reward = -1.0, seed=i)\n",
    "        \n",
    "        difficulty, accuracy = train_with_settings(architecture, duelingDQN, doubleDQN, prioritizedReplayMemory, alpha, memoryCapacity, lr, amsgrad, epochs, batch_size, gamma, capacity_increase, threshold, evaluation_frequency, tau, curriculum, save_path='models/'+experiment[0]+str(i)+'/', seed=i)\n",
    "        difficulties.append(difficulty)\n",
    "        accuracies.append(accuracy)\n",
    "        print(\"Difficulty reached:\", difficulty, \"with accuracy: \", accuracy)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 1)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 2)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 3)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 4)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 5)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 6)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 7)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 8)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 9)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 10)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 11)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 12)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 13)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 14)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final\n",
    "epochs = 50000000\n",
    "experiments = []\n",
    "experiments.append([\"Final\", [64, 32, 8], True, False, True, 0.5, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: int(np.floor(1.0286808013916016**(x+1))), lambda x: int(np.floor(1.0234394073486328**(x+1))), lambda x: int(np.floor(1.012664794921875**(x+1)))], 0.95, 10, 100, 'Joe'])\n",
    "\n",
    "for experiment in experiments:\n",
    "    print(\"---\",experiment[0],\"---\")\n",
    "    print(experiment[1:])\n",
    "    architecture, duelingDQN, doubleDQN, prioritizedReplayMemory, alpha, memoryCapacity, lr, amsgrad, epochs, batch_size, gamma, capacity_increase, threshold, evaluation_frequency, tau, curriculum = experiment[1:]\n",
    "    difficulties = []\n",
    "    accuracies = []\n",
    "    for i in range(1):\n",
    "        env = rubiks.RubiksEnv(2, unsolved_reward = -1.0, seed=i)\n",
    "        \n",
    "        difficulty, accuracy = train_with_settings(architecture, duelingDQN, doubleDQN, prioritizedReplayMemory, alpha, memoryCapacity, lr, amsgrad, epochs, batch_size, gamma, capacity_increase, threshold, evaluation_frequency, tau, curriculum, save_path='models/'+experiment[0]+str(i)+'/', seed=42, verbose=True)\n",
    "        difficulties.append(difficulty)\n",
    "        accuracies.append(accuracy)\n",
    "        print(\"Difficulty reached:\", difficulty, \"with accuracy: \", accuracy)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 1)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 2)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 3)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 4)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 5)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 6)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 7)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 8)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 9)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 10)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 11)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 12)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 13)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 14)\n",
    "        test('models/'+experiment[0]+str(i)+'/model.pt', 1000)\n",
    "        \n",
    "# 67 0.34 25000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, scrambles, seed=42):\n",
    "    env = rubiks2.RubiksEnv2(2, unsolved_reward = -1.0, seed=seed)\n",
    "    test_model = torch.load(model)\n",
    "    test_model.to(device)\n",
    "    \n",
    "    number_of_tests = 100\n",
    "    \n",
    "    max_tries = 1000\n",
    "\n",
    "    solved = 0\n",
    "    unsolved = 0\n",
    "    tries_solved = []\n",
    "    \n",
    "    for i in range(number_of_tests):\n",
    "        state = env.reset(scrambles)\n",
    "        done = 0\n",
    "        tries = 0\n",
    "\n",
    "        while tries < max_tries and not done:\n",
    "            action = test_model.act(state, 0)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "\n",
    "            state = next_state\n",
    "            tries +=1\n",
    "\n",
    "        if done:\n",
    "            solved += 1\n",
    "            tries_solved.append(tries)\n",
    "        else:\n",
    "            unsolved += 1\n",
    "    \n",
    "    print(scrambles, ' times scrambled')\n",
    "    print('Solved: ', solved, 'Unsolved: ', unsolved)\n",
    "    print('Average tries: ', np.average(tries_solved))\n",
    "    return scrambles, solved/100, np.average(tries_solved)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
