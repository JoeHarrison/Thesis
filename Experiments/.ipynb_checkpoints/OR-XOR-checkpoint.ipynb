{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import copy\n",
    "from networks import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n",
      "Torch Version:  1.0.1.post2\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)\n",
    "print(\"Torch Version: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increase_capacity_keep_lr(network, capacity, optimizer, device):\n",
    "    # Store old ids\n",
    "    old_ids = [id(p) for p in network.parameters()]\n",
    "    old_param_sizes = [p.size() for p in network.parameters()]\n",
    "\n",
    "    network.increase_capacity(capacity)\n",
    "\n",
    "    # Store new ids\n",
    "    new_ids = [id(p) for p in network.parameters()]\n",
    "    new_param_sizes = [p.size() for p in network.parameters()]\n",
    "\n",
    "    # Store old state \n",
    "    opt_state_dict = optimizer.state_dict()\n",
    "    for old_id, new_id, new_param_size, old_param_size in zip(old_ids, new_ids, new_param_sizes, old_param_sizes):\n",
    "        # Store step, and exp_avgs\n",
    "        step = opt_state_dict['state'][old_id]['step']\n",
    "        old_exp_avg = opt_state_dict['state'][old_id]['exp_avg']\n",
    "        old_exp_avg_sq = opt_state_dict['state'][old_id]['exp_avg_sq']\n",
    "        old_max_exp_avg_sq = opt_state_dict['state'][old_id]['max_exp_avg_sq']\n",
    "\n",
    "        exp_avg = torch.zeros(new_param_size)\n",
    "        exp_avg_sq = torch.zeros(new_param_size)\n",
    "        max_exp_avg_sq =  torch.zeros(new_param_size)\n",
    "        # Extend exp_avgs to new shape depending on wether param is bias or weight\n",
    "        if exp_avg.dim()>1:\n",
    "            # Weights\n",
    "            exp_avg[0:old_param_size[0],0:old_param_size[1]] = old_exp_avg\n",
    "            exp_avg_sq[0:old_param_size[0],0:old_param_size[1]] = old_exp_avg_sq\n",
    "            max_exp_avg_sq[0:old_param_size[0],0:old_param_size[1]] = old_max_exp_avg_sq\n",
    "        else:\n",
    "            # Biases/last layer\n",
    "            exp_avg[0:old_param_size[0]] = old_exp_avg\n",
    "            exp_avg_sq[0:old_param_size[0]] = old_exp_avg_sq\n",
    "            max_exp_avg_sq[0:old_param_size[0]] = old_max_exp_avg_sq\n",
    "        \n",
    "        # Delete old id from state_dict and update new params and new id\n",
    "        del opt_state_dict['state'][old_id]\n",
    "        opt_state_dict['state'][new_id] = {\n",
    "            'step': step,\n",
    "            'exp_avg': exp_avg,\n",
    "            'exp_avg_sq': exp_avg_sq.to(device),\n",
    "            'max_exp_avg_sq' : max_exp_avg_sq.to(device)\n",
    "        }\n",
    "        opt_state_dict['param_groups'][0]['params'].remove(old_id)\n",
    "        opt_state_dict['param_groups'][0]['params'].append(new_id)\n",
    "\n",
    "    network.to(device)\n",
    "    optimizer = optim.Adam(network.parameters(), amsgrad=True)\n",
    "    optimizer.load_state_dict(opt_state_dict)\n",
    "    \n",
    "    return network, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_zero():\n",
    "    return random.uniform(0, 49) / 100\n",
    "\n",
    "def generate_one():\n",
    "    return random.uniform(50, 100) / 100\n",
    "\n",
    "def generate_both(num_data_points, p):\n",
    "    Xs, Ys = [], []\n",
    "    for _ in range(num_data_points):\n",
    "        if random.random() < p:\n",
    "            Xs.append([generate_zero(), generate_zero(), 0]); Ys.append([0])\n",
    "            # or(1, 0) -> 1\n",
    "            Xs.append([generate_one(), generate_zero(), 0]); Ys.append([1])\n",
    "            # or(0, 1) -> 1\n",
    "            Xs.append([generate_zero(), generate_one(), 0]); Ys.append([1])\n",
    "            # or(1, 1) -> 1\n",
    "            Xs.append([generate_one(), generate_one(), 0]); Ys.append([1])\n",
    "        else:\n",
    "            # xor(0, 0) -> 0\n",
    "            Xs.append([generate_zero(), generate_zero(), 1]); Ys.append([0])\n",
    "            # xor(1, 0) -> 1\n",
    "            Xs.append([generate_one(), generate_zero(), 1]); Ys.append([1])\n",
    "            # xor(0, 1) -> 1\n",
    "            Xs.append([generate_zero(), generate_one(), 1]); Ys.append([1])\n",
    "            # xor(1, 1) -> 0\n",
    "            Xs.append([generate_one(), generate_one(), 1]); Ys.append([0])\n",
    "    return Xs, Ys\n",
    "\n",
    "def generate_or_XY(num_data_points):\n",
    "    Xs, Ys = [], []\n",
    "    for _ in range(num_data_points):\n",
    "        # or(0, 0) -> 0 \n",
    "        Xs.append([generate_zero(), generate_zero(), 0]); Ys.append([0])\n",
    "        # or(1, 0) -> 1\n",
    "        Xs.append([generate_one(), generate_zero(), 0]); Ys.append([1])\n",
    "        # or(0, 1) -> 1\n",
    "        Xs.append([generate_zero(), generate_one(), 0]); Ys.append([1])\n",
    "        # or(1, 1) -> 1\n",
    "        Xs.append([generate_one(), generate_one(), 0]); Ys.append([1])\n",
    "    return Xs, Ys\n",
    "\n",
    "def generate_xor_XY(num_data_points):\n",
    "    Xs, Ys = [], []\n",
    "    for _ in range(num_data_points):\n",
    "        # xor(0, 0) -> 0 \n",
    "        Xs.append([generate_zero(), generate_zero(), 1]); Ys.append([0])\n",
    "        # xor(1, 0) -> 1\n",
    "        Xs.append([generate_one(), generate_zero(), 1]); Ys.append([1])\n",
    "        # xor(0, 1) -> 1\n",
    "        Xs.append([generate_zero(), generate_one(), 1]); Ys.append([1])\n",
    "        # xor(1, 1) -> 0\n",
    "        Xs.append([generate_one(), generate_one(), 1]); Ys.append([0])\n",
    "    return Xs, Ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 2] True [3, 1]\n",
      "Average loss or before xor training:  0.30647775530815125\n",
      "Average loss xor before xor training:  0.36558668315410614\n",
      "Average loss or before xor training:  0.9143003523349762\n",
      "Average loss xor before xor training:  0.9034073650836945\n",
      "Average loss or:  0.9684329926967621\n",
      "Average loss xor:  0.9590063989162445\n",
      "Average loss:  0.9637196958065033\n",
      "[0.9969995021820068, 0.9997853636741638, 0.9942136406898499, 1, [5, 2], True, [3, 1]]\n"
     ]
    }
   ],
   "source": [
    "def xor_experiments(initial_capacity, train_or, capacity, non_linearity):\n",
    "    lowest_loss = 0\n",
    "    lowest_settings = []\n",
    "    \n",
    "    pre_pre_loss_or = []\n",
    "    pre_pre_loss_xor = []\n",
    "    \n",
    "    pre_loss_or = []\n",
    "    pre_loss_xor = []\n",
    "    \n",
    "    losses_or = []\n",
    "    losses_xor = []\n",
    "    for seed in range(100):\n",
    "        # Set seeds\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        # Initialisation network\n",
    "        network = DQN(3, initial_capacity.copy(), 1, non_linearity)\n",
    "        optimizer = optim.Adam(network.parameters(), amsgrad=True)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        prediction = network(torch.tensor([[0,0,0],[0,1,0],[1,0,0],[1,1,0]], dtype=torch.float))\n",
    "        Ys = torch.tensor([[0],[1],[1],[1]], dtype=torch.float)\n",
    "        OR_loss = 1.0/(1.0+criterion(prediction, Ys))\n",
    "            \n",
    "        prediction = network(torch.tensor([[0,0,1],[0,1,1],[1,0,1],[1,1,1]], dtype=torch.float))\n",
    "        Ys = torch.tensor([[0],[1],[1],[0]], dtype=torch.float)\n",
    "        XOR_loss = 1.0/(1.0+criterion(prediction, Ys))\n",
    "            \n",
    "        pre_pre_loss_or.append(OR_loss.item())\n",
    "        pre_pre_loss_xor.append(XOR_loss.item())\n",
    "        \n",
    "        if train_or:\n",
    "            for i in range(127*1000):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                Xs, Ys = generate_both(25,0.1)\n",
    "                    \n",
    "                Xs = torch.tensor(Xs)\n",
    "                Ys = torch.tensor(Ys, dtype=torch.float)\n",
    "\n",
    "                prediction = network(Xs)\n",
    "                loss = criterion(prediction, Ys)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # Evaluation\n",
    "                    prediction = network(torch.tensor([[0,0,0],[0,1,0],[1,0,0],[1,1,0]], dtype=torch.float))\n",
    "                    Ys = torch.tensor([[0],[1],[1],[1]], dtype=torch.float)\n",
    "                    loss = 1.0/(1.0+criterion(prediction, Ys))\n",
    "\n",
    "                if loss>0.95:\n",
    "                    break\n",
    "                    \n",
    "        prediction = network(torch.tensor([[0,0,0],[0,1,0],[1,0,0],[1,1,0]], dtype=torch.float))\n",
    "        Ys = torch.tensor([[0],[1],[1],[1]], dtype=torch.float)\n",
    "        OR_loss = 1.0/(1.0+criterion(prediction, Ys))\n",
    "            \n",
    "        prediction = network(torch.tensor([[0,0,1],[0,1,1],[1,0,1],[1,1,1]], dtype=torch.float))\n",
    "        Ys = torch.tensor([[0],[1],[1],[0]], dtype=torch.float)\n",
    "        XOR_loss = 1.0/(1.0+criterion(prediction, Ys))\n",
    "            \n",
    "        pre_loss_or.append(OR_loss.item())\n",
    "        pre_loss_xor.append(XOR_loss.item())\n",
    "        \n",
    "        if capacity is not None:\n",
    "            network, optimizer = increase_capacity_keep_lr(network, capacity, optimizer, 'cpu')\n",
    "        \n",
    "        iters = (155-127)*1000\n",
    "        if not train_or:\n",
    "            (155) * 1000\n",
    "            \n",
    "        for i in range(iters):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Uniform syllabus 20% of the time\n",
    "            Xs, Ys = generate_both(25,0.9)\n",
    "                \n",
    "            Xs = torch.tensor(Xs)\n",
    "            Ys = torch.tensor(Ys, dtype=torch.float)\n",
    "\n",
    "            prediction = network(Xs)\n",
    "            loss = criterion(prediction, Ys)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Evaluation\n",
    "                prediction = network(torch.tensor([[0,0,1],[0,1,1],[1,0,1],[1,1,1]], dtype=torch.float))\n",
    "                Ys = torch.tensor([[0],[1],[1],[0]], dtype=torch.float)\n",
    "                loss = 1.0/(1.0+criterion(prediction, Ys))\n",
    "                \n",
    "            if loss>0.95:\n",
    "                break\n",
    "        \n",
    "        average_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Test or\n",
    "            prediction = network(torch.tensor([[0,0,0],[0,1,0],[1,0,0],[1,1,0]], dtype=torch.float))\n",
    "            Ys = torch.tensor([[0],[1],[1],[1]], dtype=torch.float)\n",
    "            loss = 1.0/(1.0+criterion(prediction, Ys))\n",
    "        \n",
    "        average_loss += loss.item()\n",
    "        losses_or.append(loss.item())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Test xor\n",
    "            prediction = network(torch.tensor([[0,0,1],[0,1,1],[1,0,1],[1,1,1]], dtype=torch.float))\n",
    "            Ys = torch.tensor([[0],[1],[1],[0]], dtype=torch.float)\n",
    "            loss = 1.0/(1.0+criterion(prediction, Ys))\n",
    "        \n",
    "        average_loss += loss.item()\n",
    "        average_loss /= 2\n",
    "        losses_xor.append(loss.item())\n",
    "        \n",
    "        if average_loss > lowest_loss:\n",
    "            lowest_loss = copy.copy(average_loss)\n",
    "            lowest_settings = [average_loss, losses_or[-1], losses_xor[-1], seed, initial_capacity, train_or, capacity]\n",
    "        \n",
    "        if loss>0.95:\n",
    "            break\n",
    "        \n",
    "    # Print statistics\n",
    "    print(initial_capacity, train_or, capacity)\n",
    "    \n",
    "    print('Average loss or before xor training: ', np.average(pre_pre_loss_or))\n",
    "    print('Average loss xor before xor training: ', np.average(pre_pre_loss_xor))\n",
    "    \n",
    "    print('Average loss or before xor training: ', np.average(pre_loss_or))\n",
    "    print('Average loss xor before xor training: ', np.average(pre_loss_xor))\n",
    "    print('Average loss or: ', np.average(losses_or))\n",
    "    print('Average loss xor: ', np.average(losses_xor))\n",
    "    print('Average loss: ', (np.average(losses_or) +  np.average(losses_xor))/2)\n",
    "    print(lowest_settings)\n",
    "\n",
    "xor_experiments([5,2],True,[3,1])\n",
    "# xor_experiments([2],False, None)\n",
    "# xor_experiments([2], True, None)\n",
    "# xor_experiments([1], True, [1])\n",
    "\n",
    "# xor_experiments([3],False,None)\n",
    "# xor_experiments([3], True, None)\n",
    "# xor_experiments([2], True, [1])\n",
    "# xor_experiments([1], True, [2])\n",
    "\n",
    "# xor_experiments([4],False,None)\n",
    "# xor_experiments([4], True, None)\n",
    "# xor_experiments([1], True, [3])\n",
    "# xor_experiments([2], True, [2])\n",
    "# xor_experiments([3], True, [1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Writer = animation.writers['ffmpeg']\n",
    "writer = Writer(fps=24, metadata=dict(artist='Joe Harrison'), bitrate=1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8552559614181519\n",
      "1 0.9450685381889343\n",
      "2 0.8479112982749939\n",
      "3 0.8452067971229553\n",
      "4 0.8431272506713867\n",
      "5 0.8419037461280823\n",
      "6 0.8403158783912659\n",
      "7 0.853485643863678\n",
      "8 0.8190130591392517\n",
      "9 0.8330321311950684\n",
      "10 0.8607609868049622\n",
      "11 0.8887335658073425\n",
      "12 0.8406065702438354\n",
      "13 0.8083235621452332\n",
      "14 0.8525921106338501\n",
      "15 0.8209795951843262\n",
      "16 0.8337359428405762\n",
      "17 0.8464990854263306\n",
      "18 0.8744804263114929\n",
      "19 0.8732001781463623\n",
      "20 0.8718239068984985\n",
      "21 0.8252975940704346\n",
      "22 0.8088929057121277\n",
      "23 0.8660187125205994\n",
      "24 0.8351905941963196\n",
      "25 0.8483043313026428\n",
      "26 0.8612344861030579\n",
      "27 0.801175594329834\n",
      "28 0.8433561325073242\n",
      "29 0.8272910714149475\n",
      "30 0.825671374797821\n",
      "31 0.8386083245277405\n",
      "32 0.8080980181694031\n",
      "33 0.8065598011016846\n",
      "34 0.8194347620010376\n",
      "35 0.8324432373046875\n",
      "36 0.8019487857818604\n",
      "37 0.8004421591758728\n",
      "38 0.7990315556526184\n",
      "39 0.7830743193626404\n",
      "40 0.7959089875221252\n",
      "41 0.8230062127113342\n",
      "42 0.8216138482093811\n",
      "43 0.8199988603591919\n",
      "44 0.8042615652084351\n",
      "45 0.8311848640441895\n",
      "46 0.8012425303459167\n",
      "47 0.7997298240661621\n",
      "48 0.7981914281845093\n",
      "49 0.7684083580970764\n",
      "50 0.7668850421905518\n",
      "51 0.7654378414154053\n",
      "52 0.7922197580337524\n",
      "53 0.8189395666122437\n",
      "54 0.7611661553382874\n",
      "55 0.7597345113754272\n",
      "56 0.8004187345504761\n",
      "57 0.7708949446678162\n",
      "58 0.7694916129112244\n",
      "59 0.7819969058036804\n",
      "60 0.7665785551071167\n",
      "61 0.7512184381484985\n",
      "62 0.805470883846283\n",
      "63 0.7901317477226257\n",
      "64 0.7608827948570251\n",
      "65 0.7733210325241089\n",
      "66 0.7857686877250671\n",
      "67 0.7842833995819092\n",
      "68 0.7966096997261047\n",
      "69 0.7400662302970886\n",
      "70 0.7661779522895813\n",
      "71 0.7372621297836304\n",
      "72 0.7908030152320862\n",
      "73 0.7619512677192688\n",
      "74 0.7467906475067139\n",
      "75 0.7591116428375244\n",
      "76 0.7440585494041443\n",
      "77 0.7290523648262024\n",
      "78 0.7684581875801086\n",
      "79 0.7263364195823669\n",
      "80 0.7385363578796387\n",
      "81 0.7100732326507568\n",
      "82 0.7493525743484497\n",
      "83 0.7614718675613403\n",
      "84 0.7466650009155273\n",
      "85 0.7721379995346069\n",
      "86 0.7438799142837524\n",
      "87 0.7558759450912476\n",
      "88 0.7410749197006226\n",
      "89 0.7397230863571167\n",
      "90 0.7116170525550842\n",
      "91 0.7236648797988892\n",
      "92 0.7356104850769043\n",
      "93 0.7209742665290833\n",
      "94 0.6930270195007324\n",
      "95 0.7050197720527649\n",
      "96 0.7037729024887085\n",
      "97 0.689204216003418\n",
      "98 0.7011690139770508\n",
      "99 0.7656254768371582\n",
      "100 0.711726725101471\n",
      "101 0.7102677226066589\n",
      "102 0.7220780253410339\n",
      "103 0.6945656538009644\n",
      "104 0.7444810271263123\n",
      "105 0.7175521850585938\n",
      "106 0.7155548334121704\n",
      "107 0.7390755414962769\n",
      "108 0.7002493739128113\n",
      "109 0.7108878493309021\n",
      "110 0.6849908232688904\n",
      "111 0.7073580026626587\n",
      "112 0.7061951160430908\n",
      "113 0.7039746642112732\n",
      "114 0.6675726771354675\n",
      "115 0.6886816620826721\n",
      "116 0.6756435632705688\n",
      "117 0.6724861860275269\n",
      "118 0.6926295757293701\n",
      "119 0.699185311794281\n",
      "120 0.662056565284729\n",
      "121 0.645723283290863\n",
      "122 0.6528359055519104\n",
      "123 0.6689386963844299\n",
      "124 0.663753092288971\n",
      "125 0.6276832818984985\n",
      "126 0.6644313931465149\n",
      "127 0.6588100790977478\n",
      "128 0.6443999409675598\n",
      "129 0.6581953167915344\n",
      "130 0.652580738067627\n",
      "131 0.605952262878418\n",
      "132 0.6100049614906311\n",
      "133 0.6046968102455139\n",
      "134 0.5881971716880798\n",
      "135 0.5934988856315613\n",
      "136 0.5967012643814087\n",
      "137 0.5815019011497498\n",
      "138 0.5840381383895874\n",
      "139 0.5884881615638733\n",
      "140 0.5561056137084961\n",
      "141 0.5792514085769653\n",
      "142 0.5719745755195618\n",
      "143 0.5370991826057434\n",
      "144 0.5418393015861511\n",
      "145 0.5358181595802307\n",
      "146 0.5395781993865967\n",
      "147 0.544011652469635\n",
      "148 0.5174129605293274\n",
      "149 0.5314580202102661\n",
      "150 0.5261061787605286\n",
      "151 0.5100818872451782\n",
      "152 0.48676368594169617\n",
      "153 0.4803326725959778\n",
      "154 0.475059449672699\n",
      "155 0.4774511456489563\n",
      "156 0.48138394951820374\n",
      "157 0.49543729424476624\n",
      "158 0.4714728593826294\n",
      "159 0.46458858251571655\n",
      "160 0.4579814076423645\n",
      "161 0.4537622034549713\n",
      "162 0.4485902488231659\n",
      "163 0.42646676301956177\n",
      "164 0.425601065158844\n",
      "165 0.43120282888412476\n",
      "166 0.4181354343891144\n",
      "167 0.4036388397216797\n",
      "168 0.3981570899486542\n",
      "169 0.4028848111629486\n",
      "170 0.417057067155838\n",
      "171 0.413498193025589\n",
      "172 0.40951937437057495\n",
      "173 0.38235464692115784\n",
      "174 0.3671515882015228\n",
      "175 0.36913344264030457\n",
      "176 0.3873949348926544\n",
      "177 0.3649671971797943\n",
      "178 0.349488765001297\n",
      "179 0.3539247512817383\n",
      "180 0.3556980788707733\n",
      "181 0.3362964391708374\n",
      "182 0.33168378472328186\n",
      "183 0.3423681855201721\n",
      "184 0.31935176253318787\n",
      "185 0.3372969925403595\n",
      "186 0.3121366500854492\n",
      "187 0.3210758864879608\n",
      "188 0.3386039733886719\n",
      "189 0.32944706082344055\n",
      "190 0.3105068504810333\n",
      "191 0.3197801113128662\n",
      "192 0.3002970516681671\n",
      "193 0.30646246671676636\n",
      "194 0.29466119408607483\n",
      "195 0.3038114905357361\n",
      "196 0.3022531569004059\n",
      "197 0.2907963693141937\n",
      "198 0.29175204038619995\n",
      "199 0.28554901480674744\n",
      "200 0.2894009053707123\n",
      "201 0.27849432826042175\n",
      "202 0.27746424078941345\n",
      "203 0.270408570766449\n",
      "204 0.27836763858795166\n",
      "205 0.2889975607395172\n",
      "206 0.2794093191623688\n",
      "207 0.27091875672340393\n",
      "208 0.2773537337779999\n",
      "209 0.2710680663585663\n",
      "210 0.2756192982196808\n",
      "211 0.2671191096305847\n",
      "212 0.26376646757125854\n",
      "213 0.2613224387168884\n",
      "214 0.26636770367622375\n",
      "215 0.26843568682670593\n",
      "216 0.26974064111709595\n",
      "217 0.27359071373939514\n",
      "218 0.27169618010520935\n",
      "219 0.26724255084991455\n",
      "220 0.2682466208934784\n",
      "221 0.25800952315330505\n",
      "222 0.2539341449737549\n",
      "223 0.26636648178100586\n",
      "224 0.2551901936531067\n",
      "225 0.26017501950263977\n",
      "226 0.2557642459869385\n",
      "227 0.2564447820186615\n",
      "228 0.2558169364929199\n",
      "229 0.2601027190685272\n",
      "230 0.26113247871398926\n",
      "231 0.2513505518436432\n",
      "232 0.2614925801753998\n",
      "233 0.2591927647590637\n",
      "234 0.2520880699157715\n",
      "235 0.2557724118232727\n",
      "236 0.26229992508888245\n",
      "237 0.26437580585479736\n",
      "238 0.2602785527706146\n",
      "239 0.2538752555847168\n",
      "240 0.25767433643341064\n",
      "241 0.2507748603820801\n",
      "242 0.26538094878196716\n",
      "243 0.255705863237381\n",
      "244 0.2556239664554596\n",
      "245 0.2504372000694275\n",
      "246 0.2555665969848633\n",
      "247 0.2600933015346527\n",
      "248 0.26132652163505554\n",
      "249 0.26017600297927856\n",
      "250 0.2523621916770935\n",
      "251 0.2575021982192993\n",
      "252 0.2635636329650879\n",
      "253 0.2587268054485321\n",
      "254 0.2508828341960907\n",
      "255 0.2552783489227295\n",
      "256 0.2613995373249054\n",
      "257 0.2567328214645386\n",
      "258 0.26282623410224915\n",
      "259 0.25940778851509094\n",
      "260 0.2606067657470703\n",
      "261 0.25725600123405457\n",
      "262 0.25588056445121765\n",
      "263 0.24928122758865356\n",
      "264 0.2512291371822357\n",
      "265 0.2594563066959381\n",
      "266 0.2563702464103699\n",
      "267 0.25623759627342224\n",
      "268 0.2575261890888214\n",
      "269 0.25836339592933655\n",
      "270 0.2545299828052521\n",
      "271 0.25951746106147766\n",
      "272 0.2590174078941345\n",
      "273 0.26077815890312195\n",
      "274 0.259098082780838\n",
      "275 0.2579006850719452\n",
      "276 0.25273633003234863\n",
      "277 0.25878238677978516\n",
      "278 0.2532573640346527\n",
      "279 0.25667905807495117\n",
      "280 0.25727298855781555\n",
      "281 0.262517511844635\n",
      "282 0.2615143954753876\n",
      "283 0.2544214129447937\n",
      "284 0.25963637232780457\n",
      "285 0.2529968321323395\n",
      "286 0.2570157051086426\n",
      "287 0.25763240456581116\n",
      "288 0.25899308919906616\n",
      "289 0.25432777404785156\n",
      "290 0.2586723268032074\n",
      "291 0.2638934850692749\n",
      "292 0.2627159357070923\n",
      "293 0.25113773345947266\n",
      "294 0.257592111825943\n",
      "295 0.26154854893684387\n",
      "296 0.2656285762786865\n",
      "297 0.26356860995292664\n",
      "298 0.2554726302623749\n",
      "299 0.25748103857040405\n",
      "300 0.24770045280456543\n",
      "301 0.259786993265152\n",
      "302 0.2539491057395935\n",
      "303 0.25496745109558105\n",
      "304 0.26136666536331177\n",
      "305 0.25602686405181885\n",
      "306 0.25427332520484924\n",
      "307 0.26182499527931213\n",
      "308 0.2580891251564026\n",
      "309 0.2561444342136383\n",
      "310 0.25650396943092346\n",
      "311 0.2599859833717346\n",
      "312 0.26014429330825806\n",
      "313 0.2549941837787628\n",
      "314 0.25928306579589844\n",
      "315 0.25253826379776\n",
      "316 0.26439595222473145\n",
      "317 0.2594345808029175\n",
      "318 0.2550170123577118\n",
      "319 0.2626103162765503\n",
      "320 0.25785940885543823\n",
      "321 0.26082929968833923\n",
      "322 0.262588769197464\n",
      "323 0.2528897523880005\n",
      "324 0.2554323673248291\n",
      "325 0.2602035105228424\n",
      "326 0.25378450751304626\n",
      "327 0.2543523609638214\n",
      "328 0.2573019564151764\n",
      "329 0.2575858533382416\n",
      "330 0.2628733217716217\n",
      "331 0.25881895422935486\n",
      "332 0.2533802092075348\n",
      "333 0.2574256658554077\n",
      "334 0.25181061029434204\n",
      "335 0.25533437728881836\n",
      "336 0.25604647397994995\n",
      "337 0.2589487135410309\n",
      "338 0.2638697922229767\n",
      "339 0.25416770577430725\n",
      "340 0.25611385703086853\n",
      "341 0.2588033080101013\n",
      "342 0.2614348828792572\n",
      "343 0.25590023398399353\n",
      "344 0.25504887104034424\n",
      "345 0.26166167855262756\n",
      "346 0.25556591153144836\n",
      "347 0.25801411271095276\n",
      "348 0.2579597234725952\n",
      "349 0.2600793242454529\n",
      "350 0.25490397214889526\n",
      "351 0.2592046558856964\n",
      "352 0.2575492858886719\n",
      "353 0.25794458389282227\n",
      "354 0.25089433789253235\n",
      "355 0.25370919704437256\n",
      "356 0.2583412528038025\n",
      "357 0.2596862316131592\n",
      "358 0.25852665305137634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359 0.25323063135147095\n",
      "360 0.2610549330711365\n",
      "361 0.2551250159740448\n",
      "362 0.2639027237892151\n",
      "363 0.25560733675956726\n",
      "364 0.25369173288345337\n",
      "365 0.25213244557380676\n",
      "366 0.2588612735271454\n",
      "367 0.2515970766544342\n",
      "368 0.26022061705589294\n",
      "369 0.25410306453704834\n",
      "370 0.2586001753807068\n",
      "371 0.2528402805328369\n",
      "372 0.252511590719223\n",
      "373 0.260211706161499\n",
      "374 0.24943773448467255\n",
      "375 0.25179004669189453\n",
      "376 0.2567742168903351\n",
      "377 0.25429844856262207\n",
      "378 0.26269152760505676\n",
      "379 0.2514294981956482\n",
      "380 0.25524723529815674\n",
      "381 0.2608136534690857\n",
      "382 0.25660353899002075\n",
      "383 0.25541526079177856\n",
      "384 0.2510131299495697\n",
      "385 0.2528967261314392\n",
      "386 0.25573399662971497\n",
      "387 0.2569952607154846\n",
      "388 0.25276893377304077\n",
      "389 0.2513931393623352\n",
      "390 0.25748172402381897\n",
      "391 0.2587720453739166\n",
      "392 0.25585606694221497\n",
      "393 0.25326260924339294\n",
      "394 0.2524624168872833\n",
      "395 0.25284233689308167\n",
      "396 0.24664589762687683\n",
      "397 0.2600404620170593\n",
      "398 0.2571449279785156\n",
      "399 0.25478118658065796\n",
      "400 0.25792115926742554\n",
      "401 0.25859108567237854\n",
      "402 0.2561950981616974\n",
      "403 0.2562752962112427\n",
      "404 0.2542458474636078\n",
      "405 0.2552769184112549\n",
      "406 0.2580271065235138\n",
      "407 0.25748211145401\n",
      "408 0.2500983774662018\n",
      "409 0.25400322675704956\n",
      "410 0.25409892201423645\n",
      "411 0.2619243264198303\n",
      "412 0.25794702768325806\n",
      "413 0.2542496621608734\n",
      "414 0.25490275025367737\n",
      "415 0.25486618280410767\n",
      "416 0.24648314714431763\n",
      "417 0.25393056869506836\n",
      "418 0.2523379325866699\n",
      "419 0.2571188807487488\n",
      "420 0.258661150932312\n",
      "421 0.25564926862716675\n",
      "422 0.25385257601737976\n",
      "423 0.26071101427078247\n",
      "424 0.25344306230545044\n",
      "425 0.25740697979927063\n",
      "426 0.25393155217170715\n",
      "427 0.25794780254364014\n",
      "428 0.2519438564777374\n",
      "429 0.2550671100616455\n",
      "430 0.2622978389263153\n",
      "431 0.25989189743995667\n",
      "432 0.26116278767585754\n",
      "433 0.2521701455116272\n",
      "434 0.25760072469711304\n",
      "435 0.2509540915489197\n",
      "436 0.25743040442466736\n",
      "437 0.25724390149116516\n",
      "438 0.25337499380111694\n",
      "439 0.2600659430027008\n",
      "440 0.2539771497249603\n",
      "441 0.25529974699020386\n",
      "442 0.2576274871826172\n",
      "443 0.25413474440574646\n",
      "444 0.26010575890541077\n",
      "445 0.25564149022102356\n",
      "446 0.25392889976501465\n",
      "447 0.25945931673049927\n",
      "448 0.25666892528533936\n",
      "449 0.2561132609844208\n",
      "450 0.25718554854393005\n",
      "451 0.25617945194244385\n",
      "452 0.25313231348991394\n",
      "453 0.26017865538597107\n",
      "454 0.2554454505443573\n",
      "455 0.25259914994239807\n",
      "456 0.25514182448387146\n",
      "457 0.25872743129730225\n",
      "458 0.2599005699157715\n",
      "459 0.2543741762638092\n",
      "460 0.2590178847312927\n",
      "461 0.2592378854751587\n",
      "462 0.256748765707016\n",
      "463 0.2552129626274109\n",
      "464 0.2567903399467468\n",
      "465 0.2577962875366211\n",
      "466 0.2504936754703522\n",
      "467 0.24813254177570343\n",
      "468 0.25351959466934204\n",
      "469 0.250954806804657\n",
      "470 0.25302985310554504\n",
      "471 0.25824791193008423\n",
      "472 0.2514178156852722\n",
      "473 0.2569645047187805\n",
      "474 0.25590991973876953\n",
      "475 0.25000569224357605\n",
      "476 0.25523242354393005\n",
      "477 0.25141018629074097\n",
      "478 0.2554522752761841\n",
      "479 0.25052332878112793\n",
      "480 0.25918176770210266\n",
      "481 0.2539653778076172\n",
      "482 0.2564166188240051\n",
      "483 0.25549980998039246\n",
      "484 0.2545480728149414\n",
      "485 0.2536798417568207\n",
      "486 0.25664809346199036\n",
      "487 0.24954769015312195\n",
      "488 0.25661933422088623\n",
      "489 0.25190427899360657\n",
      "490 0.2607698142528534\n",
      "491 0.25231146812438965\n",
      "492 0.25138840079307556\n",
      "493 0.251128226518631\n",
      "494 0.25562259554862976\n",
      "495 0.2536735534667969\n",
      "496 0.25281408429145813\n",
      "497 0.25494930148124695\n",
      "498 0.25555387139320374\n",
      "499 0.2539655268192291\n",
      "0 0.2468680739402771\n",
      "1 0.24824842810630798\n",
      "2 0.25137922167778015\n",
      "3 0.24315302073955536\n",
      "4 0.24542857706546783\n",
      "5 0.24667127430438995\n",
      "6 0.23383717238903046\n",
      "7 0.23273219168186188\n",
      "8 0.2222450226545334\n",
      "9 0.2287784069776535\n",
      "10 0.21750839054584503\n",
      "11 0.2165352702140808\n",
      "12 0.22062291204929352\n",
      "13 0.2084939032793045\n",
      "14 0.20967602729797363\n",
      "15 0.19430294632911682\n",
      "16 0.19339808821678162\n",
      "17 0.1888197809457779\n",
      "18 0.18361234664916992\n",
      "19 0.17986749112606049\n",
      "20 0.18019507825374603\n",
      "21 0.178195521235466\n",
      "22 0.1915961056947708\n",
      "23 0.16006317734718323\n",
      "24 0.15814724564552307\n",
      "25 0.16116690635681152\n",
      "26 0.15393653512001038\n",
      "27 0.15244996547698975\n",
      "28 0.16188909113407135\n",
      "29 0.14104585349559784\n",
      "30 0.1820501834154129\n",
      "31 0.15927754342556\n",
      "32 0.16813144087791443\n",
      "33 0.14272236824035645\n",
      "34 0.1485445648431778\n",
      "35 0.15469682216644287\n",
      "36 0.14448805153369904\n",
      "37 0.1640983670949936\n",
      "38 0.1616707593202591\n",
      "39 0.16786327958106995\n",
      "40 0.167902871966362\n",
      "41 0.1598023921251297\n",
      "42 0.16889454424381256\n",
      "43 0.14049360156059265\n",
      "44 0.14393027126789093\n",
      "45 0.14234523475170135\n",
      "46 0.16116680204868317\n",
      "47 0.1483832597732544\n",
      "48 0.16187593340873718\n",
      "49 0.15819522738456726\n",
      "50 0.1436636596918106\n",
      "51 0.14334620535373688\n",
      "52 0.15772366523742676\n",
      "53 0.17025600373744965\n",
      "54 0.14964662492275238\n",
      "55 0.18835292756557465\n",
      "56 0.16780437529087067\n",
      "57 0.1549438238143921\n",
      "58 0.14379934966564178\n",
      "59 0.1500871330499649\n",
      "60 0.137781023979187\n",
      "61 0.15215051174163818\n",
      "62 0.17565220594406128\n",
      "63 0.1556759923696518\n",
      "64 0.13774974644184113\n",
      "65 0.1612984538078308\n",
      "66 0.15923738479614258\n",
      "67 0.15724879503250122\n",
      "68 0.15493342280387878\n",
      "69 0.1621389240026474\n",
      "70 0.14540262520313263\n",
      "71 0.15054725110530853\n",
      "72 0.16657313704490662\n",
      "73 0.14790885150432587\n",
      "74 0.15622378885746002\n",
      "75 0.16217422485351562\n",
      "76 0.1694469302892685\n",
      "77 0.16435541212558746\n",
      "78 0.14020991325378418\n",
      "79 0.1288902759552002\n",
      "80 0.14609824120998383\n",
      "81 0.14492659270763397\n",
      "82 0.1533890813589096\n",
      "83 0.15923117101192474\n",
      "84 0.1420559585094452\n",
      "85 0.14155204594135284\n",
      "86 0.1608402132987976\n",
      "87 0.16261297464370728\n",
      "88 0.1639638990163803\n",
      "89 0.14227329194545746\n",
      "90 0.13558095693588257\n",
      "91 0.14587315917015076\n",
      "92 0.14083831012248993\n",
      "93 0.1472615748643875\n",
      "94 0.1534757912158966\n",
      "95 0.15805746614933014\n",
      "96 0.12338278442621231\n",
      "97 0.12553611397743225\n",
      "98 0.13753508031368256\n",
      "99 0.14118660986423492\n",
      "100 0.1454390287399292\n",
      "101 0.15582753717899323\n",
      "102 0.1497432440519333\n",
      "103 0.12743976712226868\n",
      "104 0.1342407763004303\n",
      "105 0.14571025967597961\n",
      "106 0.14748646318912506\n",
      "107 0.16961567103862762\n",
      "108 0.15956906974315643\n",
      "109 0.15978941321372986\n",
      "110 0.1737077832221985\n",
      "111 0.15584929287433624\n",
      "112 0.1289779245853424\n",
      "113 0.14282973110675812\n",
      "114 0.16010034084320068\n",
      "115 0.1432414948940277\n",
      "116 0.14791202545166016\n",
      "117 0.14346855878829956\n",
      "118 0.1481061428785324\n",
      "119 0.1473957896232605\n",
      "120 0.14903047680854797\n",
      "121 0.16226664185523987\n",
      "122 0.16853131353855133\n",
      "123 0.15883636474609375\n",
      "124 0.17520280182361603\n",
      "125 0.14246253669261932\n",
      "126 0.1381189525127411\n",
      "127 0.16889949142932892\n",
      "128 0.16910520195960999\n",
      "129 0.14371807873249054\n",
      "130 0.1549176126718521\n",
      "131 0.185626819729805\n",
      "132 0.140080526471138\n",
      "133 0.18549920618534088\n",
      "134 0.14729009568691254\n",
      "135 0.14138861000537872\n",
      "136 0.12794218957424164\n",
      "137 0.14394445717334747\n",
      "138 0.14564502239227295\n",
      "139 0.14849014580249786\n",
      "140 0.1673537790775299\n",
      "141 0.1582530438899994\n",
      "142 0.16010932624340057\n",
      "143 0.16200491786003113\n",
      "144 0.15009596943855286\n",
      "145 0.14331364631652832\n",
      "146 0.14609016478061676\n",
      "147 0.13783304393291473\n",
      "148 0.14668792486190796\n",
      "149 0.1708458513021469\n",
      "150 0.15823155641555786\n",
      "151 0.14186492562294006\n",
      "152 0.1714673489332199\n",
      "153 0.15376663208007812\n",
      "154 0.15234188735485077\n",
      "155 0.15836824476718903\n",
      "156 0.12750214338302612\n",
      "157 0.12626323103904724\n",
      "158 0.14597827196121216\n",
      "159 0.13719746470451355\n",
      "160 0.16376659274101257\n",
      "161 0.13095375895500183\n",
      "162 0.1407589316368103\n",
      "163 0.17105688154697418\n",
      "164 0.1613103747367859\n",
      "165 0.16714075207710266\n",
      "166 0.13958106935024261\n",
      "167 0.18104127049446106\n",
      "168 0.15490128099918365\n",
      "169 0.13983848690986633\n",
      "170 0.15303589403629303\n",
      "171 0.14837490022182465\n",
      "172 0.1432981938123703\n",
      "173 0.1439683586359024\n",
      "174 0.15249116718769073\n",
      "175 0.14019834995269775\n",
      "176 0.16879802942276\n",
      "177 0.14356139302253723\n",
      "178 0.15084898471832275\n",
      "179 0.13062900304794312\n",
      "180 0.12903869152069092\n",
      "181 0.15407052636146545\n",
      "182 0.1420762836933136\n",
      "183 0.11951687186956406\n",
      "184 0.1447521597146988\n",
      "185 0.14883458614349365\n",
      "186 0.13991431891918182\n",
      "187 0.12185972183942795\n",
      "188 0.1438315361738205\n",
      "189 0.14564865827560425\n",
      "190 0.15967713296413422\n",
      "191 0.16374684870243073\n",
      "192 0.14311985671520233\n",
      "193 0.12057656049728394\n",
      "194 0.12837420403957367\n",
      "195 0.12505459785461426\n",
      "196 0.14507777988910675\n",
      "197 0.12460532784461975\n",
      "198 0.1617870032787323\n",
      "199 0.11610911786556244\n",
      "200 0.12208738178014755\n",
      "201 0.16806766390800476\n",
      "202 0.14484290778636932\n",
      "203 0.17585371434688568\n",
      "204 0.1399534046649933\n",
      "205 0.13839200139045715\n",
      "206 0.13672015070915222\n",
      "207 0.15794847905635834\n",
      "208 0.14859364926815033\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3cf045044f89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXOR_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inferno'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrab_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36mgrab_frame\u001b[0;34m(self, **savefig_kwargs)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0;31m# frame format and dpi.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             self.fig.savefig(self._frame_sink(), format=self.frame_format,\n\u001b[0;32m--> 376\u001b[0;31m                              dpi=self.dpi, **savefig_kwargs)\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mRuntimeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_proc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, frameon, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   2092\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_frameon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2094\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2096\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframeon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2073\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m                     \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2075\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m   2076\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2077\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_raw\u001b[0;34m(self, filename_or_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m         \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0mtoolbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoolbar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m             \u001b[0;31m# A GUI class may be need to update a window using this draw, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0;31m# don't forget to call the superclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m             mimage._draw_list_compositing_images(\n\u001b[0;32m-> 1649\u001b[0;31m                 renderer, self, artists, self.suppressComposite)\n\u001b[0m\u001b[1;32m   1650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figure'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2626\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2628\u001b[0;31m         \u001b[0mmimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             im, l, b, trans = self.make_image(\n\u001b[0;32m--> 584\u001b[0;31m                 renderer, renderer.get_image_magnification())\n\u001b[0m\u001b[1;32m    585\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mmake_image\u001b[0;34m(self, renderer, magnification, unsampled)\u001b[0m\n\u001b[1;32m    837\u001b[0m         return self._make_image(\n\u001b[1;32m    838\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_bbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagnification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m             unsampled=unsampled)\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_unsampled_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_make_image\u001b[0;34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[0m\n\u001b[1;32m    427\u001b[0m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_filternorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m                                 self.get_filterrad())\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m                 \u001b[0;31m# we are done with A_scaled now, remove from namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "with writer.saving(fig, 'xor.mp4' ,100):\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    x = np.linspace(0, 1.0, 100)\n",
    "    y = np.linspace(0, 1.0, 100)\n",
    "    \n",
    "    network = DQN(3, [6,3], 1, F.elu)\n",
    "    optimizer = optim.Adam(network.parameters(), amsgrad=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for i in range(500):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        Xs, Ys = generate_both(25,0.1)\n",
    "                    \n",
    "        Xs = torch.tensor(Xs)\n",
    "        Ys = torch.tensor(Ys, dtype=torch.float)\n",
    "\n",
    "        prediction = network(Xs)\n",
    "        loss = criterion(prediction, Ys)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(i, loss.item())\n",
    "\n",
    "        OR_mat = np.zeros((100,100))\n",
    "        XOR_mat = np.zeros((100,100))\n",
    "\n",
    "        for idx_y, grid_point_y in enumerate(y):\n",
    "            for idx_x, grid_point_x in enumerate(x):\n",
    "                OR_mat[idx_y, idx_x] = network(torch.tensor([grid_point_x, grid_point_y, 0.0])).item()\n",
    "                XOR_mat[idx_y, idx_x] = network(torch.tensor([grid_point_x, grid_point_y, 1.0])).item()\n",
    "\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.axvline(x=0.5, color='w', linestyle='dashed')\n",
    "        plt.axhline(y=0.5, color='w', linestyle='dashed')\n",
    "        plt.xticks(np.arange(min(x), max(x)+1, 0.5))\n",
    "        plt.yticks(np.arange(min(y), max(y)+1, 0.5))\n",
    "        plt.imshow(OR_mat, interpolation='none', cmap='inferno', extent=(0.0, 1.0, 0.0, 1.0))\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.axvline(x=0.5, color='w', linestyle='dashed')\n",
    "        plt.axhline(y=0.5, color='w', linestyle='dashed')\n",
    "        plt.xticks(np.arange(min(x), max(x)+1, 0.5))\n",
    "        plt.yticks(np.arange(min(y), max(y)+1, 0.5))\n",
    "        plt.imshow(XOR_mat, interpolation='none', cmap='inferno', extent=(0.0, 1.0, 0.0, 1.0))\n",
    "        \n",
    "        writer.grab_frame()\n",
    "    \n",
    "    for i in range(500):\n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        Xs, Ys = generate_both(25,0.9)\n",
    "                    \n",
    "        Xs = torch.tensor(Xs)\n",
    "        Ys = torch.tensor(Ys, dtype=torch.float)\n",
    "\n",
    "        prediction = network(Xs)\n",
    "        loss = criterion(prediction, Ys)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(i, loss.item())\n",
    "\n",
    "        OR_mat = np.zeros((100,100))\n",
    "        XOR_mat = np.zeros((100,100))\n",
    "\n",
    "        for idx_y, grid_point_y in enumerate(y):\n",
    "            for idx_x, grid_point_x in enumerate(x):\n",
    "                OR_mat[idx_y, idx_x] = network(torch.tensor([grid_point_x, grid_point_y, 0.0])).item()\n",
    "                XOR_mat[idx_y, idx_x] = network(torch.tensor([grid_point_x, grid_point_y, 1.0])).item()\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.axvline(x=0.5, color='w', linestyle='dashed')\n",
    "        plt.axhline(y=0.5, color='w', linestyle='dashed')\n",
    "        plt.xticks(np.arange(min(x), max(x)+1, 0.5))\n",
    "        plt.yticks(np.arange(min(y), max(y)+1, 0.5))\n",
    "        plt.imshow(OR_mat, interpolation='none', cmap='inferno', extent=(0.0, 1.0, 0.0, 1.0))\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.axvline(x=0.5, color='w', linestyle='dashed')\n",
    "        plt.axhline(y=0.5, color='w', linestyle='dashed')\n",
    "        plt.xticks(np.arange(min(x), max(x)+1, 0.5))\n",
    "        plt.yticks(np.arange(min(y), max(y)+1, 0.5))\n",
    "        plt.imshow(XOR_mat, interpolation='none', cmap='inferno', extent=(0.0, 1.0, 0.0, 1.0))\n",
    "        \n",
    "        writer.grab_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5850, -0.1173, -0.4558, -0.0853], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>) tensor([], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([ 0.1066, -0.0613], device='cuda:0', grad_fn=<SliceBackward>) tensor([], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([-0.0406, -0.4383,  0.2101, -0.2855], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>) tensor([], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([0.0420, 0.2337], device='cuda:0', grad_fn=<SliceBackward>) tensor([], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Layer  0\n",
      "Common Weights  0.15690992027521133\n",
      "New Weights nan\n",
      "Total Weights 0.15690992027521133\n",
      "Common Bias 0.13067696243524551\n",
      "New Bias 0.13067696243524551\n",
      "Total Bias 0.13067696243524551\n",
      "Layer  1\n",
      "Common Weights  0.07105209305882454\n",
      "New Weights nan\n",
      "Total Weights 0.07105209305882454\n",
      "Common Bias 0.01578469481319189\n",
      "New Bias 0.01578469481319189\n",
      "Total Bias 0.01578469481319189\n",
      "Layer  2\n",
      "Common Weights  0.07519078254699707\n",
      "New Weights nan\n",
      "Total Weights 0.07519078254699707\n",
      "Common Bias 0.00903657078742981\n",
      "New Bias 0.00903657078742981\n",
      "Total Bias 0.00903657078742981\n",
      "----\n",
      "tensor([-0.2755], device='cuda:0', grad_fn=<SliceBackward>) tensor([0.3825, 0.3131, 0.5710], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([0.4178], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.1057], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([0.2407], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.3667,  0.1729, -0.1587], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([0.7246], device='cuda:0', grad_fn=<SliceBackward>) tensor([-0.3277], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "Layer  0\n",
      "Common Weights  0.1836732104420662\n",
      "New Weights 0.24971714615821838\n",
      "Total Weights 0.23320618271827698\n",
      "Common Bias 0.17811808735132217\n",
      "New Bias 0.17811808735132217\n",
      "Total Bias 0.17811808735132217\n",
      "Layer  1\n",
      "Common Weights  0.053631752729415894\n",
      "New Weights 0.3246877118945122\n",
      "Total Weights 0.25559163093566895\n",
      "Common Bias 0.12029058113694191\n",
      "New Bias 0.12029058113694191\n",
      "Total Bias 0.12029058113694191\n",
      "Layer  2\n",
      "Common Weights  0.1423538774251938\n",
      "New Weights 0.13170555233955383\n",
      "Total Weights 0.1370297148823738\n",
      "Common Bias 0.052633851766586304\n",
      "New Bias 0.052633851766586304\n",
      "Total Bias 0.052633851766586304\n"
     ]
    }
   ],
   "source": [
    "def weight_change(initial_capacity, train_or, capacity, non_linearity):\n",
    "    common_weight = []\n",
    "    new_weight = []\n",
    "    total_weight = []\n",
    "    common_bias = []\n",
    "    new_bias = []\n",
    "    total_bias = []\n",
    "    \n",
    "    for idx in range(len(initial_capacity)+1):\n",
    "        common_weight.append([])\n",
    "        new_weight.append([])\n",
    "        total_weight.append([])\n",
    "        common_bias.append([])\n",
    "        new_bias.append([])\n",
    "        total_bias.append([])\n",
    "    \n",
    "    for seed in range(2):  \n",
    "        # Set seeds\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        # Initialisation network\n",
    "        network = DQN(3, initial_capacity.copy(), 1, non_linearity).to(device)\n",
    "        optimizer = optim.Adam(network.parameters(), amsgrad=True)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        if train_or:\n",
    "            for i in range(1000):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                Xs, Ys = generate_both(25,0.1)\n",
    "                    \n",
    "                Xs = torch.tensor(Xs, device=device)\n",
    "                Ys = torch.tensor(Ys, dtype=torch.float, device=device)\n",
    "\n",
    "                prediction = network(Xs)\n",
    "                loss = criterion(prediction, Ys)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # Evaluation\n",
    "                    prediction = network(torch.tensor([[0,0,0],[0,1,0],[1,0,0],[1,1,0]], dtype=torch.float, device=device))\n",
    "                    Ys = torch.tensor([[0],[1],[1],[1]], dtype=torch.float, device=device)\n",
    "                    loss = 1.0/(1.0+criterion(prediction, Ys))\n",
    "\n",
    "                if loss>0.95:\n",
    "                    break\n",
    "                    \n",
    "                network_before_increase = copy.deepcopy(network)\n",
    "        \n",
    "        if capacity is not None:\n",
    "            network, optimizer = increase_capacity_keep_lr(network, capacity, optimizer, device)\n",
    "            \n",
    "        network_after_increase = copy.deepcopy(network)\n",
    "        \n",
    "        iters = 1000\n",
    "#         if not train_or:\n",
    "#             (155) * 1000\n",
    "            \n",
    "        for i in range(iters):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Uniform syllabus 20% of the time\n",
    "            Xs, Ys = generate_both(25,0.9)\n",
    "                \n",
    "            Xs = torch.tensor(Xs, device=device)\n",
    "            Ys = torch.tensor(Ys, dtype=torch.float, device=device)\n",
    "\n",
    "            prediction = network(Xs)\n",
    "            loss = criterion(prediction, Ys)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Evaluation\n",
    "                prediction = network(torch.tensor([[0,0,0],[0,1,0],[1,0,0],[1,1,0]], dtype=torch.float, device=device))\n",
    "                Ys = torch.tensor([[0],[1],[1],[1]], dtype=torch.float, device=device)\n",
    "                loss = 1.0/(1.0+criterion(prediction, Ys))\n",
    "\n",
    "            if loss>0.95:\n",
    "                break\n",
    "        \n",
    "        for idx in range(len(initial_capacity) + 1):\n",
    "            if idx==0: \n",
    "                common_weight[idx].append(torch.mean(torch.abs(network.layers[idx].weight[0:initial_capacity[0],:] - network_after_increase.layers[idx].weight[0:initial_capacity[0],:])).item())\n",
    "                new_weight[idx].append(torch.mean(torch.abs(network.layers[idx].weight[initial_capacity[0]:,:] - network_after_increase.layers[idx].weight[initial_capacity[0]:,:])).item())\n",
    "                \n",
    "            elif idx<len(initial_capacity):\n",
    "                common_weight[idx].append(torch.mean(torch.abs(network.layers[idx].weight[0:initial_capacity[idx],0:initial_capacity[idx - 1] ] - network_after_increase.layers[idx].weight[0:initial_capacity[idx],0:initial_capacity[idx-1]])).item())\n",
    "                new_weight[idx].append(torch.mean(torch.abs(network.layers[idx].weight[initial_capacity[idx]:,initial_capacity[idx - 1]:] - network_after_increase.layers[idx].weight[initial_capacity[idx]:,initial_capacity[idx-1]:])).item())\n",
    "            else:\n",
    "                common_weight[idx].append(torch.mean(torch.abs(network.layers[idx].weight[:,0:initial_capacity[idx - 1] ] - network_after_increase.layers[idx].weight[:,0:initial_capacity[idx-1]])).item())\n",
    "                new_weight[idx].append(torch.mean(torch.abs(network.layers[idx].weight[:,initial_capacity[idx - 1]: ] - network_after_increase.layers[idx].weight[:,initial_capacity[idx-1]:])).item())\n",
    "            \n",
    "            total_weight[idx].append(torch.mean(torch.abs(network.layers[idx].weight - network_after_increase.layers[idx].weight)).item())\n",
    "            \n",
    "            if idx<len(initial_capacity):\n",
    "                common_bias[idx].append(torch.mean(torch.abs(network.layers[idx].bias[:initial_capacity[idx]] - network_after_increase.layers[idx].bias[:initial_capacity[idx]])).item())\n",
    "                new_bias[idx].append(torch.mean(torch.abs(network.layers[idx].bias[initial_capacity[idx]:] - network_after_increase.layers[idx].bias[initial_capacity[idx]:])).item())\n",
    "            else:\n",
    "                common_bias[idx].append(torch.mean(torch.abs(network.layers[idx].bias - network_after_increase.layers[idx].bias)).item())\n",
    "                new_bias[idx].append(torch.mean(torch.abs(network.layers[idx].bias - network_after_increase.layers[idx].bias)).item())\n",
    "            total_bias[idx].append(torch.mean(torch.abs(network.layers[idx].bias - network_after_increase.layers[idx].bias)).item())\n",
    "            \n",
    "    for idx in range(len(initial_capacity)+1):\n",
    "        print('Layer ', idx)\n",
    "        print('Common Weights ', np.average(np.array(common_weight[idx])))\n",
    "        print('New Weights', np.average(np.array(new_weight[idx])))\n",
    "        print('Total Weights', np.average(np.array(total_weight[idx])))\n",
    "        print('Common Bias', np.average(np.array(common_bias[idx])))\n",
    "        print('New Bias', np.average(np.array(new_bias[idx])))\n",
    "        print('Total Bias', np.average(np.array(total_bias[idx])))\n",
    "        \n",
    "initial_capacity = [4, 2]\n",
    "capacity = None\n",
    "train_or = True\n",
    "non_linearity = F.elu\n",
    "weight_change(initial_capacity, train_or, capacity, non_linearity)\n",
    "print('----')\n",
    "non_linearity = F.elu\n",
    "initial_capacity = [1, 1]\n",
    "capacity = [3, 1]\n",
    "train_or = True\n",
    "weight_change(initial_capacity, train_or, capacity, non_linearity)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
