{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import copy\n",
    "from networks import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n",
      "Torch Version:  1.0.1.post2\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)\n",
    "print(\"Torch Version: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increase_capacity_keep_lr(network, capacity, optimizer, device):\n",
    "    # Store old ids\n",
    "    old_ids = [id(p) for p in network.parameters()]\n",
    "    old_param_sizes = [p.size() for p in network.parameters()]\n",
    "\n",
    "    network.increase_capacity(capacity)\n",
    "\n",
    "    # Store new ids\n",
    "    new_ids = [id(p) for p in network.parameters()]\n",
    "    new_param_sizes = [p.size() for p in network.parameters()]\n",
    "\n",
    "    # Store old state \n",
    "    opt_state_dict = optimizer.state_dict()\n",
    "    for old_id, new_id, new_param_size, old_param_size in zip(old_ids, new_ids, new_param_sizes, old_param_sizes):\n",
    "        # Store step, and exp_avgs\n",
    "        step = opt_state_dict['state'][old_id]['step']\n",
    "        old_exp_avg = opt_state_dict['state'][old_id]['exp_avg']\n",
    "        old_exp_avg_sq = opt_state_dict['state'][old_id]['exp_avg_sq']\n",
    "        old_max_exp_avg_sq = opt_state_dict['state'][old_id]['max_exp_avg_sq']\n",
    "\n",
    "        exp_avg = torch.zeros(new_param_size)\n",
    "        exp_avg_sq = torch.zeros(new_param_size)\n",
    "        max_exp_avg_sq =  torch.zeros(new_param_size)\n",
    "        # Extend exp_avgs to new shape depending on wether param is bias or weight\n",
    "        if exp_avg.dim()>1:\n",
    "            # Weights\n",
    "            exp_avg[0:old_param_size[0],0:old_param_size[1]] = old_exp_avg\n",
    "            exp_avg_sq[0:old_param_size[0],0:old_param_size[1]] = old_exp_avg_sq\n",
    "            max_exp_avg_sq[0:old_param_size[0],0:old_param_size[1]] = old_max_exp_avg_sq\n",
    "        else:\n",
    "            # Biases/last layer\n",
    "            exp_avg[0:old_param_size[0]] = old_exp_avg\n",
    "            exp_avg_sq[0:old_param_size[0]] = old_exp_avg_sq\n",
    "            max_exp_avg_sq[0:old_param_size[0]] = old_max_exp_avg_sq\n",
    "        \n",
    "        # Delete old id from state_dict and update new params and new id\n",
    "        del opt_state_dict['state'][old_id]\n",
    "        opt_state_dict['state'][new_id] = {\n",
    "            'step': step,\n",
    "            'exp_avg': exp_avg,\n",
    "            'exp_avg_sq': exp_avg_sq.to(device),\n",
    "            'max_exp_avg_sq' : max_exp_avg_sq.to(device)\n",
    "        }\n",
    "        opt_state_dict['param_groups'][0]['params'].remove(old_id)\n",
    "        opt_state_dict['param_groups'][0]['params'].append(new_id)\n",
    "\n",
    "    network.to(device)\n",
    "    optimizer = optim.Adam(network.parameters(), amsgrad=True)\n",
    "    optimizer.load_state_dict(opt_state_dict)\n",
    "    \n",
    "    return network, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_zero():\n",
    "    return random.uniform(0, 49) / 100\n",
    "\n",
    "def generate_one():\n",
    "    return random.uniform(50, 100) / 100\n",
    "\n",
    "def generate_both(num_data_points, p):\n",
    "    Xs, Ys = [], []\n",
    "    for _ in range(num_data_points):\n",
    "        if random.random() < p:\n",
    "            Xs.append([generate_zero(), generate_zero(), 0]); Ys.append([0])\n",
    "            # or(1, 0) -> 1\n",
    "            Xs.append([generate_one(), generate_zero(), 0]); Ys.append([1])\n",
    "            # or(0, 1) -> 1\n",
    "            Xs.append([generate_zero(), generate_one(), 0]); Ys.append([1])\n",
    "            # or(1, 1) -> 1\n",
    "            Xs.append([generate_one(), generate_one(), 0]); Ys.append([1])\n",
    "        else:\n",
    "            # xor(0, 0) -> 0\n",
    "            Xs.append([generate_zero(), generate_zero(), 1]); Ys.append([0])\n",
    "            # xor(1, 0) -> 1\n",
    "            Xs.append([generate_one(), generate_zero(), 1]); Ys.append([1])\n",
    "            # xor(0, 1) -> 1\n",
    "            Xs.append([generate_zero(), generate_one(), 1]); Ys.append([1])\n",
    "            # xor(1, 1) -> 0\n",
    "            Xs.append([generate_one(), generate_one(), 1]); Ys.append([0])\n",
    "    return Xs, Ys\n",
    "\n",
    "def generate_or_XY(num_data_points):\n",
    "    Xs, Ys = [], []\n",
    "    for _ in range(num_data_points):\n",
    "        # or(0, 0) -> 0 \n",
    "        Xs.append([generate_zero(), generate_zero(), 0]); Ys.append([0])\n",
    "        # or(1, 0) -> 1\n",
    "        Xs.append([generate_one(), generate_zero(), 0]); Ys.append([1])\n",
    "        # or(0, 1) -> 1\n",
    "        Xs.append([generate_zero(), generate_one(), 0]); Ys.append([1])\n",
    "        # or(1, 1) -> 1\n",
    "        Xs.append([generate_one(), generate_one(), 0]); Ys.append([1])\n",
    "    return Xs, Ys\n",
    "\n",
    "def generate_xor_XY(num_data_points):\n",
    "    Xs, Ys = [], []\n",
    "    for _ in range(num_data_points):\n",
    "        # xor(0, 0) -> 0 \n",
    "        Xs.append([generate_zero(), generate_zero(), 1]); Ys.append([0])\n",
    "        # xor(1, 0) -> 1\n",
    "        Xs.append([generate_one(), generate_zero(), 1]); Ys.append([1])\n",
    "        # xor(0, 1) -> 1\n",
    "        Xs.append([generate_zero(), generate_one(), 1]); Ys.append([1])\n",
    "        # xor(1, 1) -> 0\n",
    "        Xs.append([generate_one(), generate_one(), 1]); Ys.append([0])\n",
    "    return Xs, Ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] False None\n",
      "Average loss or before xor training:  0.5666652438044548\n",
      "Average loss xor before xor training:  0.6165286988019943\n",
      "Average loss or before xor training:  0.5666652438044548\n",
      "Average loss xor before xor training:  0.6165286988019943\n",
      "Average loss or:  0.923716379404068\n",
      "Average loss xor:  0.766582578420639\n",
      "Average loss:  0.8451494789123535\n",
      "[0.9461823105812073, 0.9936859011650085, 0.898678719997406, 52, [2], False, None]\n",
      "644\n",
      "793\n",
      "577\n",
      "6010\n",
      "11895\n",
      "[2] True None\n",
      "Average loss or before xor training:  0.5666652438044548\n",
      "Average loss xor before xor training:  0.6165286988019943\n",
      "Average loss or before xor training:  0.8549469441175461\n",
      "Average loss xor before xor training:  0.8341650491952897\n",
      "Average loss or:  0.9273117077350617\n",
      "Average loss xor:  0.7729986488819123\n",
      "Average loss:  0.850155178308487\n",
      "[0.9454349875450134, 0.9938973784446716, 0.8969725966453552, 96, [2], True, None]\n",
      "[1] True [1]\n",
      "Average loss or before xor training:  0.5736202634871006\n",
      "Average loss xor before xor training:  0.602302153557539\n",
      "Average loss or before xor training:  0.8382703143358231\n",
      "Average loss xor before xor training:  0.8066715425252915\n",
      "Average loss or:  0.9224358582496643\n",
      "Average loss xor:  0.7902994585037232\n",
      "Average loss:  0.8563676583766937\n",
      "[0.9461467862129211, 0.9936705827713013, 0.898622989654541, 61, [1], True, [1]]\n",
      "[3] False None\n",
      "Average loss or before xor training:  0.5574332610482261\n",
      "Average loss xor before xor training:  0.6075130019869123\n",
      "Average loss or before xor training:  0.5574332610482261\n",
      "Average loss xor before xor training:  0.6075130019869123\n",
      "Average loss or:  0.9304523113228026\n",
      "Average loss xor:  0.7909245178813026\n",
      "Average loss:  0.8606884146020526\n",
      "[0.9777402579784393, 0.9767739772796631, 0.9787065386772156, 41, [3], False, None]\n",
      "15747\n",
      "5424\n",
      "29721\n",
      "1303\n",
      "[3] True None\n",
      "Average loss or before xor training:  0.5574332610482261\n",
      "Average loss xor before xor training:  0.6075130019869123\n",
      "Average loss or before xor training:  0.8429143201737177\n",
      "Average loss xor before xor training:  0.8610266432875678\n",
      "Average loss or:  0.9326653523104531\n",
      "Average loss xor:  0.8114264281023116\n",
      "Average loss:  0.8720458902063823\n",
      "[0.9795491397380829, 0.9788264036178589, 0.9802718758583069, 41, [3], True, None]\n",
      "644\n",
      "[2] True [1]\n",
      "Average loss or before xor training:  0.5538496673107147\n",
      "Average loss xor before xor training:  0.6029701713058684\n",
      "Average loss or before xor training:  0.8443188832865821\n",
      "Average loss xor before xor training:  0.8476261032952203\n",
      "Average loss or:  0.932902584473292\n",
      "Average loss xor:  0.8398837844530741\n",
      "Average loss:  0.886393184463183\n",
      "[0.9414065182209015, 0.9358416199684143, 0.9469714164733887, 0, [2], True, [1]]\n",
      "[1] True [2]\n",
      "Average loss or before xor training:  0.49822520713011426\n",
      "Average loss xor before xor training:  0.5893503179152807\n",
      "Average loss or before xor training:  0.8370404740174612\n",
      "Average loss xor before xor training:  0.7989740173021952\n",
      "Average loss or:  0.9325534999370575\n",
      "Average loss xor:  0.8454785744349161\n",
      "Average loss:  0.8890160371859868\n",
      "[0.9458978474140167, 0.9367967247962952, 0.9549989700317383, 5, [1], True, [2]]\n",
      "[4] False None\n",
      "Average loss or before xor training:  0.5748822093009949\n",
      "Average loss xor before xor training:  0.6749840378761292\n",
      "Average loss or before xor training:  0.5748822093009949\n",
      "Average loss xor before xor training:  0.6749840378761292\n",
      "Average loss or:  0.8842284679412842\n",
      "Average loss xor:  0.8639135658740997\n",
      "Average loss:  0.874071016907692\n",
      "[0.9427948296070099, 0.9268316626548767, 0.9587579965591431, 1, [4], False, None]\n",
      "14347\n",
      "1052\n",
      "[4] True None\n",
      "Average loss or before xor training:  0.5597774494778026\n",
      "Average loss xor before xor training:  0.6508458934047006\n",
      "Average loss or before xor training:  0.8590154485269026\n",
      "Average loss xor before xor training:  0.8780849034135992\n",
      "Average loss or:  0.9443848349831321\n",
      "Average loss xor:  0.8680326234210621\n",
      "Average loss:  0.906208729202097\n",
      "[0.9831987023353577, 0.9717408418655396, 0.9946565628051758, 10, [4], True, None]\n",
      "[1] True [3]\n",
      "Average loss or before xor training:  0.7033356428146362\n",
      "Average loss xor before xor training:  0.76881210009257\n",
      "Average loss or before xor training:  0.8462653557459513\n",
      "Average loss xor before xor training:  0.798844556013743\n",
      "Average loss or:  0.9305543502171835\n",
      "Average loss xor:  0.8865782817204794\n",
      "Average loss:  0.9085663159688314\n",
      "[0.9522378444671631, 0.9128049612045288, 0.9916707277297974, 2, [1], True, [3]]\n",
      "[2] True [2]\n",
      "Average loss or before xor training:  0.47590985894203186\n",
      "Average loss xor before xor training:  0.5929722189903259\n",
      "Average loss or before xor training:  0.9104793071746826\n",
      "Average loss xor before xor training:  0.7999916076660156\n",
      "Average loss or:  0.9625261425971985\n",
      "Average loss xor:  0.9906518459320068\n",
      "Average loss:  0.9765889942646027\n",
      "[0.9765889942646027, 0.9625261425971985, 0.9906518459320068, 0, [2], True, [2]]\n",
      "15747\n",
      "5424\n",
      "[3] True [1]\n",
      "Average loss or before xor training:  0.5831514010826747\n",
      "Average loss xor before xor training:  0.6237405369679133\n",
      "Average loss or before xor training:  0.8380846083164215\n",
      "Average loss xor before xor training:  0.862369441986084\n",
      "Average loss or:  0.9400983730951945\n",
      "Average loss xor:  0.8572719494501749\n",
      "Average loss:  0.8986851612726847\n",
      "[0.9699008762836456, 0.9582441449165344, 0.9815576076507568, 29, [3], True, [1]]\n"
     ]
    }
   ],
   "source": [
    "def xor_experiments(initial_capacity, train_or, capacity):\n",
    "    lowest_loss = 0\n",
    "    lowest_settings = []\n",
    "    \n",
    "    pre_pre_loss_or = []\n",
    "    pre_pre_loss_xor = []\n",
    "    \n",
    "    pre_loss_or = []\n",
    "    pre_loss_xor = []\n",
    "    \n",
    "    losses_or = []\n",
    "    losses_xor = []\n",
    "    for seed in range(100):\n",
    "        # Set seeds\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        # Initialisation network\n",
    "        network = DQN(3, initial_capacity.copy(), 1, F.relu)\n",
    "        optimizer = optim.Adam(network.parameters(), amsgrad=True)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        prediction = network(torch.tensor([[0,0,0],[0,1,0],[1,0,0],[1,1,0]], dtype=torch.float))\n",
    "        Ys = torch.tensor([[0],[1],[1],[1]], dtype=torch.float)\n",
    "        OR_loss = 1.0/(1.0+criterion(prediction, Ys))\n",
    "            \n",
    "        prediction = network(torch.tensor([[0,0,1],[0,1,1],[1,0,1],[1,1,1]], dtype=torch.float))\n",
    "        Ys = torch.tensor([[0],[1],[1],[0]], dtype=torch.float)\n",
    "        XOR_loss = 1.0/(1.0+criterion(prediction, Ys))\n",
    "            \n",
    "        pre_pre_loss_or.append(OR_loss.item())\n",
    "        pre_pre_loss_xor.append(XOR_loss.item())\n",
    "        \n",
    "        if train_or:\n",
    "            for i in range(37*1000):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                Xs, Ys = generate_both(25,0.1)\n",
    "                    \n",
    "                Xs = torch.tensor(Xs)\n",
    "                Ys = torch.tensor(Ys, dtype=torch.float)\n",
    "\n",
    "                prediction = network(Xs)\n",
    "                loss = criterion(prediction, Ys)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # Evaluation\n",
    "                    prediction = network(torch.tensor([[0,0,0],[0,1,0],[1,0,0],[1,1,0]], dtype=torch.float))\n",
    "                    Ys = torch.tensor([[0],[1],[1],[1]], dtype=torch.float)\n",
    "                    loss = 1.0/(1.0+criterion(prediction, Ys))\n",
    "\n",
    "                if loss>0.95:\n",
    "                    break\n",
    "                    \n",
    "        prediction = network(torch.tensor([[0,0,0],[0,1,0],[1,0,0],[1,1,0]], dtype=torch.float))\n",
    "        Ys = torch.tensor([[0],[1],[1],[1]], dtype=torch.float)\n",
    "        OR_loss = 1.0/(1.0+criterion(prediction, Ys))\n",
    "            \n",
    "        prediction = network(torch.tensor([[0,0,1],[0,1,1],[1,0,1],[1,1,1]], dtype=torch.float))\n",
    "        Ys = torch.tensor([[0],[1],[1],[0]], dtype=torch.float)\n",
    "        XOR_loss = 1.0/(1.0+criterion(prediction, Ys))\n",
    "            \n",
    "        pre_loss_or.append(OR_loss.item())\n",
    "        pre_loss_xor.append(XOR_loss.item())\n",
    "        \n",
    "        if capacity is not None:\n",
    "            network, optimizer = increase_capacity_keep_lr(network, capacity, optimizer, 'cpu')\n",
    "        \n",
    "        iters = 41*1000\n",
    "        if not train_or:\n",
    "            iters * 2\n",
    "            \n",
    "        for i in range(iters):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Uniform syllabus 20% of the time\n",
    "            Xs, Ys = generate_both(25,0.9)\n",
    "                \n",
    "            Xs = torch.tensor(Xs)\n",
    "            Ys = torch.tensor(Ys, dtype=torch.float)\n",
    "\n",
    "            prediction = network(Xs)\n",
    "            loss = criterion(prediction, Ys)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        average_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Test or\n",
    "            prediction = network(torch.tensor([[0,0,0],[0,1,0],[1,0,0],[1,1,0]], dtype=torch.float))\n",
    "            Ys = torch.tensor([[0],[1],[1],[1]], dtype=torch.float)\n",
    "            loss = 1.0/(1.0+criterion(prediction, Ys))\n",
    "        \n",
    "        average_loss += loss.item()\n",
    "        losses_or.append(loss.item())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Test xor\n",
    "            prediction = network(torch.tensor([[0,0,1],[0,1,1],[1,0,1],[1,1,1]], dtype=torch.float))\n",
    "            Ys = torch.tensor([[0],[1],[1],[0]], dtype=torch.float)\n",
    "            loss = 1.0/(1.0+criterion(prediction, Ys))\n",
    "        \n",
    "        average_loss += loss.item()\n",
    "        average_loss /= 2\n",
    "        losses_xor.append(loss.item())\n",
    "        \n",
    "        if average_loss > lowest_loss:\n",
    "            lowest_loss = copy.copy(average_loss)\n",
    "            lowest_settings = [average_loss, losses_or[-1], losses_xor[-1], seed, initial_capacity, train_or, capacity]\n",
    "        \n",
    "        if loss>0.95:\n",
    "            break\n",
    "        \n",
    "    # Print statistics\n",
    "    print(initial_capacity, train_or, capacity)\n",
    "    \n",
    "    print('Average loss or before xor training: ', np.average(pre_pre_loss_or))\n",
    "    print('Average loss xor before xor training: ', np.average(pre_pre_loss_xor))\n",
    "    \n",
    "    print('Average loss or before xor training: ', np.average(pre_loss_or))\n",
    "    print('Average loss xor before xor training: ', np.average(pre_loss_xor))\n",
    "    print('Average loss or: ', np.average(losses_or))\n",
    "    print('Average loss xor: ', np.average(losses_xor))\n",
    "    print('Average loss: ', (np.average(losses_or) +  np.average(losses_xor))/2)\n",
    "    print(lowest_settings)\n",
    "\n",
    "\n",
    "xor_experiments([2],False, None)\n",
    "xor_experiments([2], True, None)\n",
    "xor_experiments([1], True, [1])\n",
    "\n",
    "xor_experiments([3],False,None)\n",
    "xor_experiments([3], True, None)\n",
    "xor_experiments([2], True, [1])\n",
    "xor_experiments([1], True, [2])\n",
    "\n",
    "xor_experiments([4],False,None)\n",
    "xor_experiments([4], True, None)\n",
    "xor_experiments([1], True, [3])\n",
    "xor_experiments([2], True, [2])\n",
    "xor_experiments([3], True, [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Writer = animation.writers['ffmpeg']\n",
    "writer = Writer(fps=24, metadata=dict(artist='Joe Harrison'), bitrate=1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8552559614181519\n",
      "1 0.9450685381889343\n",
      "2 0.8479112982749939\n",
      "3 0.8452067971229553\n",
      "4 0.8431272506713867\n",
      "5 0.8419037461280823\n",
      "6 0.8403158783912659\n",
      "7 0.853485643863678\n",
      "8 0.8190130591392517\n",
      "9 0.8330321311950684\n",
      "10 0.8607609868049622\n",
      "11 0.8887335658073425\n",
      "12 0.8406065702438354\n",
      "13 0.8083235621452332\n",
      "14 0.8525921106338501\n",
      "15 0.8209795951843262\n",
      "16 0.8337359428405762\n",
      "17 0.8464990854263306\n",
      "18 0.8744804263114929\n",
      "19 0.8732001781463623\n",
      "20 0.8718239068984985\n",
      "21 0.8252975940704346\n",
      "22 0.8088929057121277\n",
      "23 0.8660187125205994\n",
      "24 0.8351905941963196\n",
      "25 0.8483043313026428\n",
      "26 0.8612344861030579\n",
      "27 0.801175594329834\n",
      "28 0.8433561325073242\n",
      "29 0.8272910714149475\n",
      "30 0.825671374797821\n",
      "31 0.8386083245277405\n",
      "32 0.8080980181694031\n",
      "33 0.8065598011016846\n",
      "34 0.8194347620010376\n",
      "35 0.8324432373046875\n",
      "36 0.8019487857818604\n",
      "37 0.8004421591758728\n",
      "38 0.7990315556526184\n",
      "39 0.7830743193626404\n",
      "40 0.7959089875221252\n",
      "41 0.8230062127113342\n",
      "42 0.8216138482093811\n",
      "43 0.8199988603591919\n",
      "44 0.8042615652084351\n",
      "45 0.8311848640441895\n",
      "46 0.8012425303459167\n",
      "47 0.7997298240661621\n",
      "48 0.7981914281845093\n",
      "49 0.7684083580970764\n",
      "50 0.7668850421905518\n",
      "51 0.7654378414154053\n",
      "52 0.7922197580337524\n",
      "53 0.8189395666122437\n",
      "54 0.7611661553382874\n",
      "55 0.7597345113754272\n",
      "56 0.8004187345504761\n",
      "57 0.7708949446678162\n",
      "58 0.7694916129112244\n",
      "59 0.7819969058036804\n",
      "60 0.7665785551071167\n",
      "61 0.7512184381484985\n",
      "62 0.805470883846283\n",
      "63 0.7901317477226257\n",
      "64 0.7608827948570251\n",
      "65 0.7733210325241089\n",
      "66 0.7857686877250671\n",
      "67 0.7842833995819092\n",
      "68 0.7966096997261047\n",
      "69 0.7400662302970886\n",
      "70 0.7661779522895813\n",
      "71 0.7372621297836304\n",
      "72 0.7908030152320862\n",
      "73 0.7619512677192688\n",
      "74 0.7467906475067139\n",
      "75 0.7591116428375244\n",
      "76 0.7440585494041443\n",
      "77 0.7290523648262024\n",
      "78 0.7684581875801086\n",
      "79 0.7263364195823669\n",
      "80 0.7385363578796387\n",
      "81 0.7100732326507568\n",
      "82 0.7493525743484497\n",
      "83 0.7614718675613403\n",
      "84 0.7466650009155273\n",
      "85 0.7721379995346069\n",
      "86 0.7438799142837524\n",
      "87 0.7558759450912476\n",
      "88 0.7410749197006226\n",
      "89 0.7397230863571167\n",
      "90 0.7116170525550842\n",
      "91 0.7236648797988892\n",
      "92 0.7356104850769043\n",
      "93 0.7209742665290833\n",
      "94 0.6930270195007324\n",
      "95 0.7050197720527649\n",
      "96 0.7037729024887085\n",
      "97 0.689204216003418\n",
      "98 0.7011690139770508\n",
      "99 0.7656254768371582\n",
      "100 0.711726725101471\n",
      "101 0.7102677226066589\n",
      "102 0.7220780253410339\n",
      "103 0.6945656538009644\n",
      "104 0.7444810271263123\n",
      "105 0.7175521850585938\n",
      "106 0.7155548334121704\n",
      "107 0.7390755414962769\n",
      "108 0.7002493739128113\n",
      "109 0.7108878493309021\n",
      "110 0.6849908232688904\n",
      "111 0.7073580026626587\n",
      "112 0.7061951160430908\n",
      "113 0.7039746642112732\n",
      "114 0.6675726771354675\n",
      "115 0.6886816620826721\n",
      "116 0.6756435632705688\n",
      "117 0.6724861860275269\n",
      "118 0.6926295757293701\n",
      "119 0.699185311794281\n",
      "120 0.662056565284729\n",
      "121 0.645723283290863\n",
      "122 0.6528359055519104\n",
      "123 0.6689386963844299\n",
      "124 0.663753092288971\n",
      "125 0.6276832818984985\n",
      "126 0.6644313931465149\n",
      "127 0.6588100790977478\n",
      "128 0.6443999409675598\n",
      "129 0.6581953167915344\n",
      "130 0.652580738067627\n",
      "131 0.605952262878418\n",
      "132 0.6100049614906311\n",
      "133 0.6046968102455139\n",
      "134 0.5881971716880798\n",
      "135 0.5934988856315613\n",
      "136 0.5967012643814087\n",
      "137 0.5815019011497498\n",
      "138 0.5840381383895874\n",
      "139 0.5884881615638733\n",
      "140 0.5561056137084961\n",
      "141 0.5792514085769653\n",
      "142 0.5719745755195618\n",
      "143 0.5370991826057434\n",
      "144 0.5418393015861511\n",
      "145 0.5358181595802307\n",
      "146 0.5395781993865967\n",
      "147 0.544011652469635\n",
      "148 0.5174129605293274\n",
      "149 0.5314580202102661\n",
      "150 0.5261061787605286\n",
      "151 0.5100818872451782\n",
      "152 0.48676368594169617\n",
      "153 0.4803326725959778\n",
      "154 0.475059449672699\n",
      "155 0.4774511456489563\n",
      "156 0.48138394951820374\n",
      "157 0.49543729424476624\n",
      "158 0.4714728593826294\n",
      "159 0.46458858251571655\n",
      "160 0.4579814076423645\n",
      "161 0.4537622034549713\n",
      "162 0.4485902488231659\n",
      "163 0.42646676301956177\n",
      "164 0.425601065158844\n",
      "165 0.43120282888412476\n",
      "166 0.4181354343891144\n",
      "167 0.4036388397216797\n",
      "168 0.3981570899486542\n",
      "169 0.4028848111629486\n",
      "170 0.417057067155838\n",
      "171 0.413498193025589\n",
      "172 0.40951937437057495\n",
      "173 0.38235464692115784\n",
      "174 0.3671515882015228\n",
      "175 0.36913344264030457\n",
      "176 0.3873949348926544\n",
      "177 0.3649671971797943\n",
      "178 0.349488765001297\n",
      "179 0.3539247512817383\n",
      "180 0.3556980788707733\n",
      "181 0.3362964391708374\n",
      "182 0.33168378472328186\n",
      "183 0.3423681855201721\n",
      "184 0.31935176253318787\n",
      "185 0.3372969925403595\n",
      "186 0.3121366500854492\n",
      "187 0.3210758864879608\n",
      "188 0.3386039733886719\n",
      "189 0.32944706082344055\n",
      "190 0.3105068504810333\n",
      "191 0.3197801113128662\n",
      "192 0.3002970516681671\n",
      "193 0.30646246671676636\n",
      "194 0.29466119408607483\n",
      "195 0.3038114905357361\n",
      "196 0.3022531569004059\n",
      "197 0.2907963693141937\n",
      "198 0.29175204038619995\n",
      "199 0.28554901480674744\n",
      "200 0.2894009053707123\n",
      "201 0.27849432826042175\n",
      "202 0.27746424078941345\n",
      "203 0.270408570766449\n",
      "204 0.27836763858795166\n",
      "205 0.2889975607395172\n",
      "206 0.2794093191623688\n",
      "207 0.27091875672340393\n",
      "208 0.2773537337779999\n",
      "209 0.2710680663585663\n",
      "210 0.2756192982196808\n",
      "211 0.2671191096305847\n",
      "212 0.26376646757125854\n",
      "213 0.2613224387168884\n",
      "214 0.26636770367622375\n",
      "215 0.26843568682670593\n",
      "216 0.26974064111709595\n",
      "217 0.27359071373939514\n",
      "218 0.27169618010520935\n",
      "219 0.26724255084991455\n",
      "220 0.2682466208934784\n",
      "221 0.25800952315330505\n",
      "222 0.2539341449737549\n",
      "223 0.26636648178100586\n",
      "224 0.2551901936531067\n",
      "225 0.26017501950263977\n",
      "226 0.2557642459869385\n",
      "227 0.2564447820186615\n",
      "228 0.2558169364929199\n",
      "229 0.2601027190685272\n",
      "230 0.26113247871398926\n",
      "231 0.2513505518436432\n",
      "232 0.2614925801753998\n",
      "233 0.2591927647590637\n",
      "234 0.2520880699157715\n",
      "235 0.2557724118232727\n",
      "236 0.26229992508888245\n",
      "237 0.26437580585479736\n",
      "238 0.2602785527706146\n",
      "239 0.2538752555847168\n",
      "240 0.25767433643341064\n",
      "241 0.2507748603820801\n",
      "242 0.26538094878196716\n",
      "243 0.255705863237381\n",
      "244 0.2556239664554596\n",
      "245 0.2504372000694275\n",
      "246 0.2555665969848633\n",
      "247 0.2600933015346527\n",
      "248 0.26132652163505554\n",
      "249 0.26017600297927856\n",
      "250 0.2523621916770935\n",
      "251 0.2575021982192993\n",
      "252 0.2635636329650879\n",
      "253 0.2587268054485321\n",
      "254 0.2508828341960907\n",
      "255 0.2552783489227295\n",
      "256 0.2613995373249054\n",
      "257 0.2567328214645386\n",
      "258 0.26282623410224915\n",
      "259 0.25940778851509094\n",
      "260 0.2606067657470703\n",
      "261 0.25725600123405457\n",
      "262 0.25588056445121765\n",
      "263 0.24928122758865356\n",
      "264 0.2512291371822357\n",
      "265 0.2594563066959381\n",
      "266 0.2563702464103699\n",
      "267 0.25623759627342224\n",
      "268 0.2575261890888214\n",
      "269 0.25836339592933655\n",
      "270 0.2545299828052521\n",
      "271 0.25951746106147766\n",
      "272 0.2590174078941345\n",
      "273 0.26077815890312195\n",
      "274 0.259098082780838\n",
      "275 0.2579006850719452\n",
      "276 0.25273633003234863\n",
      "277 0.25878238677978516\n",
      "278 0.2532573640346527\n",
      "279 0.25667905807495117\n",
      "280 0.25727298855781555\n",
      "281 0.262517511844635\n",
      "282 0.2615143954753876\n",
      "283 0.2544214129447937\n",
      "284 0.25963637232780457\n",
      "285 0.2529968321323395\n",
      "286 0.2570157051086426\n",
      "287 0.25763240456581116\n",
      "288 0.25899308919906616\n",
      "289 0.25432777404785156\n",
      "290 0.2586723268032074\n",
      "291 0.2638934850692749\n",
      "292 0.2627159357070923\n",
      "293 0.25113773345947266\n",
      "294 0.257592111825943\n",
      "295 0.26154854893684387\n",
      "296 0.2656285762786865\n",
      "297 0.26356860995292664\n",
      "298 0.2554726302623749\n",
      "299 0.25748103857040405\n",
      "300 0.24770045280456543\n",
      "301 0.259786993265152\n",
      "302 0.2539491057395935\n",
      "303 0.25496745109558105\n",
      "304 0.26136666536331177\n",
      "305 0.25602686405181885\n",
      "306 0.25427332520484924\n",
      "307 0.26182499527931213\n",
      "308 0.2580891251564026\n",
      "309 0.2561444342136383\n",
      "310 0.25650396943092346\n",
      "311 0.2599859833717346\n",
      "312 0.26014429330825806\n",
      "313 0.2549941837787628\n",
      "314 0.25928306579589844\n",
      "315 0.25253826379776\n",
      "316 0.26439595222473145\n",
      "317 0.2594345808029175\n",
      "318 0.2550170123577118\n",
      "319 0.2626103162765503\n",
      "320 0.25785940885543823\n",
      "321 0.26082929968833923\n",
      "322 0.262588769197464\n",
      "323 0.2528897523880005\n",
      "324 0.2554323673248291\n",
      "325 0.2602035105228424\n",
      "326 0.25378450751304626\n",
      "327 0.2543523609638214\n",
      "328 0.2573019564151764\n",
      "329 0.2575858533382416\n",
      "330 0.2628733217716217\n",
      "331 0.25881895422935486\n",
      "332 0.2533802092075348\n",
      "333 0.2574256658554077\n",
      "334 0.25181061029434204\n",
      "335 0.25533437728881836\n",
      "336 0.25604647397994995\n",
      "337 0.2589487135410309\n",
      "338 0.2638697922229767\n",
      "339 0.25416770577430725\n",
      "340 0.25611385703086853\n",
      "341 0.2588033080101013\n",
      "342 0.2614348828792572\n",
      "343 0.25590023398399353\n",
      "344 0.25504887104034424\n",
      "345 0.26166167855262756\n",
      "346 0.25556591153144836\n",
      "347 0.25801411271095276\n",
      "348 0.2579597234725952\n",
      "349 0.2600793242454529\n",
      "350 0.25490397214889526\n",
      "351 0.2592046558856964\n",
      "352 0.2575492858886719\n",
      "353 0.25794458389282227\n",
      "354 0.25089433789253235\n",
      "355 0.25370919704437256\n",
      "356 0.2583412528038025\n",
      "357 0.2596862316131592\n",
      "358 0.25852665305137634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359 0.25323063135147095\n",
      "360 0.2610549330711365\n",
      "361 0.2551250159740448\n",
      "362 0.2639027237892151\n",
      "363 0.25560733675956726\n",
      "364 0.25369173288345337\n",
      "365 0.25213244557380676\n",
      "366 0.2588612735271454\n",
      "367 0.2515970766544342\n",
      "368 0.26022061705589294\n",
      "369 0.25410306453704834\n",
      "370 0.2586001753807068\n",
      "371 0.2528402805328369\n",
      "372 0.252511590719223\n",
      "373 0.260211706161499\n",
      "374 0.24943773448467255\n",
      "375 0.25179004669189453\n",
      "376 0.2567742168903351\n",
      "377 0.25429844856262207\n",
      "378 0.26269152760505676\n",
      "379 0.2514294981956482\n",
      "380 0.25524723529815674\n",
      "381 0.2608136534690857\n",
      "382 0.25660353899002075\n",
      "383 0.25541526079177856\n",
      "384 0.2510131299495697\n",
      "385 0.2528967261314392\n",
      "386 0.25573399662971497\n",
      "387 0.2569952607154846\n",
      "388 0.25276893377304077\n",
      "389 0.2513931393623352\n",
      "390 0.25748172402381897\n",
      "391 0.2587720453739166\n",
      "392 0.25585606694221497\n",
      "393 0.25326260924339294\n",
      "394 0.2524624168872833\n",
      "395 0.25284233689308167\n",
      "396 0.24664589762687683\n",
      "397 0.2600404620170593\n",
      "398 0.2571449279785156\n",
      "399 0.25478118658065796\n",
      "400 0.25792115926742554\n",
      "401 0.25859108567237854\n",
      "402 0.2561950981616974\n",
      "403 0.2562752962112427\n",
      "404 0.2542458474636078\n",
      "405 0.2552769184112549\n",
      "406 0.2580271065235138\n",
      "407 0.25748211145401\n",
      "408 0.2500983774662018\n",
      "409 0.25400322675704956\n",
      "410 0.25409892201423645\n",
      "411 0.2619243264198303\n",
      "412 0.25794702768325806\n",
      "413 0.2542496621608734\n",
      "414 0.25490275025367737\n",
      "415 0.25486618280410767\n",
      "416 0.24648314714431763\n",
      "417 0.25393056869506836\n",
      "418 0.2523379325866699\n",
      "419 0.2571188807487488\n",
      "420 0.258661150932312\n",
      "421 0.25564926862716675\n",
      "422 0.25385257601737976\n",
      "423 0.26071101427078247\n",
      "424 0.25344306230545044\n",
      "425 0.25740697979927063\n",
      "426 0.25393155217170715\n",
      "427 0.25794780254364014\n",
      "428 0.2519438564777374\n",
      "429 0.2550671100616455\n",
      "430 0.2622978389263153\n",
      "431 0.25989189743995667\n",
      "432 0.26116278767585754\n",
      "433 0.2521701455116272\n",
      "434 0.25760072469711304\n",
      "435 0.2509540915489197\n",
      "436 0.25743040442466736\n",
      "437 0.25724390149116516\n",
      "438 0.25337499380111694\n",
      "439 0.2600659430027008\n",
      "440 0.2539771497249603\n",
      "441 0.25529974699020386\n",
      "442 0.2576274871826172\n",
      "443 0.25413474440574646\n",
      "444 0.26010575890541077\n",
      "445 0.25564149022102356\n",
      "446 0.25392889976501465\n",
      "447 0.25945931673049927\n",
      "448 0.25666892528533936\n",
      "449 0.2561132609844208\n",
      "450 0.25718554854393005\n",
      "451 0.25617945194244385\n",
      "452 0.25313231348991394\n",
      "453 0.26017865538597107\n",
      "454 0.2554454505443573\n",
      "455 0.25259914994239807\n",
      "456 0.25514182448387146\n",
      "457 0.25872743129730225\n",
      "458 0.2599005699157715\n",
      "459 0.2543741762638092\n",
      "460 0.2590178847312927\n",
      "461 0.2592378854751587\n",
      "462 0.256748765707016\n",
      "463 0.2552129626274109\n",
      "464 0.2567903399467468\n",
      "465 0.2577962875366211\n",
      "466 0.2504936754703522\n",
      "467 0.24813254177570343\n",
      "468 0.25351959466934204\n",
      "469 0.250954806804657\n",
      "470 0.25302985310554504\n",
      "471 0.25824791193008423\n",
      "472 0.2514178156852722\n",
      "473 0.2569645047187805\n",
      "474 0.25590991973876953\n",
      "475 0.25000569224357605\n",
      "476 0.25523242354393005\n",
      "477 0.25141018629074097\n",
      "478 0.2554522752761841\n",
      "479 0.25052332878112793\n",
      "480 0.25918176770210266\n",
      "481 0.2539653778076172\n",
      "482 0.2564166188240051\n",
      "483 0.25549980998039246\n",
      "484 0.2545480728149414\n",
      "485 0.2536798417568207\n",
      "486 0.25664809346199036\n",
      "487 0.24954769015312195\n",
      "488 0.25661933422088623\n",
      "489 0.25190427899360657\n",
      "490 0.2607698142528534\n",
      "491 0.25231146812438965\n",
      "492 0.25138840079307556\n",
      "493 0.251128226518631\n",
      "494 0.25562259554862976\n",
      "495 0.2536735534667969\n",
      "496 0.25281408429145813\n",
      "497 0.25494930148124695\n",
      "498 0.25555387139320374\n",
      "499 0.2539655268192291\n",
      "0 0.2468680739402771\n",
      "1 0.24824842810630798\n",
      "2 0.25137922167778015\n",
      "3 0.24315302073955536\n",
      "4 0.24542857706546783\n",
      "5 0.24667127430438995\n",
      "6 0.23383717238903046\n",
      "7 0.23273219168186188\n",
      "8 0.2222450226545334\n",
      "9 0.2287784069776535\n",
      "10 0.21750839054584503\n",
      "11 0.2165352702140808\n",
      "12 0.22062291204929352\n",
      "13 0.2084939032793045\n",
      "14 0.20967602729797363\n",
      "15 0.19430294632911682\n",
      "16 0.19339808821678162\n",
      "17 0.1888197809457779\n",
      "18 0.18361234664916992\n",
      "19 0.17986749112606049\n",
      "20 0.18019507825374603\n",
      "21 0.178195521235466\n",
      "22 0.1915961056947708\n",
      "23 0.16006317734718323\n",
      "24 0.15814724564552307\n",
      "25 0.16116690635681152\n",
      "26 0.15393653512001038\n",
      "27 0.15244996547698975\n",
      "28 0.16188909113407135\n",
      "29 0.14104585349559784\n",
      "30 0.1820501834154129\n",
      "31 0.15927754342556\n",
      "32 0.16813144087791443\n",
      "33 0.14272236824035645\n",
      "34 0.1485445648431778\n",
      "35 0.15469682216644287\n",
      "36 0.14448805153369904\n",
      "37 0.1640983670949936\n",
      "38 0.1616707593202591\n",
      "39 0.16786327958106995\n",
      "40 0.167902871966362\n",
      "41 0.1598023921251297\n",
      "42 0.16889454424381256\n",
      "43 0.14049360156059265\n",
      "44 0.14393027126789093\n",
      "45 0.14234523475170135\n",
      "46 0.16116680204868317\n",
      "47 0.1483832597732544\n",
      "48 0.16187593340873718\n",
      "49 0.15819522738456726\n",
      "50 0.1436636596918106\n",
      "51 0.14334620535373688\n",
      "52 0.15772366523742676\n",
      "53 0.17025600373744965\n",
      "54 0.14964662492275238\n",
      "55 0.18835292756557465\n",
      "56 0.16780437529087067\n",
      "57 0.1549438238143921\n",
      "58 0.14379934966564178\n",
      "59 0.1500871330499649\n",
      "60 0.137781023979187\n",
      "61 0.15215051174163818\n",
      "62 0.17565220594406128\n",
      "63 0.1556759923696518\n",
      "64 0.13774974644184113\n",
      "65 0.1612984538078308\n",
      "66 0.15923738479614258\n",
      "67 0.15724879503250122\n",
      "68 0.15493342280387878\n",
      "69 0.1621389240026474\n",
      "70 0.14540262520313263\n",
      "71 0.15054725110530853\n",
      "72 0.16657313704490662\n",
      "73 0.14790885150432587\n",
      "74 0.15622378885746002\n",
      "75 0.16217422485351562\n",
      "76 0.1694469302892685\n",
      "77 0.16435541212558746\n",
      "78 0.14020991325378418\n",
      "79 0.1288902759552002\n",
      "80 0.14609824120998383\n",
      "81 0.14492659270763397\n",
      "82 0.1533890813589096\n",
      "83 0.15923117101192474\n",
      "84 0.1420559585094452\n",
      "85 0.14155204594135284\n",
      "86 0.1608402132987976\n",
      "87 0.16261297464370728\n",
      "88 0.1639638990163803\n",
      "89 0.14227329194545746\n",
      "90 0.13558095693588257\n",
      "91 0.14587315917015076\n",
      "92 0.14083831012248993\n",
      "93 0.1472615748643875\n",
      "94 0.1534757912158966\n",
      "95 0.15805746614933014\n",
      "96 0.12338278442621231\n",
      "97 0.12553611397743225\n",
      "98 0.13753508031368256\n",
      "99 0.14118660986423492\n",
      "100 0.1454390287399292\n",
      "101 0.15582753717899323\n",
      "102 0.1497432440519333\n",
      "103 0.12743976712226868\n",
      "104 0.1342407763004303\n",
      "105 0.14571025967597961\n",
      "106 0.14748646318912506\n",
      "107 0.16961567103862762\n",
      "108 0.15956906974315643\n",
      "109 0.15978941321372986\n",
      "110 0.1737077832221985\n",
      "111 0.15584929287433624\n",
      "112 0.1289779245853424\n",
      "113 0.14282973110675812\n",
      "114 0.16010034084320068\n",
      "115 0.1432414948940277\n",
      "116 0.14791202545166016\n",
      "117 0.14346855878829956\n",
      "118 0.1481061428785324\n",
      "119 0.1473957896232605\n",
      "120 0.14903047680854797\n",
      "121 0.16226664185523987\n",
      "122 0.16853131353855133\n",
      "123 0.15883636474609375\n",
      "124 0.17520280182361603\n",
      "125 0.14246253669261932\n",
      "126 0.1381189525127411\n",
      "127 0.16889949142932892\n",
      "128 0.16910520195960999\n",
      "129 0.14371807873249054\n",
      "130 0.1549176126718521\n",
      "131 0.185626819729805\n",
      "132 0.140080526471138\n",
      "133 0.18549920618534088\n",
      "134 0.14729009568691254\n",
      "135 0.14138861000537872\n",
      "136 0.12794218957424164\n",
      "137 0.14394445717334747\n",
      "138 0.14564502239227295\n",
      "139 0.14849014580249786\n",
      "140 0.1673537790775299\n",
      "141 0.1582530438899994\n",
      "142 0.16010932624340057\n",
      "143 0.16200491786003113\n",
      "144 0.15009596943855286\n",
      "145 0.14331364631652832\n",
      "146 0.14609016478061676\n",
      "147 0.13783304393291473\n",
      "148 0.14668792486190796\n",
      "149 0.1708458513021469\n",
      "150 0.15823155641555786\n",
      "151 0.14186492562294006\n",
      "152 0.1714673489332199\n",
      "153 0.15376663208007812\n",
      "154 0.15234188735485077\n",
      "155 0.15836824476718903\n",
      "156 0.12750214338302612\n",
      "157 0.12626323103904724\n",
      "158 0.14597827196121216\n",
      "159 0.13719746470451355\n",
      "160 0.16376659274101257\n",
      "161 0.13095375895500183\n",
      "162 0.1407589316368103\n",
      "163 0.17105688154697418\n",
      "164 0.1613103747367859\n",
      "165 0.16714075207710266\n",
      "166 0.13958106935024261\n",
      "167 0.18104127049446106\n",
      "168 0.15490128099918365\n",
      "169 0.13983848690986633\n",
      "170 0.15303589403629303\n",
      "171 0.14837490022182465\n",
      "172 0.1432981938123703\n",
      "173 0.1439683586359024\n",
      "174 0.15249116718769073\n",
      "175 0.14019834995269775\n",
      "176 0.16879802942276\n",
      "177 0.14356139302253723\n",
      "178 0.15084898471832275\n",
      "179 0.13062900304794312\n",
      "180 0.12903869152069092\n",
      "181 0.15407052636146545\n",
      "182 0.1420762836933136\n",
      "183 0.11951687186956406\n",
      "184 0.1447521597146988\n",
      "185 0.14883458614349365\n",
      "186 0.13991431891918182\n",
      "187 0.12185972183942795\n",
      "188 0.1438315361738205\n",
      "189 0.14564865827560425\n",
      "190 0.15967713296413422\n",
      "191 0.16374684870243073\n",
      "192 0.14311985671520233\n",
      "193 0.12057656049728394\n",
      "194 0.12837420403957367\n",
      "195 0.12505459785461426\n",
      "196 0.14507777988910675\n",
      "197 0.12460532784461975\n",
      "198 0.1617870032787323\n",
      "199 0.11610911786556244\n",
      "200 0.12208738178014755\n",
      "201 0.16806766390800476\n",
      "202 0.14484290778636932\n",
      "203 0.17585371434688568\n",
      "204 0.1399534046649933\n",
      "205 0.13839200139045715\n",
      "206 0.13672015070915222\n",
      "207 0.15794847905635834\n",
      "208 0.14859364926815033\n",
      "209 0.13446810841560364\n",
      "210 0.12888139486312866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211 0.15177854895591736\n",
      "212 0.12468124181032181\n",
      "213 0.14028051495552063\n",
      "214 0.14338894188404083\n",
      "215 0.134041890501976\n",
      "216 0.12157458066940308\n",
      "217 0.13371233642101288\n",
      "218 0.14199139177799225\n",
      "219 0.12691929936408997\n",
      "220 0.1627533882856369\n",
      "221 0.13165035843849182\n",
      "222 0.1486794799566269\n",
      "223 0.13653267920017242\n",
      "224 0.14311279356479645\n",
      "225 0.16049237549304962\n",
      "226 0.11671725660562515\n",
      "227 0.1303177922964096\n",
      "228 0.13496075570583344\n",
      "229 0.12444890290498734\n",
      "230 0.12959066033363342\n",
      "231 0.15571331977844238\n",
      "232 0.15834420919418335\n",
      "233 0.1474006474018097\n",
      "234 0.1190827265381813\n",
      "235 0.16836807131767273\n",
      "236 0.14310632646083832\n",
      "237 0.14346548914909363\n",
      "238 0.13373439013957977\n",
      "239 0.1154412105679512\n",
      "240 0.14146891236305237\n",
      "241 0.14503788948059082\n",
      "242 0.1472846269607544\n",
      "243 0.12150600552558899\n",
      "244 0.13131406903266907\n",
      "245 0.13532134890556335\n",
      "246 0.1575714647769928\n",
      "247 0.13636338710784912\n",
      "248 0.12845712900161743\n",
      "249 0.1625880002975464\n",
      "250 0.12528052926063538\n",
      "251 0.12990307807922363\n",
      "252 0.14700758457183838\n",
      "253 0.1336670219898224\n",
      "254 0.1288730800151825\n",
      "255 0.1410267949104309\n",
      "256 0.12136603146791458\n",
      "257 0.13725632429122925\n",
      "258 0.13780565559864044\n",
      "259 0.1345834732055664\n",
      "260 0.11685761064291\n",
      "261 0.13134565949440002\n",
      "262 0.13727539777755737\n",
      "263 0.13973873853683472\n",
      "264 0.12286695837974548\n",
      "265 0.1519441455602646\n",
      "266 0.1254013478755951\n",
      "267 0.13944745063781738\n",
      "268 0.1644136607646942\n",
      "269 0.15199187397956848\n",
      "270 0.135292187333107\n",
      "271 0.12188363820314407\n",
      "272 0.1436808556318283\n",
      "273 0.1699967086315155\n",
      "274 0.14066597819328308\n",
      "275 0.12866173684597015\n",
      "276 0.1362105906009674\n",
      "277 0.1187707856297493\n",
      "278 0.12945766746997833\n",
      "279 0.12286722660064697\n",
      "280 0.13157297670841217\n",
      "281 0.1570521891117096\n",
      "282 0.12726229429244995\n",
      "283 0.13472896814346313\n",
      "284 0.1257350891828537\n",
      "285 0.14071282744407654\n",
      "286 0.1506560742855072\n",
      "287 0.10710388422012329\n",
      "288 0.12513834238052368\n",
      "289 0.14070670306682587\n",
      "290 0.14712104201316833\n",
      "291 0.1507861316204071\n",
      "292 0.14670829474925995\n",
      "293 0.1330796331167221\n",
      "294 0.1439778059720993\n",
      "295 0.13078904151916504\n",
      "296 0.13153210282325745\n",
      "297 0.11764254420995712\n",
      "298 0.12453412264585495\n",
      "299 0.13277673721313477\n",
      "300 0.11511076986789703\n",
      "301 0.13056375086307526\n",
      "302 0.1365332156419754\n",
      "303 0.15725071728229523\n",
      "304 0.1557386815547943\n",
      "305 0.1569087654352188\n",
      "306 0.10730815678834915\n",
      "307 0.12474623322486877\n",
      "308 0.13652388751506805\n",
      "309 0.12227877229452133\n",
      "310 0.14643128216266632\n",
      "311 0.12393087893724442\n",
      "312 0.11232826858758926\n",
      "313 0.1402863711118698\n",
      "314 0.15032385289669037\n",
      "315 0.13907387852668762\n",
      "316 0.12027297168970108\n",
      "317 0.14523285627365112\n",
      "318 0.15978950262069702\n",
      "319 0.13399523496627808\n",
      "320 0.13383063673973083\n",
      "321 0.12532852590084076\n",
      "322 0.14318379759788513\n",
      "323 0.151375874876976\n",
      "324 0.13350482285022736\n",
      "325 0.15065425634384155\n",
      "326 0.1385701447725296\n",
      "327 0.1273256540298462\n",
      "328 0.14723534882068634\n",
      "329 0.14530254900455475\n",
      "330 0.1357172578573227\n",
      "331 0.12122722715139389\n",
      "332 0.1601468324661255\n",
      "333 0.1355268657207489\n",
      "334 0.16356109082698822\n",
      "335 0.13694415986537933\n",
      "336 0.14242354035377502\n",
      "337 0.1596483290195465\n",
      "338 0.11253612488508224\n",
      "339 0.12765684723854065\n",
      "340 0.13562831282615662\n",
      "341 0.13585257530212402\n",
      "342 0.12697158753871918\n",
      "343 0.13748948276042938\n",
      "344 0.1443551778793335\n",
      "345 0.14339454472064972\n",
      "346 0.1355873942375183\n",
      "347 0.1415337175130844\n",
      "348 0.12796737253665924\n",
      "349 0.1267053186893463\n",
      "350 0.13765019178390503\n",
      "351 0.1437685489654541\n",
      "352 0.11553726345300674\n",
      "353 0.131724551320076\n",
      "354 0.12478575110435486\n",
      "355 0.12385013699531555\n",
      "356 0.1280895322561264\n",
      "357 0.11472322046756744\n",
      "358 0.12773990631103516\n",
      "359 0.15195225179195404\n",
      "360 0.1588331013917923\n",
      "361 0.14680103957653046\n",
      "362 0.1307784765958786\n",
      "363 0.15231537818908691\n",
      "364 0.12019633501768112\n",
      "365 0.14243198931217194\n",
      "366 0.13215461373329163\n",
      "367 0.11711665242910385\n",
      "368 0.11955215036869049\n",
      "369 0.13389518857002258\n",
      "370 0.13461612164974213\n",
      "371 0.11865728348493576\n",
      "372 0.14346346259117126\n",
      "373 0.13451331853866577\n",
      "374 0.12639100849628448\n",
      "375 0.14954133331775665\n",
      "376 0.1286453902721405\n",
      "377 0.14968620240688324\n",
      "378 0.14166080951690674\n",
      "379 0.1210368424654007\n",
      "380 0.11597808450460434\n",
      "381 0.14594948291778564\n",
      "382 0.10521256178617477\n",
      "383 0.13324593007564545\n",
      "384 0.13972108066082\n",
      "385 0.11397769302129745\n",
      "386 0.13150878250598907\n",
      "387 0.12369583547115326\n",
      "388 0.13921332359313965\n",
      "389 0.11966625601053238\n",
      "390 0.1327643245458603\n",
      "391 0.11877375096082687\n",
      "392 0.12030892074108124\n",
      "393 0.14670054614543915\n",
      "394 0.12759244441986084\n",
      "395 0.13414441049098969\n",
      "396 0.1321486234664917\n",
      "397 0.14225730299949646\n",
      "398 0.1314702033996582\n",
      "399 0.12579244375228882\n",
      "400 0.14452126622200012\n",
      "401 0.14459091424942017\n",
      "402 0.11523719877004623\n",
      "403 0.13520583510398865\n",
      "404 0.1241500973701477\n",
      "405 0.1189572885632515\n",
      "406 0.15690700709819794\n",
      "407 0.12072639912366867\n",
      "408 0.1339089423418045\n",
      "409 0.14652688801288605\n",
      "410 0.12453656643629074\n",
      "411 0.14095796644687653\n",
      "412 0.15602856874465942\n",
      "413 0.15191088616847992\n",
      "414 0.13626441359519958\n",
      "415 0.12291918694972992\n",
      "416 0.1416551172733307\n",
      "417 0.1269952803850174\n",
      "418 0.11974897235631943\n",
      "419 0.12377277761697769\n",
      "420 0.11328073590993881\n",
      "421 0.1225225031375885\n",
      "422 0.11435458064079285\n",
      "423 0.13791930675506592\n",
      "424 0.10875829309225082\n",
      "425 0.15098366141319275\n",
      "426 0.1353648453950882\n",
      "427 0.13212640583515167\n",
      "428 0.1539859026670456\n",
      "429 0.12590225040912628\n",
      "430 0.13194040954113007\n",
      "431 0.1350848525762558\n",
      "432 0.14643794298171997\n",
      "433 0.14929942786693573\n",
      "434 0.13898177444934845\n",
      "435 0.11869195103645325\n",
      "436 0.12713925540447235\n",
      "437 0.14042094349861145\n",
      "438 0.12072651088237762\n",
      "439 0.12139508873224258\n",
      "440 0.15546515583992004\n",
      "441 0.1055414155125618\n",
      "442 0.1288791298866272\n",
      "443 0.1408403366804123\n",
      "444 0.13092127442359924\n",
      "445 0.11325781792402267\n",
      "446 0.10635679960250854\n",
      "447 0.13279767334461212\n",
      "448 0.15621550381183624\n",
      "449 0.13182197511196136\n",
      "450 0.14503289759159088\n",
      "451 0.10734320431947708\n",
      "452 0.12793076038360596\n",
      "453 0.11074171960353851\n",
      "454 0.11849149316549301\n",
      "455 0.15398186445236206\n",
      "456 0.13409270346164703\n",
      "457 0.12100504338741302\n",
      "458 0.14133648574352264\n",
      "459 0.12663578987121582\n",
      "460 0.14250825345516205\n",
      "461 0.1366550326347351\n",
      "462 0.12671113014221191\n",
      "463 0.12492310255765915\n",
      "464 0.10988476872444153\n",
      "465 0.12953419983386993\n",
      "466 0.10403246432542801\n",
      "467 0.1269044429063797\n",
      "468 0.12019670754671097\n",
      "469 0.1545877456665039\n",
      "470 0.10422768443822861\n",
      "471 0.11133889108896255\n",
      "472 0.15000666677951813\n",
      "473 0.11461149156093597\n",
      "474 0.1028701588511467\n",
      "475 0.09606198966503143\n",
      "476 0.1222025603055954\n",
      "477 0.11722355335950851\n",
      "478 0.14908885955810547\n",
      "479 0.13044622540473938\n",
      "480 0.12303060293197632\n",
      "481 0.1254912167787552\n",
      "482 0.18396726250648499\n",
      "483 0.14094866812229156\n",
      "484 0.12883959710597992\n",
      "485 0.11352134495973587\n",
      "486 0.12193070352077484\n",
      "487 0.14292126893997192\n",
      "488 0.11365865170955658\n",
      "489 0.11246123164892197\n",
      "490 0.14244404435157776\n",
      "491 0.12308336049318314\n",
      "492 0.12044594436883926\n",
      "493 0.11990789324045181\n",
      "494 0.13427738845348358\n",
      "495 0.1362227350473404\n",
      "496 0.12878583371639252\n",
      "497 0.12077149748802185\n",
      "498 0.13736377656459808\n",
      "499 0.13088516891002655\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "with writer.saving(fig, 'xor.mp4' ,100):\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    x = np.linspace(0, 1.0, 100)\n",
    "    y = np.linspace(0, 1.0, 100)\n",
    "    \n",
    "    network = DQN(3, [6,3], 1, F.leaky_relu)\n",
    "    optimizer = optim.Adam(network.parameters(), amsgrad=True)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for i in range(500):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        Xs, Ys = generate_both(25,0.1)\n",
    "                    \n",
    "        Xs = torch.tensor(Xs)\n",
    "        Ys = torch.tensor(Ys, dtype=torch.float)\n",
    "\n",
    "        prediction = network(Xs)\n",
    "        loss = criterion(prediction, Ys)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(i, loss.item())\n",
    "\n",
    "        OR_mat = np.zeros((100,100))\n",
    "        XOR_mat = np.zeros((100,100))\n",
    "\n",
    "        for idx_y, grid_point_y in enumerate(y):\n",
    "            for idx_x, grid_point_x in enumerate(x):\n",
    "                OR_mat[idx_y, idx_x] = network(torch.tensor([grid_point_x, grid_point_y, 0.0])).item()\n",
    "                XOR_mat[idx_y, idx_x] = network(torch.tensor([grid_point_x, grid_point_y, 1.0])).item()\n",
    "\n",
    "        plt.axis('off')\n",
    "        \n",
    "#         plt.subplot(1, 2, 1)\n",
    "#         plt.axvline(x=0.5, color='w', linestyle='dashed')\n",
    "#         plt.axhline(y=0.5, color='w', linestyle='dashed')\n",
    "#         plt.xticks(np.arange(min(x), max(x)+1, 0.5))\n",
    "#         plt.yticks(np.arange(min(y), max(y)+1, 0.5))\n",
    "#         plt.imshow(OR_mat, interpolation='none', cmap='inferno', extent=(0.0, 1.0, 0.0, 1.0))\n",
    "#         plt.subplot(1, 2, 2)\n",
    "#         plt.axvline(x=0.5, color='w', linestyle='dashed')\n",
    "#         plt.axhline(y=0.5, color='w', linestyle='dashed')\n",
    "#         plt.xticks(np.arange(min(x), max(x)+1, 0.5))\n",
    "#         plt.yticks(np.arange(min(y), max(y)+1, 0.5))\n",
    "        plt.imshow(XOR_mat, interpolation='none', cmap='inferno', extent=(0.0, 1.0, 0.0, 1.0))\n",
    "        \n",
    "        writer.grab_frame()\n",
    "    \n",
    "    for i in range(500):\n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        Xs, Ys = generate_both(25,0.9)\n",
    "                    \n",
    "        Xs = torch.tensor(Xs)\n",
    "        Ys = torch.tensor(Ys, dtype=torch.float)\n",
    "\n",
    "        prediction = network(Xs)\n",
    "        loss = criterion(prediction, Ys)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(i, loss.item())\n",
    "\n",
    "        OR_mat = np.zeros((100,100))\n",
    "        XOR_mat = np.zeros((100,100))\n",
    "\n",
    "        for idx_y, grid_point_y in enumerate(y):\n",
    "            for idx_x, grid_point_x in enumerate(x):\n",
    "                OR_mat[idx_y, idx_x] = network(torch.tensor([grid_point_x, grid_point_y, 0.0])).item()\n",
    "                XOR_mat[idx_y, idx_x] = network(torch.tensor([grid_point_x, grid_point_y, 1.0])).item()\n",
    "\n",
    "        plt.axis('off')\n",
    "#         plt.subplot(1, 2, 1)\n",
    "#         plt.axvline(x=0.5, color='w', linestyle='dashed')\n",
    "#         plt.axhline(y=0.5, color='w', linestyle='dashed')\n",
    "#         plt.xticks(np.arange(min(x), max(x)+1, 0.5))\n",
    "#         plt.yticks(np.arange(min(y), max(y)+1, 0.5))\n",
    "#         plt.imshow(OR_mat, interpolation='none', cmap='inferno', extent=(0.0, 1.0, 0.0, 1.0))\n",
    "#         plt.subplot(1, 2, 2)\n",
    "#         plt.axvline(x=0.5, color='w', linestyle='dashed')\n",
    "#         plt.axhline(y=0.5, color='w', linestyle='dashed')\n",
    "#         plt.xticks(np.arange(min(x), max(x)+1, 0.5))\n",
    "#         plt.yticks(np.arange(min(y), max(y)+1, 0.5))\n",
    "        plt.imshow(XOR_mat, interpolation='none', cmap='inferno', extent=(0.0, 1.0, 0.0, 1.0))\n",
    "        \n",
    "        writer.grab_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "104"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
