{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import rubiks\n",
    "import rubiks2\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os \n",
    "import copy\n",
    "import time\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from replay_memories import ReplayMemory, PrioritizedReplayMemory\n",
    "from networks import DQN, DuelingDQN, DuelingDQNHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n",
      "Torch Version:  1.0.1.post2\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \", device)\n",
    "print(\"Torch Version: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon decay\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 1000\n",
    "\n",
    "epsilon_by_exponential_step = lambda step_idx: epsilon_final + (epsilon_start - epsilon_final) * np.exp(-1. * step_idx / epsilon_decay)\n",
    "\n",
    "epsilon_by_linear_step = lambda step_idx: epsilon_final + (epsilon_start-epsilon_final)*((epsilon_decay-step_idx)/epsilon_decay) if step_idx < epsilon_decay else epsilon_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5a8d6fa320>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl8XHW9//HXZyZbmyZpkkkX0r1NgLJIoUIRuCyisgm4IHDFXbnucvWHwkNFr/eq133lXoXrAqisYq0CIgLKIhRSoEDplu5L2ibpvmf5/P44J2EakmaSzMlMMu/n4zHM2eacz8kp85nz/X7P92vujoiICEAs0wGIiEj2UFIQEZFOSgoiItJJSUFERDopKYiISCclBRER6aSkIDnNzN5tZn9NmnczmzEIx/21mf1X1McR6SslBRkyzGy1me0zs91Jr58OZJ/u/lt3f3O6YhQZ6vIyHYBIH73V3f+W6SBEhivdKciwYGbvN7MnzeynZrbDzJaY2Ru7rF9pZrvMbJWZvTtp+RM97LPMzG41s0YzW2NmXzKzWPLnzOy7ZrYt3Of5h4lvlpk9Fx7/TqCoy/qLzOwFM9tuZv80s+OT1k00s3vDOJo77o7MbLqZPRIuazKz35rZ6HDdtWb2+y7H+LGZ/ajPf1zJKUoKMpycAqwAEsBXgHvNrMLMioEfA+e7ewnwBuCFFPb3E6AMmAacCbwX+ECX4y0Nj/dt4BdmZl13YmYFwFzgNqACuBt4R9L6WcAvgX8DKoGfA/PMrNDM4sCfgTXAFKAauKPjo8A3gSOAo4GJwFfDdb8BzktKEnnAFcCtKZy35DAlBRlq5oa/pjteH0latwX4obu3uPudBF/YF4br2oFjzWyEuze4+6LDHST8Mr4CuN7dd7n7auB7wHuSNlvj7je7extwCzAeGNvN7uYA+Umx3QM8m7T+auDn7j7f3dvc/RbgQPi5kwm+9K919z3uvt/dnwBw93p3f8jdD7h7I/B9guSFuzcAjwGXhcc4D2hy9wWHO28RJQUZai5199FJr5uT1m3wQ3t4XAMc4e57gMuBjwINZnafmR3Vy3ESBF/ka7rsrzppflPHhLvvDSdHdbOvI3qIrcNk4HPJyY7gV/8R4fsad2/tulMzG2tmd5jZBjPbSXB3kEja5BbgqnD6KoI7FZHDUlKQ4aS6S/HNJGAjgLs/6O5vIvg1vwS4uZvPJ2sCWgi+sJP3t6EfcTX0EFuHdcDXuyS7ke5+e7huUlj809U3AAeOc/dSgi/+5GPMBY43s2OBi4Df9iN2yTFKCjKcjAE+bWb5ZnYZQTn7/eEv6kvCuoUDwG6C4qQehUVCdwFfN7MSM5sMfJbg13hfPQW0JsX2doJioQ43Ax81s1MsUGxmF5pZCfAMQVL573B5kZmdFn6uJDyXHWZWDVzb5Rz2A/cAvwOecfe1/YhdcoySggw1f+rynMIfktbNB2oIfuV/HXinuzcT/Dv/LMFdw1aCcvePpXCsTwF7gJXAEwRfrr/sa8DufhB4O/D+8PiXA/cmra8DPgL8FNgG1IfbdiSntwIzgLXA+vDzAP8BnAjsAO5L3meSW4DjUNGRpMg0yI4MB2b2fuDD7n56pmPJJmY2iaC4bJy778x0PJL9dKcgMkyFz1R8FrhDCUFSpSeaRYahsP5kM0Erp/MyHI4MISo+EhGRTio+EhGRTkOu+CiRSPiUKVMyHYaIyJCyYMGCJnev6m27IZcUpkyZQl1dXabDEBEZUsxsTe9bqfhIRESSKCmIiEgnJQUREemkpCAiIp2UFEREpFNkScHMfmlmW8zs5R7WWzg8YL2ZvWhmJ0YVi4iIpCbKO4Vfc/jH688n6NGyhmDkqf+NMBYREUlBZEnB3R8j6Ca4J5cAt3rgaWC0mY2PKp4l8//KUz//FN5+2G70RURyWibrFKoJRpXqsJ5DhzrsZGZXm1mdmdU1Njb262DbVz7LqQ230rRlfb8+LyKSC4ZERbO73+Tus919dlVVr09pd6u4+mgANq/stopDRETIbFLYQDAoeYcJ9G/825SMnXocALvXvxLVIUREhrxMJoV5wHvDVkhzgB3u3hDVwaqqp7HPC2hvXB7VIUREhrzIOsQzs9uBs4CEma0HvgLkA7j7z4D7gQsIxqPdC3wgqlgALBanIW8CI3atjPIwIiJDWmRJwd2v7GW9A5+I6vjd2VE8hcROFR+JiPRkSFQ0p0tb+XSO8M3s3L0r06GIiGSlnEoKheOPJm7O+hWLMh2KiEhWyqmkUDlpJgDb1qoISUSkOzmVFMZOOxaA1s1LMhyJiEh2yqmkEC8qYYslyN+2ItOhiIhkpZxKCgDNIyZTvi+loUpFRHJOziWF/WXTqW5bz/6DrZkORUQk6+RcUsgfU0up7WPt2lWZDkVEJOvkXFIonRi0QGpcrWapIiJd5VxSGDs1aIG0b+PiDEciIpJ9ci4pFJZPZB9F2FZ1jCci0lXOJQViMRoLJlCyS3UKIiJd5V5SAPaWTmN86zr2t7RlOhQRkaySk0nBqmqppokVG5syHYqISFbJyaRQNmEmMXM2amhOEZFD5GRSSEwJWiDt2qAWSCIiyXIyKeRV1QDgjcsyHImISHbJyaRAwUi25o9jpFogiYgcIjeTArBn1BSqW9exfe/BTIciIpI1cjYpWFUt06yBpQ07Mx2KiEjWyNmkUDJhJqNsP2vXamwFEZEOOZsUSiccDcCOdWqBJCLSIWeTgiVqAWhXCyQRkU45mxQoGc+B2AiKd67A3TMdjYhIVsjdpGDG7lFTmdi+gQ3b92U6GhGRrJC7SQEgUcO0WANLN+3KdCQiIlkhp5PCqOqjmWBNLFu3JdOhiIhkhZxOCoXjjgKgee0rGY5ERCQ75HRSoDLoA6lly9IMByIikh1yPClMxzFG71nNjn0tmY5GRCTjcjsp5I9gf3E102INvLJR3V2IiESaFMzsPDNbamb1ZnZdN+snmdmjZva8mb1oZhdEGU934lW1TLeNLNq4Y7APLSKSdSJLCmYWB24EzgdmAlea2cwum30JuMvdZwFXAP8TVTw9KRh3FNNjDSzesH2wDy0iknWivFM4Gah395XufhC4A7ikyzYOlIbTZcDGCOPpXuUMRnCAzRs0toKISJRJoRpYlzS/PlyW7KvAVWa2Hrgf+FR3OzKzq82szszqGhsb0xtl2AeSba1nf0tbevctIjLEZLqi+Urg1+4+AbgAuM3MXhOTu9/k7rPdfXZVVVV6I0gEzVKnskFPNotIzosyKWwAJibNTwiXJfsQcBeAuz8FFAGJCGN6rVFjaS8oCSub1QJJRHJblEnhWaDGzKaaWQFBRfK8LtusBd4IYGZHEySFNJcP9cIMS9RQm7dJLZBEJOdFlhTcvRX4JPAgsJigldEiM/uamV0cbvY54CNmthC4HXi/Z6Afa0vUUhNr0J2CiOS8vCh37u73E1QgJy+7IWn6FeC0KGNISWIGifY7WLtpC61t7eTFM13VIiKSGfr2g84WSONbN7B8y+4MByMikjlKCtDZMd50a+DF9XqITURyl5ICQMU03GLMLNjEC+tU2SwiuUtJASC/CBs9mRNGNOpOQURympJCh0QN02wjSzbt0pPNIpKzlBQ6JGqpPLCO9vY2NU0VkZylpNChcgbxtv0cQTML16kISURyk5JCh7BZ6uxRTapXEJGcpaTQIewY79SyrSxcrxZIIpKblBQ6FFdBURnHFG5mVdMejdksIjlJSaGDGVTWMLEt6Mj1Jd0tiEgOUlJIlqildE8wAttC1SuISA5SUkiWmEFs9yaOqTSeX6ukICK5R0khWdgC6dwxu3h+7TYy0Iu3iEhGKSkkCzvGO6W0meY9B1ndvDfDAYmIDC4lhWQVU8HiHJm3CYAFa7ZlOCARkcGlpJAsrxDKp1Cxbw2lRXksWLM10xGJiAwqJYWuEjVY03JOnFyuOwURyTlKCl0laqC5ntdPKmXZ5t3s2KuH2EQkdygpdFVZA20HmFO5D4Dn1uluQURyh5JCV2Gz1GMKNxOPGc+pCElEcoiSQldhx3hF21cyc3wpdauVFEQkdygpdDWyEkaUQ9MyTppczgvrttPa1p7pqEREBoWSQldhx3g013PS5HL2tbSxuGFXpqMSERkUSgrdSdRC0zJmTykH4JnVel5BRHKDkkJ3EjNg92bGFx5kUsVI5q9sznREIiKDQkmhO2ELJJrqmTOtgvmrttLers7xRGT4U1LoTtgxHs3LmTOtkh37WliySfUKIjL8KSl0p2IqxPKgaRmnTKsEYP4qFSGJyPCnpNCdeD6UT4WmZVSPHsHEihE8rXoFEckBSgo9SdRAUz0Ac6ZWql5BRHJCpEnBzM4zs6VmVm9m1/WwzbvM7BUzW2Rmv4synj5J1MDWFdDexpxplWzf28LSzapXEJHhLbKkYGZx4EbgfGAmcKWZzeyyTQ1wPXCaux8DXBNVPH1WWQNtB2H7Gk6ZVgGgpqkiMuxFeadwMlDv7ivd/SBwB3BJl20+Atzo7tsA3H1LhPH0TWez1OVMKB/JhPIRPL1SD7GJyPAWZVKoBtYlza8PlyWrBWrN7Ekze9rMzutuR2Z2tZnVmVldY2NjROF2EXaMR9NyAOZMq2T+qmbVK4jIsJbpiuY8oAY4C7gSuNnMRnfdyN1vcvfZ7j67qqpqcCIbWRF0jte0DIBTp1WybW8LrzTsHJzji4hkQJRJYQMwMWl+Qrgs2Xpgnru3uPsqYBlBksgOYcd4AGfUJAB4fHlTJiMSEYlUlEnhWaDGzKaaWQFwBTCvyzZzCe4SMLMEQXHSyghj6ptETeedwpjSIo4cW8IT9YNUfCUikgGRJQV3bwU+CTwILAbucvdFZvY1M7s43OxBoNnMXgEeBa519+xp4pOogT2NsC8YaOeMmgTPrtrGvoNtGQ5MRCQaKSUFM3u7mS03sx1mttPMdplZr4Xr7n6/u9e6+3R3/3q47AZ3nxdOu7t/1t1nuvtx7n7HwE4nzZI6xgM4o7aKg23t6kpbRIatVO8Uvg1c7O5l7l7q7iXuXhplYFmho2O8sAjp5CkVFMRjPL5MRUgiMjylmhQ2u/viSCPJRuWTIZYPzUGz1BEFcV4/tVyVzSIybKWaFOrM7E4zuzIsSnq7mb090siyQTwfKqZ1PqsAcEZNFUs372LLzv0ZDExEJBqpJoVSYC/wZuCt4euiqILKKomaQ5LC6TPUNFVEhq+8VDZy9w9EHUjWStTAsgehrRXiecwcX0plcQGPLW/kHSdNyHR0IiJplWrrowlm9gcz2xK+fm9mufGNWFkD7S2wfQ0AsZhxZm0V/1jWSJu6vBCRYSbV4qNfETx4dkT4+lO4bPjrbJa6rHPROUePYfveFp5fuy1DQYmIRCPVpFDl7r9y99bw9WtgkDohyrDEjOC9S2VzXsx4eEn2dOoqIpIOqSaFZjO7yszi4esqIHuePI7SiHIorjrkTqFsRD6vn1LBI4uVFERkeEk1KXwQeBewCWgA3gnkTuVzUsd4Hd549BiWbt7Fuq17MxSUiEj6pZQU3H2Nu1/s7lXuPsbdL3X3tVEHlzWSOsbrcM5RYwB4dKnuFkRk+Dhsk1Qz+wnQYxMbd/902iPKRoka2NsMe7cG4ywA06pGMTVRzMOLt/DeU6dkNj4RkTTp7TmFukGJItslDc3JpFM6F59z1Bhue2oNew60UlyY0iMfIiJZ7bDfZO5+y2AFktUqO1ogLTskKbzxqDH84olVPFHfxFuOGZeh4ERE0qe34qMfuvs1ZvYnuilGcveLu/nY8DN6MsQLOjvG6/D6qRWUFuXx4KJNSgoiMiz0VuZxW/j+3agDyWrxPKiYfsizCgD58RjnzhzL317ZzMHWdgryMj3ktYjIwBz2W8zdF4Tv/+h4AS8C28Lp3JGY8ZqkAHD+sePZub+Vp1bmxmMbIjK8pdr30d/NrNTMKoDngJvN7PvRhpZlErWwbRW0tRyy+IyaBMUFcf7yckOGAhMRSZ9UyzvK3H0n8HbgVnc/BTg3urCyUGUNtLfCttWHLC7Kj3P2UWN4cNFmWtvaMxObiEiapJoU8sxsPMFTzX+OMJ7s1U3HeB3OP3Y8W/cc1NjNIjLkpZoUvgY8CKxw92fNbBrw2gL24aybjvE6nHVkFUX5Mf7y8qZBDkpEJL1S7ebibnc/3t0/Fs6vdPd3RBtalikqg1Fju00KxYV5nFlbxV9e3kS7xlgQkSEs1YrmaWb2JzNrDAfZ+WN4t5BbKmte86xChwuOG8+WXQd4VkVIIjKEpVp89DvgLmA8wSA7dwO3RxVU1krUQONS8NfeDZx79FhG5MeZ+8LGDAQmIpIeqSaFke5+W9IgO78BiqIMLCslamD/9qBzvC6KC/N48zFjuf+lBg62qhWSiAxNqSaFB8zsOjObYmaTzezzwP1mVhE+u5AbkjvG68alJ1SzY18L/1jWOIhBiYikT6pde74rfP+3LsuvIOgTKTfqF5I7xpt86mtWn16ToKK4gLkvbOBNM8cOcnAiIgOXUlJw96lRBzIkjJ4E8cIeK5vz4zEuPG48d9WtY9f+FkqK8gc5QBGRgTls8VFYTNQxfVmXdd+IKqisFYsHdws9FB8BXDrrCA60tvPgos2DGJiISHr0VqdwRdL09V3WnZfmWIaGHjrG63DipHImVoxg7vMbBjEoEZH06C0pWA/T3c3nhkRt0P9R68FuV5sZb5s1gSdXNLF+297BjU1EZIB6Swrew3R3869hZueZ2VIzqzez6w6z3TvMzM1sdm/7zLjKGvC2oMfUHlx20gQA7lmwfrCiEhFJi96SwuvMbKeZ7QKOD6c75o873AfNLA7cCJwPzASuNLOZ3WxXAnwGmN+vMxhsiZrgvZuO8TpMrBjJ6TMS3F23njZ1eyEiQ0hvg+zE3b3U3UvcPS+c7pjvrWnNyUB92E/SQeAO4JJutvtP4FvA/n6dwWCr7LljvGTvmj2RDdv38WR90yAEJSKSHlGOH1kNrEuaXx8u62RmJwIT3f2+w+3IzK42szozq2tszPCDYUWlUDK+16Tw5mPGMnpkPnfWrTvsdiIi2SRjgwqbWQz4PvC53rZ195vcfba7z66qqoo+uN5Uzjhs8RFAYV6ct82q5qFFm9m2p/tKaRGRbBNlUtgATEyanxAu61ACHAv83cxWA3OAeUOisjlRGzzA1k3HeMkuf/1EDra1c6+ap4rIEBFlUngWqDGzqWZWQPDMw7yOle6+w90T7j7F3acATwMXu3tdhDGlR6IG9u+APYcvyjpqXCmzJo3mN0+v0TgLIjIkRJYU3L0V+CTBiG2LgbvcfZGZfc3MLo7quIOiswVS74PPvf8NU1jVtIfHlquTPBHJfpHWKbj7/e5e6+7T3f3r4bIb3H1eN9ueNSTuEiB4VgF6rVeAYPzmxKhCbn1qTcRBiYgMXMYqmoe0somQVwTN9b1uWpAX419PmcSjS7ewpnnPIAQnItJ/Sgr9EYsFdwsp3CkAvPuUScTNuE13CyKS5ZQU+quXjvGSjS0t4rxjx3FX3Tr2HmyNODARkf5TUuivRC1sXwOtB1La/AOnTWHn/lburlN/SCKSvZQU+quyBrwdtq5MafMTJ5Vz4qTR3Pz4SlrbNIaziGQnJYX+SqFjvGRmxkfPnM76bfu476WGCAMTEek/JYX+Sh6vOUXnHj2WGWNG8bN/rMR7eRpaRCQTlBT6q3AUlFZDU+/NUjvEYsbV/zKNxQ07eWy5ek8VkeyjpDAQKXSM19WlJ1QzrrSIn/19RURBiYj0n5LCQCRqgwfY+lAUVJAX48NnTOWplc3Urd4aYXAiIn2npDAQiRo4sBN2b+7Tx959ymQSowr5wd/6dpchIhI1JYWB6EPHeMlGFMT52FnTebK+madXNkcQmIhI/ygpDEQfOsbr6t2nTGJMSSHff2iZWiKJSNZQUhiI0mrIH5lSx3hdFeXH+cTZM3hm1Vb+uUJ3CyKSHZQUBiIW61cLpA5XnDyR8WVFfOfBpbpbEJGsoKQwUImaPtcpdCjMi3PNuTW8sG4797+0Kc2BiYj0nZLCQCVqYftaaNnXr4+/86SJHDWuhP/+y2IOtLalOTgRkb5RUhioyhmAp9wxXlfxmPHFC49m3dZ93PpPjbcgIpmlpDBQidrgvZ/1CgBn1FRxZm0VP3lkOdv2HExTYCIifaekMFCV04P3ftYrdPjihUez+0ArP9QDbSKSQUoKA1VQHIzZPMCkUDu2hKvmTOa2p9fw8oYdaQpORKRvlBTSYQDNUpN97s1HUlFcwBfnvkxbu5qoisjgU1JIh350jNedshH5fPHCo1m4bjt3PLs2TcGJiKROSSEdEjVwcDfsGviIapeeUM2caRV864ElNO1ObfxnEZF0UVJIh352jNcdM+O/Lj2WfS1tfGXeogHvT0SkL5QU0mEAHeN1Z8aYEj59Tg33vdjA/RrPWUQGkZJCOpQeAfnF/eoYrycfPWs6x1WX8aW5L6sYSUQGjZJCOpiFfSCl7xmD/HiM7172Onbvb+XLc19Wh3kiMiiUFNIlUQNN6btTADhyXAnXvKmGB17exNwXNqR13yIi3VFSSJdELexYCwf3pnW3V58xjddPKedLf3iZlY2707pvEZGulBTSpXJG8J7GegWAvHiMH185i/y8GJ/83fPsb1FPqiISnUiTgpmdZ2ZLzazezK7rZv1nzewVM3vRzB42s8lRxhOpjo7xmgfeLLWr8WUj+O47X8crDTv55v2L075/EZEOkSUFM4sDNwLnAzOBK81sZpfNngdmu/vxwD3At6OKJ3KV0wFLy7MK3Tl35lg+dPpUbnlqDX9auDGSY4iIRHmncDJQ7+4r3f0gcAdwSfIG7v6ou3cUwj8NTIgwnmjlj4DRA+8Y73C+cN5RzJ5czrX3LFSneSISiSiTQjWwLml+fbisJx8CHuhuhZldbWZ1ZlbX2NiYxhDTrDK9zVK7KsiL8b9XnUT5yAL+7bYFen5BRNIuKyqazewqYDbwne7Wu/tN7j7b3WdXVVUNbnB90dExXnt7ZIeoKinkpvfMpmn3AT7+m+c42BrdsUQk90SZFDYAE5PmJ4TLDmFm5wJfBC5296H90zcxA1r2wq5oy/yPm1DGt995PM+s3srn71lIu7rZFpE0iTIpPAvUmNlUMysArgDmJW9gZrOAnxMkhC0RxjI4OofmjK5eocMlJ1Rz7VuOZO4LG/nmA2qRJCLpEVlScPdW4JPAg8Bi4C53X2RmXzOzi8PNvgOMAu42sxfMbF4PuxsaKtPXW2oqPn7WdN536mRufnwVNz+2clCOKSLDW16UO3f3+4H7uyy7IWn63CiPP+hKxkFBSSTPKnTHzLjhrcfQuPsAX79/MWUj8nnX6yf2/kERkR5EmhRyTgQd4/UmHjO+/64T2H1gAV+490XM4LLZSgwi0j9Z0fpoWEnUDFrxUYei/Dg3veckTp+R4PO/f5F7Fqwf1OOLyPChpJBuiRrYuQEODG7ndUX5cW5+72xOn5Hg2nsWcvszGuNZRPpOSSHdOiqb09wxXio6EsOZtVVcf+9L/OTh5RqHQUT6REkh3To7xhv8pACvJoa3zarmew8t4yvzFtGm5xhEJEWqaE63imkEHeMNXmVzV/nxGN+77HUkRhVw8+OraNixnx9cfgKjCnW5ReTwdKeQbvlFUD550Cubu4rFjC9eOJMbLprJw4s3847/+Sdrm9M7AJCIDD9KClGoHPwWSD354OlTueWDJ7Np534uvvEJnqxvynRIIpLFlBSiMAgd4/XFGTVV/PETp1E1qpD3/GI+P3hoGa1t2RGbiGQXJYUoJGZA6z7YmT3PC0xJFDP3E6fxtlkT+NHDy/nXm+ezcfu+TIclIllGSSEKg9gxXl8UF+bxvXe9jh9c/joWbdzB+T96nLnPb1CzVRHppKQQhUHuGK+v3jZrAvd9+gymVRVzzZ0v8KFb6mjYobsGEVFSiMaoMVBYltFmqb2Zkijmno++gS9fNJN/rmjiTd9/jNueWq1nGkRynJJCFDo6xhuk3lL7Kx4zPnT6VP56zZkcP6GML/9xERf++HGeWtGc6dBEJEOUFKKSgY7x+mtS5Uh+++FTuPFfT2TX/lauvPlpPv7bBaxp3pPp0ERkkCkpRCVRA7sa4MCuTEeSEjPjwuPH8/DnzuSzb6rlkSVbOOd7/+C637/I+m166E0kVygpRCXLK5t7UpQf59NvrOGxa8/mPXMmc+9zGzj7u3/ny3NfZt1WJQeR4U5JISoZ7hhvoMaUFvHVi4/h79eexWWzJ3L7M2s58zuP8snfPcfCddszHZ6IREQ9pEWlYipYLKtbIKXiiNEj+MbbjuNT58zg10+u5nfz1/LnFxs4eUoFV506mbccM5bCvHimwxSRNFFSiEpeIZRPGXLFRz0ZXzaC6y84mk+9sYY7n13Hr55cxadvf57RI/N5+6wJXHHyRGrHlmQ6TBEZICWFKGVRx3jpMqowjw+dPpUPvGEKT65o4o5n1nHb06v55ZOrOK66jIuOH8+Fx49nQvnITIcqIv2gpBClRA2s+ge0t0FseBWxxGLGGTVVnFFTRfPuA/zh+Q3MW7iRbz6whG8+sIRZk0Zz0fFHcO7RY5hcWZzpcEUkRTbU+r2ZPXu219XVZTqM1Cz4NfzpM/CZhUFRUg5Y3bSH+15q4E8LN7JkU9Acd1qimLOPGsPZR47h5KkVFOSpfYPIYDOzBe4+u7ftdKcQpeSO8XIkKUxJFPOJs2fwibNnsKZ5D48s2cKjSxu57ek1/OKJVYzIj3PS5HLmTKvglGmVHD+hTBXVIllESSFKyc8q1Lwps7FkwOTKYj5w2lQ+cNpU9h5s5Z/1zTy+vJH5q7by3b8GrbIK82KcOKmcEyaN5vjqMo6fOJojyoowswxHL5KblBSiVJyAotFDvllqOowsyOPcmWM5d+ZYALbtOcgzq7fy9Mpmnl29lZsfW0lr2BlfYlQBx1WXcVx1GbXjSjhybAlTEsXkx1XsJBI1JYUomb06Cpscory4gLccM463HDMOgP0tbSxu2MlLG3awcN0OXly/nX8sa6Sj09b8uDE1UUzt2BJqx5YwuXIkkyuLmVwxktEj83VnIZImSgpRS9RA/d8yHUXWK8qPM2vQwd8sAAALbUlEQVRSObMmlcOpwbL9LW3Ub9nN8i27WLppN8s372Lh+u38+cWGQz5bUpTHpIqRTK4cyaSKYsaXFTG2tIjxZUWMKysiMaqQeExJQyQVSgpRS9TAC7+F/TugqCzT0QwpRflxjq0u49jqQ/9u+w62sXbrXtY072Ht1r3h9F4WN+zioVc209J2aIu6eMwYU1LI2NIixpUWUTGqgIqRBVQUF1A5KnhPfqniW3KZkkLUOiub62HCSZmNZZgYURDnyHElHDnutU9Qt7c7zXsOsmnHfjbtDF879rFpxwE279zPisbd1K05yNY9B+lpPKHigjglRfmUFOWFr3xKR7w6X5q0bmRBHiPy44wsiFMUvo8oiDMyP4+ighgF8ZiKtmRIUVKIWmfHeMuVFAZBLGZUlRRSVVLIcfR8Z9be7uzY10LzniBBbN1zgK17Wti65wDb9rawa38Lu/a3snN/C9v2HmTt1r3s2t/Czv2tHGxtTzmeeMwYkf9qwijKj5Efj1GQF7wXhu8FScsK8mIUxO3Q+bxgm3jMyIsZ8ViMeAzisVg4/+orL2bEOrcz4mbkxa1z21jnfLDODGLhu5kR65jn1fnk947tYwbGoZ+PJW0nQ1OkScHMzgN+BMSB/3P3/+6yvhC4FTgJaAYud/fVUcY06MqngMXhr1+CJ36Q6WgkFAPKw1evDBgRvoB2d9o9SCztOO7Bsp7e2x3cnfZW8BbHAXfw8LPBfHDb0vGZjmn3YPsotIWvqBkEjS6S5w+7cY+z/Tt2jwt73vuAjxtRTmw66RpmX/jhaHYeiiwpmFkcuBF4E7AeeNbM5rn7K0mbfQjY5u4zzOwK4FvA5VHFlBF5BfDGL8PG5zMdiaRJjMHtc75rcnlNUgmnX7O+l+mOxNV5nPBgnnTcbpeH/3l13g+d79xnmBi7fC78RNLC10z2/LfoeSK1zx9mo9esOiS23vfe3TmlW2FJRXQ7D0V5p3AyUO/uKwHM7A7gEiA5KVwCfDWcvgf4qZmZD7W+N3pz+r9nOgIZwozgVlvV3zIYovzBUw2sS5pfHy7rdht3bwV2AJVdd2RmV5tZnZnVNTY2RhSuiIgMiUdE3f0md5/t7rOrqqoyHY6IyLAVZVLYAExMmp8QLut2GzPLA8oIKpxFRCQDokwKzwI1ZjbVzAqAK4B5XbaZB7wvnH4n8Miwq08QERlCIqtodvdWM/sk8CBBHdkv3X2RmX0NqHP3ecAvgNvMrB7YSpA4REQkQyJ9TsHd7wfu77LshqTp/cBlUcYgIiKpGxIVzSIiMjiUFEREpNOQG6PZzBqBNf38eAJoSmM4Q4HOOTfonHPDQM55srv32qZ/yCWFgTCzulQGrh5OdM65QeecGwbjnFV8JCIinZQURESkU64lhZsyHUAG6Jxzg845N0R+zjlVpyAiIoeXa3cKIiJyGEoKIiLSKWeSgpmdZ2ZLzazezK7LdDz9ZWYTzexRM3vFzBaZ2WfC5RVm9pCZLQ/fy8PlZmY/Ds/7RTM7MWlf7wu3X25m7+vpmNnCzOJm9ryZ/Tmcn2pm88NzuzPseBEzKwzn68P1U5L2cX24fKmZvSUzZ5IaMxttZveY2RIzW2xmpw7362xm/x7+u37ZzG43s6Lhdp3N7JdmtsXMXk5alrbramYnmdlL4Wd+bNbHwUHdfdi/CDrkWwFMAwqAhcDMTMfVz3MZD5wYTpcAy4CZwLeB68Ll1wHfCqcvAB4gGMBrDjA/XF4BrAzfy8Pp8kyfXy/n/lngd8Cfw/m7gCvC6Z8BHwunPw78LJy+ArgznJ4ZXvtCYGr4byKe6fM6zPneAnw4nC4ARg/n60ww6NYqYETS9X3/cLvOwL8AJwIvJy1L23UFngm3tfCz5/cpvkz/gQbpIpwKPJg0fz1wfabjStO5/ZFgHOylwPhw2XhgaTj9c+DKpO2XhuuvBH6etPyQ7bLtRTAex8PAOcCfw3/wTUBe12tM0DPvqeF0Xriddb3uydtl24tgbJFVhI1Bul6/4XideXUkxorwuv0ZeMtwvM7AlC5JIS3XNVy3JGn5Idul8sqV4qNUhgYdcsLb5VnAfGCsuzeEqzYBY8Ppns59qP1Nfgh8nlfHga8EtnswjCscGn9Pw7wOpXOeCjQCvwqLzP7PzIoZxtfZ3TcA3wXWAg0E120Bw/s6d0jXda0Op7suT1muJIVhx8xGAb8HrnH3ncnrPPiJMGzaGpvZRcAWd1+Q6VgGUR5BEcP/uvssYA9BsUKnYXidy4FLCBLiEUAxcF5Gg8qATF/XXEkKqQwNOmSYWT5BQvitu98bLt5sZuPD9eOBLeHyns59KP1NTgMuNrPVwB0ERUg/AkZbMIwrHBp/T8O8DqVzXg+sd/f54fw9BEliOF/nc4FV7t7o7i3AvQTXfjhf5w7puq4bwumuy1OWK0khlaFBh4SwJcEvgMXu/v2kVclDm76PoK6hY/l7w1YMc4Ad4W3qg8Cbzaw8/IX25nBZ1nH36919grtPIbh2j7j7u4FHCYZxhdeec3fDvM4DrghbrUwFaggq5bKOu28C1pnZkeGiNwKvMIyvM0Gx0RwzGxn+O+8452F7nZOk5bqG63aa2Zzwb/jepH2lJtMVLoNYsXMBQUudFcAXMx3PAM7jdIJbyxeBF8LXBQRlqQ8Dy4G/ARXh9gbcGJ73S8DspH19EKgPXx/I9LmleP5n8Wrro2kE/7PXA3cDheHyonC+Plw/LenzXwz/FkvpY6uMDJzrCUBdeK3nErQyGdbXGfgPYAnwMnAbQQuiYXWdgdsJ6kxaCO4IP5TO6wrMDv9+K4Cf0qWxQm8vdXMhIiKdcqX4SEREUqCkICIinZQURESkk5KCiIh0UlIQEZFOSgoyLJnZWDP7nZmtNLMFZvaUmb0tXHeWhT2tHubzXzWz/9fHY+7uw7bXmNnIvuxfZDAoKciwEz60Mxd4zN2nuftJBA+9TTj8JwfVNYCSgmQdJQUZjs4BDrr7zzoWuPsad/9J1w3Dfuznhn3VP21mxyetfl14h7HczD4Sbj/KzB42s+fCPusvOVwgZlZsZveZ2UILxgi43Mw+TdC3z6Nm9mi43ZvDYz1nZneHfVthZqvN7NvhsZ4xsxnh8svC/S00s8cG+gcT6ZDX+yYiQ84xwHMpbvsfwPPufqmZnQPcSvAkMcDxBP3SFwPPm9l9BH3SvM3dd5pZAnjazOZ5z0+BngdsdPcLAcyszN13mNlngbPdvSncz5eAc919j5l9gWDsiK+F+9jh7seZ2XsJeou9CLgBeIu7bzCz0an+YUR6ozsFGfbM7MbwF/Wz3aw+naA7Bdz9EaDSzErDdX90933u3kTQ/87JBN0OfMPMXiTojqCaV7s57s5LwJvM7Ftmdoa77+hmmzkEA8M8aWYvEPR9Mzlp/e1J76eG008Cvw7vYOKHO3+RvtCdggxHi4B3dMy4+yfCX+N1fdxP11//DrwbqAJOcveWsOfWoh534L7MgiEULwD+y8wedvevddnMgIfc/coU4vBwvx81s1OAC4EFZnaSuzenemIiPdGdggxHjwBFZvaxpGU9Veo+TvBFj5mdBTT5q+NTXGLBGMGVBB3xPUvQPfOWMCGczaG/6F/DzI4A9rr7b4DvEHR/DbCLYDhVgKeB05LqC4rNrDZpN5cnvT8VbjPd3ee7+w0Eg/Ekd6Ms0m+6U5Bhx93dzC4FfmBmnyf40twDfKGbzb8K/DIsDtrLq90XQ9A76aNAAvhPd99oZr8F/mRmLxHceSzpJZzjgO+YWTtBr5gdieom4C9mttHdzzaz9wO3m1lhuP5LBL36ApSH8R0gGF6RcJ81BHcZDxOMSSwyYOolVSSLhcVTs8N6DZHIqfhIREQ66U5BREQ66U5BREQ6KSmIiEgnJQUREemkpCAiIp2UFEREpNP/B+Cnh71T/OyXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure\n",
    "plt.title('Epsilon decay')\n",
    "plt.xlabel('Global steps')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.plot([epsilon_by_exponential_step(i) for i in range(10000)])\n",
    "plt.plot([epsilon_by_linear_step(i) for i in range(10000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the q-values of an action in a state\n",
    "def compute_q_val(model, state, action):\n",
    "    qactions = model(state)\n",
    "    return torch.gather(qactions,1,action.view(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the target. When done, 0 is added to the reward as there is no next state.\n",
    "def compute_target_dqn(model, reward, next_state, done, gamma):\n",
    "    return reward + gamma * model(next_state).max(1)[0] * (1-done)\n",
    "\n",
    "# Computes the target. When done, 0 is added to the reward as there is no next state. But now for Double DQN\n",
    "def compute_target_ddqn(model, target_model, reward, next_state, done, gamma):\n",
    "    a = model(next_state)\n",
    "    return reward.view(-1,1) + gamma * torch.gather(target_model(next_state),1,model(next_state).max(1)[1].view(-1,1)) * (1-done).view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(online_network, target_network, memory, optimizer, batch_size, gamma, local_steps, doubleDQN):\n",
    "    if len(memory) < batch_size:\n",
    "        return None\n",
    "    \n",
    "    batch, indices, weights = memory.sample(batch_size, local_steps, device)\n",
    "\n",
    "    state, action, reward, next_state, done = zip(*batch)\n",
    "    \n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "    action = torch.tensor(action, dtype=torch.long, device=device)\n",
    "    next_state = torch.tensor(next_state, dtype=torch.float32, device=device)\n",
    "    reward = torch.tensor(reward, dtype=torch.float32, device=device)\n",
    "    done = torch.tensor(done, dtype=torch.float32, device=device)\n",
    "    \n",
    "    weights.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    q_val = compute_q_val(online_network, state, action)\n",
    "\n",
    "    with torch.no_grad():\n",
    "# Vanilla\n",
    "#         target = compute_target_dqn(q1, reward, next_state, done, gamma)\n",
    "        if doubleDQN:\n",
    "            target = compute_target_ddqn(online_network, target_network, reward, next_state, done, gamma)\n",
    "        else:\n",
    "            target = compute_target_dqn(target_network, reward, next_state, done, gamma)\n",
    "#     loss = F.mse_loss(q_val, target)\n",
    "    difference = (q_val - target.view(-1,1))\n",
    "    \n",
    "    # Weights is 1 for normal replay buffer so nothing changes\n",
    "    # McAleer divides the loss by the number of moves of the scramble here. Might not make sense in non-MCTS setting\n",
    "    loss = difference.pow(2) * weights\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    # Also taken from higgsfield\n",
    "    memory.update_priorities(indices, difference.detach().squeeze().abs().cpu().numpy().tolist())\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increase_capacity_keep_lr(network, capacity, optimizer, device):\n",
    "    # Store old ids\n",
    "    old_ids = [id(p) for p in network.parameters()]\n",
    "    old_param_sizes = [p.size() for p in network.parameters()]\n",
    "\n",
    "    network.increase_capacity(capacity)\n",
    "\n",
    "    # Store new ids\n",
    "    new_ids = [id(p) for p in network.parameters()]\n",
    "    new_param_sizes = [p.size() for p in network.parameters()]\n",
    "\n",
    "    # Store old state \n",
    "    opt_state_dict = optimizer.state_dict()\n",
    "    for old_id, new_id, new_param_size, old_param_size in zip(old_ids, new_ids, new_param_sizes, old_param_sizes):\n",
    "        # Store step, and exp_avgs\n",
    "        step = opt_state_dict['state'][old_id]['step']\n",
    "        old_exp_avg = opt_state_dict['state'][old_id]['exp_avg']\n",
    "        old_exp_avg_sq = opt_state_dict['state'][old_id]['exp_avg_sq']\n",
    "        old_max_exp_avg_sq = opt_state_dict['state'][old_id]['max_exp_avg_sq']\n",
    "\n",
    "        exp_avg = torch.zeros(new_param_size)\n",
    "        exp_avg_sq = torch.zeros(new_param_size)\n",
    "        max_exp_avg_sq =  torch.zeros(new_param_size)\n",
    "        # Extend exp_avgs to new shape depending on wether param is bias or weight\n",
    "        if exp_avg.dim()>1:\n",
    "            # Weights\n",
    "            exp_avg[0:old_param_size[0],0:old_param_size[1]] = old_exp_avg\n",
    "            exp_avg_sq[0:old_param_size[0],0:old_param_size[1]] = old_exp_avg_sq\n",
    "            max_exp_avg_sq[0:old_param_size[0],0:old_param_size[1]] = old_max_exp_avg_sq\n",
    "        else:\n",
    "            # Biases/last layer\n",
    "            exp_avg[0:old_param_size[0]] = old_exp_avg\n",
    "            exp_avg_sq[0:old_param_size[0]] = old_exp_avg_sq\n",
    "            max_exp_avg_sq[0:old_param_size[0]] = old_max_exp_avg_sq\n",
    "        \n",
    "        # Delete old id from state_dict and update new params and new id\n",
    "        del opt_state_dict['state'][old_id]\n",
    "        opt_state_dict['state'][new_id] = {\n",
    "            'step': step,\n",
    "            'exp_avg': exp_avg,\n",
    "            'exp_avg_sq': exp_avg_sq.to(device),\n",
    "            'max_exp_avg_sq' : max_exp_avg_sq.to(device)\n",
    "        }\n",
    "        opt_state_dict['param_groups'][0]['params'].remove(old_id)\n",
    "        opt_state_dict['param_groups'][0]['params'].append(new_id)\n",
    "\n",
    "    network.to(device)\n",
    "    optimizer = optim.Adam(network.parameters(), amsgrad=True)\n",
    "    optimizer.load_state_dict(opt_state_dict)\n",
    "    \n",
    "    return network, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_settings(architecture, duelingDQN, doubleDQN, prioritizedReplayMemory, alpha, \n",
    "                        memoryCapacity, lr, amsgrad, epochs, batch_size, gamma, capacity_increase, \n",
    "                        threshold, evaluation_frequency, tau, curriculum, non_linearity,\n",
    "                        verbose=False, load_path=None, save_path=None, seed=None):\n",
    "    # If the directory does not exist, make one\n",
    "    if save_path:\n",
    "        if not os.path.isdir(save_path):\n",
    "            os.mkdir(save_path)\n",
    "    \n",
    "    # If a seed is set, set the seed for all sources of randomness\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "            \n",
    "    # Difficulty the problem starts with\n",
    "    difficulty = 0\n",
    "    # The maximum number of tries the agent gets at the start\n",
    "    max_tries_start = 1\n",
    "    max_tries = max_tries_start\n",
    "    # 3 is chosen because this allows the network to learn the difference between short and long paths from the beginning. Take for example a cube that has been scrambled as follows: U. The solution within 3 steps is eather U' (r=1) or U, U, U (r=-1)\n",
    "    \n",
    "    # Arrays to keep track of losses and accuracies over time\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    # Global steps keeps track of the total number of optimisation steps\n",
    "    global_steps = 0\n",
    "    # Local steps keeps track of number of optimisation steps within a level\n",
    "    local_steps = 0\n",
    "    # Total time keeps track of how long the training process takes\n",
    "    \n",
    "    # Epsilon exponential decay\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_final = 0.01\n",
    "    # Duration of decay dependent on difficulty\n",
    "#     epsilon_decay = 10000*(difficulty//12 + 1)\n",
    "    epsilon_decay = 1000\n",
    "    # Duration independent on dificulty\n",
    "    epsilon_by_step = lambda step_idx: epsilon_final + (epsilon_start - epsilon_final) * np.exp(-1. * step_idx / epsilon_decay)\n",
    "    \n",
    "    # If a path with a model is provided certain variables are loaded\n",
    "    if load_path:\n",
    "        current_model = torch.load(load_path + 'model.pt')\n",
    "        target_model = copy.deepcopy(current_model)\n",
    "        max_tries = torch.load(load_path + 'max_tries')\n",
    "        epsilon_decay = torch.load(load_path + 'epsilon_decay')\n",
    "        global_steps = torch.load(load_path + 'global_steps')\n",
    "        local_steps = torch.load(load_path + 'local_steps')\n",
    "        difficulty = torch.load(load_path + 'difficulty')\n",
    "    else:\n",
    "        # n^2 per face, 6 faces, 6 colours one-hot encoded. 3x3x6x6=324, 2x2x6x6=144\n",
    "        state_size = env.size**2 * 6**2\n",
    "        \n",
    "        # The number of output nodes is different for the 2x2x2. The move L is the same as the move R, except the orientation changes.\n",
    "        output_nodes = 12\n",
    "        if env.size == 2:\n",
    "            output_nodes = 6\n",
    "        \n",
    "        # Initialising either a network with dueling architecture or regular network\n",
    "        if duelingDQN:\n",
    "            current_model = DuelingDQN(state_size, architecture, output_nodes, non_linearity)\n",
    "            target_model = DuelingDQN(state_size, copy.copy(architecture), output_nodes, non_linearity)\n",
    "        else:\n",
    "            current_model = DQN(state_size, architecture, output_nodes, non_linearity)\n",
    "            target_model = DQN(state_size, copy.copy(architecture), output_nodes, non_linearity)\n",
    "            \n",
    "    if torch.cuda.is_available():\n",
    "        current_model.to('cuda')\n",
    "        target_model.to('cuda')\n",
    "    else:\n",
    "        current_model.to('cpu')\n",
    "        target_model.to('cpu')\n",
    "    \n",
    "    # Uses prioritized replay sampling when set to true, otherwise uniform replay sampling is used\n",
    "    if prioritizedReplayMemory:\n",
    "        memory = PrioritizedReplayMemory(memoryCapacity, alpha)\n",
    "    else:\n",
    "        memory = ReplayMemory(memoryCapacity)\n",
    "    \n",
    "    # Initialise optimiser\n",
    "    optimizer = optim.Adam(current_model.parameters(), lr=lr, amsgrad=amsgrad)\n",
    "    \n",
    "    # This allows you to stop the training progress\n",
    "    try:\n",
    "        for epoch in range(epochs):\n",
    "            epoch_losses = []\n",
    "            # Different types of curricula decide what state to show the network next.\n",
    "            if curriculum is 'Naive':\n",
    "                state = env.curriculum_reset(difficulty)\n",
    "            elif curriculum is 'Joe':\n",
    "                p = np.random.rand()\n",
    "                if p < 0.2:\n",
    "                    state = env.curriculum_reset(np.random.randint(1, difficulty + 12))\n",
    "                else:\n",
    "                    state = env.curriculum_reset(difficulty)\n",
    "            elif curriculum is 'Sutskever':\n",
    "                p = np.random.rand()\n",
    "                if p < 0.2:\n",
    "                    state = env.reset(np.random.randint(1, 1000))\n",
    "                else:\n",
    "                    state = env.curriculum_reset(difficulty)\n",
    "            elif curriculum is 'Mixed':\n",
    "                state = env.curriculum_reset(np.random.randint(1,1000))\n",
    "            else:\n",
    "                state = env.reset(1000)\n",
    "\n",
    "            done = 0\n",
    "            tries = 0\n",
    "            while tries < max_tries and not done:\n",
    "                \n",
    "                epsilon = epsilon_by_step(local_steps)\n",
    "                action = current_model.act(state, epsilon, [0]*env.action_space.n, device)\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                memory.push((state, action, reward, next_state, done))\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                tries += 1\n",
    "            \n",
    "            loss = train_dqn(current_model, target_model, memory, optimizer, batch_size, gamma, local_steps, doubleDQN)\n",
    "\n",
    "            if loss:\n",
    "                epoch_losses.append(loss)\n",
    "                \n",
    "            global_steps += 1\n",
    "            local_steps += 1\n",
    "                \n",
    "            if global_steps % tau == 0:\n",
    "                target_model.load_state_dict(current_model.state_dict())\n",
    "                if save_path:\n",
    "                    torch.save(current_model, save_path + \"model.pt\")\n",
    "                    torch.save(max_tries, save_path + \"max_tries\")\n",
    "                    torch.save(epsilon_decay, save_path + \"epsilon_decay\")\n",
    "                    torch.save(global_steps, save_path + \"global_steps\")\n",
    "                    torch.save(local_steps, save_path + \"local_steps\")\n",
    "                    torch.save(local_steps, save_path + \"difficulty\")\n",
    "                    \n",
    "            if global_steps % evaluation_frequency == 0 and curriculum is not None:\n",
    "                total_done = 0\n",
    "                for i in range(difficulty + 1):\n",
    "                    # Here the agent is forced to evaluate its ability to solve certain last moves\n",
    "                    hashes = defaultdict(list)\n",
    "                    state = env.force_last_action_reset(i)\n",
    "                    hashes[hash(state.tostring())] = [0]*env.action_space.n\n",
    "                    done = 0\n",
    "                    tries = 0\n",
    "                    while tries < max_tries and not done:\n",
    "                        mask = hashes[hash(state.tostring())]\n",
    "                        action = current_model.act(state, 0, mask, device)\n",
    "                        next_state, reward, done, info = env.step(action)\n",
    "                        # memory.push((state, action, reward, next_state, done))\n",
    "                        hstate = state.copy()\n",
    "                        state = next_state\n",
    "                        \n",
    "                        h = hash(state.tostring())\n",
    "                        if h in hashes.keys():\n",
    "                            hashes[hash(hstate.tostring())][action] = -999\n",
    "                        else:\n",
    "                            hashes[h] = [0]*env.action_space.n\n",
    "                        \n",
    "                        total_done += done\n",
    "                        tries += 1\n",
    "                        \n",
    "                # Here the agent evaluates the ability to solve puzzles that have the last move from a set of moves with the same difficulty\n",
    "                for i in range(1000):\n",
    "                    hashes = defaultdict(list)\n",
    "                    state = env.curriculum_reset(difficulty)\n",
    "                    hashes[hash(state.tostring())] = [0]*env.action_space.n\n",
    "                    done = 0\n",
    "                    tries = 0\n",
    "                    while tries < max_tries and not done:\n",
    "                        mask = hashes[hash(state.tostring())]\n",
    "                        action = current_model.act(state, 0, mask, device)\n",
    "                        next_state, reward, done, info = env.step(action)\n",
    "                        # memory.push((state, action, reward, next_state, done))\n",
    "                        hstate = state.copy()\n",
    "                        state = next_state\n",
    "                        \n",
    "                        h = hash(state.tostring())\n",
    "                        if h in hashes.keys():\n",
    "                            hashes[hash(hstate.tostring())][action] = -999\n",
    "                        else:\n",
    "                            hashes[h] = [0]*env.action_space.n\n",
    "                            \n",
    "                        \n",
    "                        total_done += done\n",
    "                        tries += 1\n",
    "                \n",
    "                accuracy = total_done/(1000 + difficulty + 1)\n",
    "                accuracies.append(accuracy)\n",
    "                \n",
    "                if accuracy >= threshold:\n",
    "                    difficulty += 1\n",
    "                    max_tries = difficulty // output_nodes + max_tries_start\n",
    "                    local_steps = 0\n",
    "                    \n",
    "                    epsilon_start = 1.0\n",
    "                    epsilon_final = 0.01\n",
    "#                     epsilon_decay = 10000*(difficulty//output_nodes + 1)\n",
    "                    epsilon_decay = 1000\n",
    "\n",
    "                    epsilon_by_step = lambda step_idx: epsilon_final + (epsilon_start - epsilon_final) * np.exp(-1. * step_idx / epsilon_decay)\n",
    "                \n",
    "                if local_steps > 1000:\n",
    "                    if capacity_increase:\n",
    "                        local_steps = 0\n",
    "                        capacity = [c(difficulty) for c in capacity_increase]\n",
    "\n",
    "                        current_model, optimizer = increase_capacity_keep_lr(current_model, capacity, optimizer, device)\n",
    "                        target_model.increase_capacity(capacity)\n",
    "\n",
    "                        target_model.to(device)\n",
    "\n",
    "                if verbose:\n",
    "                    clear_output(True)\n",
    "                    print(\"Epoch: \", epoch, \"Global steps: \", global_steps)\n",
    "                    print(\"Difficulty: \", difficulty, \"Max tries:\", max_tries)\n",
    "                    print(\"Memory: \", len(memory), \"epsilon: \", epsilon, \"local steps: \", local_steps)\n",
    "                    print(\"Accuracy: \", accuracy, \"Threshold: \", threshold)\n",
    "                    print(current_model)\n",
    "                    \n",
    "                losses.append(np.average(epoch_losses))\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    if save_path:\n",
    "        torch.save(current_model, save_path + \"model.pt\")\n",
    "        torch.save(max_tries, save_path + \"max_tries\")\n",
    "        torch.save(epsilon_decay, save_path + \"epsilon_decay\")\n",
    "        torch.save(global_steps, save_path + \"global_steps\")\n",
    "        torch.save(local_steps, save_path + \"local_steps\")\n",
    "        torch.save(local_steps, save_path + \"difficulty\")\n",
    "            \n",
    "    return difficulty, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50000\n",
    "experiments = []\n",
    "# experiments.append([\"Baseline3\", [4096, 2048, 512], False, False, False, 0.7, 100000, 1e-3, False, epochs, 128, 0.99, None, 0.95, 100, 1000, 'Naive', F.relu])\n",
    "# experiments.append([\"Mixed3\", [4096, 2048, 512], False, False, False, 0.7, 100000, 1e-3, True, epochs, 128, 0.99, None, 0.95, 100, 1000, 'Mixed', F.relu])\n",
    "# experiments.append([\"Joe3\", [4096, 2048, 512], False, False, False, 0.7, 100000, 1e-3, True, epochs, 128, 0.99, None, 0.95, 100, 1000, 'Joe', F.relu])\n",
    "# experiments.append([\"Sutskever3\", [4096, 2048, 512], False, False, False, 0.7, 100000, 1e-3, True, epochs, 128, 0.99, None, 0.95, 100, 1000, 'Sutskever', F.relu])\n",
    "# experiments.append([\"Prioritised0.53\", [4096, 2048, 512], False, False, True, 0.5, 100000, 1e-3, True, epochs, 128, 0.99, None, 0.95, 100, 1000, 'Sutskever', F.relu])\n",
    "# experiments.append([\"Prioritised0.73\", [4096, 2048, 512], False, False, True, 0.7, 100000, 1e-3, True, epochs, 128, 0.99, None, 0.95, 100, 1000, 'Sutskever', F.relu])\n",
    "# experiments.append([\"Prioritised0.93\", [4096, 2048, 512], False, False, True, 0.9, 100000, 1e-3, True, epochs, 128, 0.99, None, 0.95, 100, 1000, 'Sutskever', F.relu])\n",
    "# experiments.append([\"Dueling3\", [4096, 2048, 512], True, False, True, 0.9, 100000, 1e-3, True, epochs, 128, 0.99, None, 0.95, 100, 1000, 'Sutskever', F.relu])\n",
    "# experiments.append([\"Double3\", [4096, 2048, 512], True, True, True, 0.9, 100000, 1e-3, True, epochs, 128, 0.99, None, 0.95, 100, 1000, 'Sutskever', F.relu])\n",
    "# experiments.append([\"Linear_all3\", [64, 32, 8], True, True, True, 0.9, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: 48, lambda x: 24, lambda x: 6], 0.95, 100, 1000, 'Sutskever', F.relu])\n",
    "# experiments.append([\"Elu3\", [4096, 2048, 512], True, True, True, 0.9, 100000, 1e-3, True, epochs, 128, 0.99, None, 0.95, 100, 1000, 'Sutskever', F.elu])\n",
    "experiments.append([\"Increasing_all\", [8, 4, 1], True, True, True, 0.9, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: 8, lambda x: 4, lambda x: 1], 0.95, 100, 1000, 'Sutskever', F.relu])\n",
    "experiments.append([\"Increasing_all\", [64, 32, 8], True, True, True, 0.9, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: 48, lambda x: 24, lambda x: 6], 0.95, 100, 1000, 'Sutskever', F.relu])\n",
    "experiments.append([\"Increasing_bottom\", [64, 32, 512], True, True, True, 0.9, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: 48, lambda x: 24, lambda x: 0], 0.95, 100, 1000, 'Sutskever', F.relu])\n",
    "experiments.append([\"Same\", [4096, 2048, 512], True, True, True, 0.9, 100000, 1e-3, True, epochs, 128, 0.99, [lambda x: 0, lambda x: 0, lambda x: 0], 0.95, 100, 1000, 'Sutskever', F.relu])\n",
    "\n",
    "for experiment in experiments:\n",
    "    architecture, duelingDQN, doubleDQN, prioritizedReplayMemory, alpha, memoryCapacity, lr, amsgrad, epochs, batch_size, gamma, capacity_increase, threshold, evaluation_frequency, tau, curriculum, non_linearity = experiment[1:]\n",
    "\n",
    "    env = rubiks2.RubiksEnv2(2, unsolved_reward = -1.0, seed= 420)\n",
    "\n",
    "    train_with_settings(architecture, duelingDQN, doubleDQN, prioritizedReplayMemory, alpha, memoryCapacity, lr, amsgrad, epochs, batch_size, gamma, capacity_increase, threshold, evaluation_frequency, tau, curriculum, non_linearity, save_path='models/'+experiment[0]+'/', seed=420, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def superflip_test(model):\n",
    "    eval_model = torch.load(model)\n",
    "    eval_model.to(device)\n",
    "    \n",
    "    max_tries = 100\n",
    "    solve_rate_per_difficulty = []\n",
    "    average_tries_per_difficulty = []\n",
    "    \n",
    "    for i in range(14):\n",
    "        solved = 0\n",
    "        tries_solved = []\n",
    "        cycles = 0\n",
    "        start_states = defaultdict(list)\n",
    "        \n",
    "        for sequence in superflip_set:\n",
    "            env = rubiks2.RubiksEnv2(2, unsolved_reward=-1.0)\n",
    "            goal = env.get_observation()\n",
    "            \n",
    "            for j in range(i + 1):\n",
    "                env.step(sequence[j])\n",
    "\n",
    "            \n",
    "            hashes = defaultdict(list)\n",
    "            state = env.get_observation()\n",
    "            \n",
    "            # Remove duplicate starting states\n",
    "            if hash(state.tostring()) in start_states.keys():\n",
    "                continue\n",
    "                \n",
    "            hashes[hash(state.tostring())] = [0,0,0,0,0,0]\n",
    "            done = 0\n",
    "            tries = 0\n",
    "            \n",
    "            while not done and tries < max_tries:\n",
    "                mask = hashes[hash(state.tostring())]\n",
    "                \n",
    "                action = eval_model.act(state, 0, mask, device)\n",
    "\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "\n",
    "                hstate = state.copy()\n",
    "                state = next_state\n",
    "\n",
    "                tries += 1\n",
    "\n",
    "                h = hash(state.tostring())\n",
    "                if h in hashes.keys():\n",
    "                    #hashes[hash(hstate.tostring())][action] = -999\n",
    "                    cycles += 1\n",
    "                else:\n",
    "                    hashes[h] = [0,0,0,0,0,0]\n",
    "            if done:\n",
    "                solved += 1\n",
    "            tries_solved.append(tries)   \n",
    "            \n",
    "        solve_rate_per_difficulty.append(solved / len(superflip_set))\n",
    "        average_tries_per_difficulty.append(np.average(tries_solved))\n",
    "        print('Level', i, 'accuracy:', solved / len(superflip_set))\n",
    "        print(np.average(tries_solved), np.median(tries_solved))\n",
    "        \n",
    "    score = 0\n",
    "    for target_tries, avg_tries in enumerate(average_tries_per_difficulty):\n",
    "        score += ((target_tries + 1)/avg_tries)*solve_rate_per_difficulty[target_tries]\n",
    "    score /= 14\n",
    "    return solve_rate_per_difficulty, average_tries_per_difficulty, score\n",
    "    \n",
    "\n",
    "print(superflip_test('./models/Increasing_all/model.pt'))\n",
    "print(superflip_test('./models/Increasing_bottom/model.pt'))\n",
    "print(superflip_test('./models/Same/model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = rubiks2.RubiksEnv2(2, unsolved_reward = -1.0, seed= 42)\n",
    "\n",
    "# Which tests are failing?\n",
    "difficulty = 42\n",
    "max_tries = 16\n",
    "current_model = torch.load('./models/Same/model.pt')\n",
    "current_model.to(device)\n",
    "\n",
    "total = 0\n",
    "total_done = 0\n",
    "for i in range(difficulty + 1):\n",
    "    # Here the agent is forced to evaluate its ability to solve certain last moves\n",
    "    hashes = defaultdict(list)\n",
    "    state = env.force_last_action_reset(i)\n",
    "    hashes[hash(state.tostring())] = [0]*env.action_space.n\n",
    "    done = 0\n",
    "    tries = 0\n",
    "    while tries < max_tries and not done:\n",
    "        mask = hashes[hash(state.tostring())]\n",
    "        action = current_model.act(state, 0, mask, device)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        hstate = state.copy()\n",
    "        state = next_state\n",
    "\n",
    "        h = hash(state.tostring())\n",
    "        if h in hashes.keys():\n",
    "            hashes[hash(hstate.tostring())][action] = -999\n",
    "        else:\n",
    "            hashes[h] = [0]*env.action_space.n\n",
    "\n",
    "        total_done += done\n",
    "        tries += 1\n",
    "    total += 1\n",
    "print(total_done)\n",
    "total_done_2 = 0\n",
    "# Here the agent evaluates the ability to solve puzzles that have the last move from a set of moves with the same difficulty\n",
    "for i in range(1000):\n",
    "    hashes = defaultdict(list)\n",
    "    state = env.curriculum_reset(difficulty)\n",
    "    \n",
    "    hashes[hash(state.tostring())] = [0]*env.action_space.n\n",
    "    done = 0\n",
    "    tries = 0\n",
    "    while tries < max_tries and not done:\n",
    "        mask = hashes[hash(state.tostring())]\n",
    "        action = current_model.act(state, 0, mask, device)\n",
    "        q_val = current_model(torch.tensor(state, dtype=torch.float, device=device))\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        hstate = state.copy()\n",
    "        state = next_state\n",
    "\n",
    "        h = hash(state.tostring())\n",
    "        if h in hashes.keys():\n",
    "            hashes[hash(hstate.tostring())][action] = -999\n",
    "        else:\n",
    "            hashes[h] = [0]*env.action_space.n\n",
    "\n",
    "\n",
    "        total_done += done\n",
    "        total_done_2 += done\n",
    "        tries += 1\n",
    "\n",
    "    total+=1\n",
    "    \n",
    "    if not done:\n",
    "        print(q_val\n",
    "             )\n",
    "        print([env.ACTION_MEANING_QUARTER_METRIC[scr] for scr in env.last_scramble])\n",
    "\n",
    "print(total_done_2)\n",
    "print(total_done/total)\n",
    "accuracy = total_done/(1000 + difficulty + 1)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "superflip_set = [[4,0,5,1,5,4,5,5,1,3,2,1,3,3],\n",
    "[0,5,1,1,0,0,1,5,0,2,2,4,4,5],\n",
    "[1,2,2,3,3,5,0,4,0,2,4,3,5,0],\n",
    "[1,0,5,0,4,3,3,1,3,1,0,2,3,2],\n",
    "[5,0,5,1,5,1,5,1,5,1,5,3,1,5],\n",
    "[3,5,3,2,4,2,4,5,1,0,4,3,5,0],\n",
    "[2,4,2,0,0,2,4,5,3,1,0,0,4,4],\n",
    "[5,0,5,4,3,2,3,4,3,2,4,2,2,1],\n",
    "[2,0,4,0,4,0,4,0,4,5,3,5,4,5],\n",
    "[2,4,2,4,3,2,2,1,3,5,4,4,2,3],\n",
    "[4,4,0,4,0,5,4,5,0,0,1,3,1,2],\n",
    "[0,4,0,5,1,1,0,2,2,3,2,3,5,5],\n",
    "[0,1,0,4,2,4,2,4,0,2,4,5,4,3],\n",
    "[2,3,2,2,1,5,1,5,3,5,1,5,5,4],\n",
    "[4,0,5,4,0,4,3,2,3,5,1,0,5,0],\n",
    "[2,3,1,2,1,5,3,2,3,1,2,1,2,2],\n",
    "[5,4,0,5,3,1,2,4,2,1,5,1,0,2],\n",
    "[2,4,0,1,3,4,0,0,2,2,4,0,5,4],\n",
    "[5,1,2,3,4,3,5,3,2,4,2,0,2,0],\n",
    "[1,2,0,4,2,3,1,3,1,0,4,5,0,1],\n",
    "[4,2,1,5,0,2,3,5,4,2,3,1,5,3],\n",
    "[2,3,4,5,1,5,0,4,2,3,1,2,1,3],\n",
    "[0,5,1,3,2,4,0,0,4,0,4,0,5,0],\n",
    "[5,1,2,3,5,1,3,4,2,4,5,5,0,2],\n",
    "[0,4,3,5,1,0,5,0,2,3,4,4,3,1],\n",
    "[3,3,4,2,4,5,1,3,3,5,0,2,0,0],\n",
    "[5,3,1,3,5,1,0,0,1,0,1,5,4,4],\n",
    "[2,0,1,5,1,3,1,2,4,3,2,4,0,2],\n",
    "[1,0,4,3,5,3,5,1,5,0,4,3,2,1],\n",
    "[3,1,0,2,2,0,5,3,1,3,4,5,1,3],\n",
    "[1,3,5,4,0,2,2,3,1,3,4,0,2,3],\n",
    "[5,3,1,1,3,2,1,5,0,5,1,1,2,3],\n",
    "[4,3,2,1,3,5,1,5,4,4,2,4,3,4],\n",
    "[4,4,3,5,0,5,5,3,1,3,2,4,5,1],\n",
    "[4,4,5,0,5,3,2,1,1,3,1,0,1,1],\n",
    "[5,1,2,3,2,1,2,3,3,4,3,2,4,4],\n",
    "[3,3,1,2,4,0,4,2,4,3,3,4,2,2],\n",
    "[3,5,0,5,5,3,1,3,2,3,2,1,5,4],\n",
    "[3,4,2,2,4,5,1,5,0,5,1,2,4,5],\n",
    "[4,2,4,2,2,1,0,4,0,1,2,4,5,4],\n",
    "[4,0,4,0,5,1,2,1,3,2,1,0,5,3],\n",
    "[1,2,2,3,3,4,0,2,1,0,4,0,5,0],\n",
    "[2,3,3,5,1,3,2,3,1,5,4,4,2,2],\n",
    "[1,2,2,4,2,4,0,4,3,3,5,5,4,0],\n",
    "[4,0,4,4,5,4,4,2,4,0,5,5,0,4],\n",
    "[5,1,3,2,1,3,2,1,2,1,0,5,4,3],\n",
    "[4,4,2,0,1,1,5,3,1,3,2,0,4,5],\n",
    "[0,1,5,3,2,1,5,3,2,4,0,2,4,2],\n",
    "[1,3,2,3,5,0,1,5,0,1,3,4,0,0],\n",
    "[1,2,1,3,1,5,1,3,1,5,4,3,5,5],\n",
    "[3,1,3,4,2,3,4,3,4,4,0,2,3,1],\n",
    "[5,1,2,2,1,5,0,5,5,3,5,5,1,3],\n",
    "[5,3,2,4,0,5,4,3,2,4,5,3,2,3],\n",
    "[1,2,1,3,2,4,0,4,2,0,1,2,3,5],\n",
    "[3,5,5,3,5,0,5,4,2,2,4,2,3,2],\n",
    "[4,5,1,5,4,2,1,1,3,5,1,2,3,2],\n",
    "[3,5,1,3,3,1,2,3,2,3,1,2,4,2],\n",
    "[2,0,1,3,2,3,1,3,2,4,5,0,4,4],\n",
    "[1,2,4,2,0,0,4,3,1,3,2,2,0,0],\n",
    "[2,3,3,5,4,0,4,0,4,5,4,5,4,0],\n",
    "[2,1,3,2,2,3,1,3,2,0,4,3,3,2],\n",
    "[4,4,5,4,2,2,4,0,5,0,1,3,2,3],\n",
    "[3,2,4,4,3,5,0,4,0,4,5,5,1,0],\n",
    "[3,4,2,4,2,4,2,4,0,2,0,2,0,1],\n",
    "[4,5,3,4,0,4,5,0,1,3,1,2,1,2],\n",
    "[3,5,5,1,3,2,4,3,3,4,3,2,1,1],\n",
    "[3,4,4,3,1,2,3,1,5,0,5,4,5,0],\n",
    "[0,2,0,1,1,5,0,1,3,1,3,5,1,5],\n",
    "[3,1,5,1,2,0,5,3,1,5,5,4,2,4],\n",
    "[2,4,4,5,3,1,3,5,1,2,4,3,3,4],\n",
    "[3,1,3,4,3,5,1,5,0,5,4,4,0,4],\n",
    "[2,3,1,5,3,1,5,1,2,1,5,5,1,2],\n",
    "[3,3,4,0,1,5,3,2,3,3,2,3,1,3],\n",
    "[1,5,3,1,2,0,2,0,4,0,5,3,5,4],\n",
    "[0,1,1,5,0,4,5,0,2,4,5,1,5,4],\n",
    "[3,2,4,2,1,5,0,0,1,2,4,5,4,0],\n",
    "[0,4,0,5,0,4,2,4,2,2,4,2,3,2],\n",
    "[5,0,5,5,4,3,5,1,5,1,5,3,1,5],\n",
    "[0,5,4,5,0,4,0,4,0,1,5,3,3,2],\n",
    "[5,5,3,2,3,1,0,1,5,1,1,3,5,5],\n",
    "[0,5,1,0,5,3,3,5,1,2,0,5,5,4],\n",
    "[0,4,5,5,1,5,0,5,3,1,5,4,3,1],\n",
    "[5,1,0,4,2,2,0,1,2,2,1,5,1,1],\n",
    "[4,4,0,2,3,1,5,1,3,4,4,3,1,1],\n",
    "[0,5,1,5,1,2,2,0,0,1,1,2,4,2],\n",
    "[2,0,2,2,3,1,0,4,0,2,2,3,1,2],\n",
    "[4,0,2,3,1,5,3,1,5,0,2,4,0,4],\n",
    "[1,3,5,4,0,2,4,2,4,3,5,4,4,0],\n",
    "[5,4,3,5,1,5,1,1,2,4,2,0,4,3],\n",
    "[1,5,4,0,5,4,3,1,2,3,4,4,5,3],\n",
    "[0,2,3,2,0,2,3,1,3,1,5,1,0,0],\n",
    "[5,4,3,2,3,2,0,5,0,4,4,2,0,2],\n",
    "[2,1,5,1,2,4,4,0,2,1,2,4,3,3],\n",
    "[2,3,2,2,1,0,1,3,5,1,2,1,1,2],\n",
    "[5,0,4,3,1,3,4,0,1,5,1,0,1,3],\n",
    "[5,5,0,0,2,4,2,1,5,0,4,3,4,4],\n",
    "[1,0,5,0,1,2,4,5,1,3,5,0,2,3],\n",
    "[4,4,2,0,5,1,5,0,5,1,1,5,3,3],\n",
    "[0,2,4,5,5,4,5,3,1,3,3,2,3,1],\n",
    "[4,0,5,5,1,3,5,1,3,1,3,5,0,4],\n",
    "[0,2,2,4,2,4,3,2,2,3,3,1,0,2],\n",
    "[5,1,3,1,2,0,4,2,2,0,4,4,3,1],\n",
    "[1,2,1,1,0,5,0,5,1,2,0,5,3,5],\n",
    "[3,1,5,1,0,4,3,1,0,2,4,5,5,0],\n",
    "[5,5,1,0,4,2,3,2,4,5,5,1,2,2],\n",
    "[2,3,2,3,1,5,0,1,2,3,3,5,1,3],\n",
    "[4,2,4,4,0,4,5,0,5,0,4,3,1,3],\n",
    "[1,3,1,3,4,0,2,2,0,1,0,4,2,2],\n",
    "[3,1,5,1,5,0,4,3,4,5,5,1,3,1],\n",
    "[5,4,0,1,5,0,2,4,0,5,1,1,2,3],\n",
    "[2,1,0,4,5,0,4,2,3,2,4,2,1,0],\n",
    "[0,4,2,1,3,2,4,0,2,1,5,5,3,1],\n",
    "[0,5,3,4,4,3,2,0,5,0,4,5,4,3],\n",
    "[2,3,2,3,5,0,1,5,4,5,0,5,5,4],\n",
    "[4,0,5,5,0,2,0,4,2,0,4,5,0,4],\n",
    "[4,0,4,5,1,5,0,1,2,3,2,1,1,3],\n",
    "[3,4,4,2,1,5,1,3,2,4,4,5,1,1],\n",
    "[0,5,3,3,2,0,2,4,0,4,0,5,1,2],\n",
    "[2,1,5,1,3,1,0,5,0,2,2,3,3,1],\n",
    "[0,4,5,5,0,4,3,1,3,2,3,2,1,3],\n",
    "[1,0,0,5,0,5,1,3,3,1,2,3,2,2],\n",
    "[4,5,1,3,4,2,0,5,4,2,3,5,5,1],\n",
    "[3,2,1,5,3,2,3,4,5,4,4,2,1,3],\n",
    "[3,5,5,1,3,5,3,2,2,1,3,5,1,2],\n",
    "[3,4,2,1,1,2,3,4,3,5,3,5,5,1],\n",
    "[0,4,0,5,0,4,3,2,3,1,1,0,2,1],\n",
    "[2,1,5,0,4,3,4,2,3,3,1,0,0,1],\n",
    "[4,4,3,2,4,2,0,2,0,0,5,0,5,4],\n",
    "[2,0,4,0,4,5,4,2,0,0,2,4,5,0],\n",
    "[1,2,3,1,3,2,4,4,5,3,4,3,5,0],\n",
    "[4,2,3,5,3,2,3,5,3,1,2,3,5,0],\n",
    "[3,4,0,4,2,3,1,0,5,4,4,0,2,2],\n",
    "[4,2,2,4,0,2,3,1,5,1,1,0,4,4],\n",
    "[1,2,0,2,0,4,0,5,3,1,3,4,0,4],\n",
    "[0,1,3,1,1,5,0,1,3,2,4,4,0,5],\n",
    "[2,3,1,3,1,2,3,4,4,2,4,4,3,1],\n",
    "[4,0,5,5,4,5,1,5,0,4,5,3,1,0],\n",
    "[2,4,2,3,2,0,5,4,4,0,5,5,4,0],\n",
    "[5,3,1,5,0,2,1,3,4,2,1,5,1,3],\n",
    "[4,4,0,5,1,3,1,2,1,5,0,1,0,5],\n",
    "[5,1,1,5,1,2,3,1,3,5,5,1,0,0],\n",
    "[1,5,3,2,3,5,0,1,2,2,1,5,1,3],\n",
    "[5,4,0,0,2,1,0,4,2,3,1,3,1,2],\n",
    "[0,1,1,0,4,3,5,0,4,5,3,1,5,3],\n",
    "[5,4,2,4,2,3,2,1,0,0,4,5,0,0],\n",
    "[4,0,5,0,5,0,2,0,4,3,2,3,4,5],\n",
    "[2,0,5,0,4,4,5,4,2,4,0,0,4,4],\n",
    "[0,4,2,3,1,3,4,0,2,2,4,2,2,0],\n",
    "[2,3,4,5,0,1,2,3,2,1,3,5,3,3],\n",
    "[4,2,3,4,3,1,1,3,4,3,4,2,0,4],\n",
    "[0,4,3,2,4,5,5,1,3,2,1,3,5,5],\n",
    "[5,1,0,1,0,5,1,5,4,2,2,4,3,4],\n",
    "[5,5,4,5,4,0,1,5,3,2,1,3,4,4],\n",
    "[4,4,0,5,3,1,5,0,1,3,3,4,2,4],\n",
    "[5,3,2,1,5,1,3,4,5,0,4,4,5,5],\n",
    "[5,0,5,0,5,1,5,5,0,4,3,4,0,2],\n",
    "[0,2,3,4,5,1,5,1,1,3,2,0,4,0],\n",
    "[5,3,2,3,5,0,1,5,4,2,4,3,3,2],\n",
    "[2,4,4,0,4,0,4,2,3,1,2,0,4,5],\n",
    "[5,5,1,3,5,4,3,2,1,5,4,2,3,4],\n",
    "[5,5,4,2,3,1,5,1,2,0,4,3,4,5],\n",
    "[4,2,3,5,3,2,3,5,3,1,2,3,5,3],\n",
    "[1,3,2,4,5,3,3,1,0,2,3,1,0,2],\n",
    "[3,2,4,4,0,5,3,2,0,2,2,1,1,3],\n",
    "[1,0,4,2,1,3,2,2,3,4,5,1,5,5],\n",
    "[3,1,0,4,2,2,4,3,4,0,4,2,2,4],\n",
    "[2,3,1,2,2,4,2,1,5,0,5,1,5,3],\n",
    "[1,5,0,5,3,4,5,4,2,3,5,1,3,2],\n",
    "[0,4,0,4,0,4,5,0,4,5,0,1,2,2],\n",
    "[3,2,4,4,3,4,2,4,5,1,0,0,5,5],\n",
    "[1,3,4,4,5,1,3,5,0,4,2,3,5,3],\n",
    "[3,3,1,5,1,5,1,2,1,3,5,0,5,3],\n",
    "[3,1,0,4,2,3,2,0,2,4,4,3,2,4],\n",
    "[4,2,3,1,1,2,0,2,2,3,1,3,1,5],\n",
    "[5,3,1,5,1,5,3,4,4,2,4,5,4,2],\n",
    "[5,1,2,3,1,1,2,0,5,5,0,5,1,1],\n",
    "[2,4,0,1,3,1,3,2,0,1,1,0,2,2],\n",
    "[4,0,5,5,0,2,2,1,0,1,0,4,5,5],\n",
    "[3,2,1,5,0,5,0,1,5,1,2,0,4,0],\n",
    "[2,4,2,3,5,0,2,2,1,5,3,2,3,1],\n",
    "[3,3,5,1,2,3,3,5,1,3,4,2,2,3],\n",
    "[5,1,3,4,3,5,3,1,2,4,4,0,2,4],\n",
    "[4,4,3,4,5,0,2,3,2,3,4,2,1,2],\n",
    "[2,3,3,2,3,3,1,5,4,5,0,4,0,1],\n",
    "[4,2,3,1,3,5,4,0,1,3,4,2,1,1],\n",
    "[5,0,2,4,5,3,3,1,3,5,4,0,1,1],\n",
    "[3,4,0,1,5,0,0,1,2,4,0,4,4,3],\n",
    "[2,3,5,4,2,4,0,2,0,5,1,1,5,5],\n",
    "[4,2,3,5,1,3,2,1,5,4,3,1,5,3],\n",
    "[5,1,3,5,4,2,3,4,5,1,2,3,4,0],\n",
    "[5,5,3,2,3,4,0,4,3,1,0,5,4,5],\n",
    "[3,5,0,4,3,5,5,1,3,2,0,2,4,2],\n",
    "[3,1,2,0,5,1,0,2,3,5,1,0,4,5],\n",
    "[2,0,5,3,3,5,1,3,4,2,3,5,3,4],\n",
    "[1,0,5,1,2,3,1,1,2,4,4,2,2,0],\n",
    "[1,5,1,5,1,3,5,0,4,3,1,5,4,2],\n",
    "[0,1,3,3,2,3,4,5,1,5,1,0,0,4],\n",
    "[1,3,4,0,5,3,2,0,1,3,2,4,0,2],\n",
    "[3,2,3,2,2,1,2,1,3,1,3,1,2,4],\n",
    "[4,2,4,0,2,0,4,3,5,0,4,0,4,5],\n",
    "[3,1,2,2,4,0,4,2,3,1,0,1,5,1],\n",
    "[0,1,1,5,1,3,1,5,4,3,1,5,3,3],\n",
    "[3,3,2,2,3,1,2,0,4,2,3,4,3,1],\n",
    "[4,4,0,4,3,1,2,1,5,5,4,2,0,2],\n",
    "[2,4,0,0,1,2,4,2,3,1,2,4,4,3],\n",
    "[3,1,0,4,2,2,4,3,4,0,5,5,0,5],\n",
    "[5,3,1,1,0,5,1,2,1,0,2,2,1,2],\n",
    "[5,3,4,0,4,3,3,5,1,2,3,2,4,3],\n",
    "[2,3,5,1,3,3,5,4,2,2,4,2,0,0],\n",
    "[0,5,1,3,2,3,2,4,3,1,1,2,3,4],\n",
    "[4,2,4,4,3,5,0,4,3,2,3,3,1,1],\n",
    "[2,4,0,0,4,3,3,5,3,5,4,0,5,5],\n",
    "[2,3,2,2,1,5,4,2,3,4,5,3,2,4],\n",
    "[2,1,3,1,1,0,4,2,3,1,2,0,5,1],\n",
    "[0,5,0,5,1,3,2,4,5,4,2,1,3,5],\n",
    "[5,1,0,4,3,2,4,2,4,5,5,3,2,4],\n",
    "[5,0,4,0,4,5,0,2,0,4,0,5,0,1],\n",
    "[2,3,2,4,3,1,5,4,2,3,5,0,4,2],\n",
    "[1,5,0,5,1,2,3,2,4,0,0,4,3,4],\n",
    "[2,3,4,0,2,2,1,3,2,4,2,4,2,4],\n",
    "[2,0,1,5,0,0,4,2,4,3,2,0,0,1],\n",
    "[5,5,3,1,3,3,5,0,2,3,2,4,0,4],\n",
    "[0,2,2,4,2,3,2,4,4,2,1,3,4,4],\n",
    "[2,0,4,2,2,1,3,2,3,2,4,3,1,0],\n",
    "[3,1,2,3,5,1,5,5,3,3,1,0,4,0],\n",
    "[3,5,0,1,1,3,2,3,1,5,0,2,0,4],\n",
    "[4,5,1,3,2,2,1,1,3,1,5,0,5,3],\n",
    "[5,0,4,2,3,2,4,3,2,3,5,0,5,4],\n",
    "[3,2,1,2,4,2,1,2,4,4,0,2,4,3],\n",
    "[3,5,5,4,4,2,3,2,0,5,4,4,5,0],\n",
    "[1,0,2,4,2,0,0,5,0,5,1,0,4,2],\n",
    "[4,2,4,3,2,4,2,1,0,5,1,2,2,3],\n",
    "[2,1,0,0,5,0,1,3,2,3,1,1,2,0],\n",
    "[4,2,2,1,2,4,2,3,2,0,2,1,2,4],\n",
    "[2,3,5,4,4,0,1,2,1,5,0,0,1,1],\n",
    "[5,5,1,0,2,3,1,5,5,4,0,5,3,4],\n",
    "[3,1,5,4,2,3,2,0,2,4,0,0,4,0],\n",
    "[3,2,3,4,0,1,3,4,5,0,1,1,5,1],\n",
    "[5,3,1,2,0,4,0,2,4,2,0,1,2,2],\n",
    "[2,4,3,4,2,4,0,4,2,3,1,0,1,5],\n",
    "[3,3,1,2,2,1,0,5,1,3,1,0,5,0],\n",
    "[4,2,1,5,0,4,4,2,2,0,5,3,2,4],\n",
    "[0,5,1,2,2,1,0,1,3,5,0,4,0,1],\n",
    "[2,4,5,4,0,1,1,5,3,1,3,2,3,2],\n",
    "[5,0,4,0,4,5,4,2,1,0,4,0,4,4],\n",
    "[4,4,5,4,2,2,0,1,3,1,3,5,1,2],\n",
    "[2,1,3,5,0,2,3,4,5,1,3,4,5,1],\n",
    "[0,2,3,5,5,3,1,3,4,0,5,3,5,4],\n",
    "[4,3,1,3,2,4,0,4,2,4,5,0,1,1],\n",
    "[0,5,3,4,4,2,4,2,3,5,1,1,0,1],\n",
    "[3,4,4,2,4,0,1,0,0,1,0,0,2,1],\n",
    "[4,3,2,1,5,3,1,5,3,5,3,4,0,1],\n",
    "[2,4,2,3,1,0,2,3,3,4,3,4,5,1],\n",
    "[2,4,2,0,1,5,1,3,5,1,2,3,3,5],\n",
    "[3,4,0,4,4,0,4,2,0,1,1,0,5,5],\n",
    "[5,5,1,0,5,0,4,3,1,3,2,2,3,3],\n",
    "[3,1,5,3,1,5,1,3,5,1,5,1,1,2],\n",
    "[4,2,3,1,5,5,0,5,0,2,1,5,1,1],\n",
    "[5,4,2,1,5,0,4,0,5,4,5,5,4,5],\n",
    "[0,5,4,2,0,4,2,1,2,0,0,5,4,2],\n",
    "[5,0,0,2,2,1,5,4,2,4,4,3,3,4],\n",
    "[3,1,0,5,1,5,4,2,4,5,4,3,5,1],\n",
    "[4,4,5,0,4,5,3,3,5,1,0,4,2,1],\n",
    "[4,2,3,4,0,2,4,2,2,4,0,5,4,4],\n",
    "[2,4,5,4,0,1,5,1,5,3,5,0,0,1],\n",
    "[3,4,2,4,0,1,2,4,4,3,1,0,1,2],\n",
    "[5,1,2,2,3,5,0,5,4,0,5,5,4,4],\n",
    "[3,2,3,1,2,2,4,5,3,3,1,3,1,5],\n",
    "[3,4,3,2,4,0,4,0,1,2,1,5,0,5],\n",
    "[1,5,4,3,1,5,3,2,0,5,0,2,3,1],\n",
    "[4,5,1,3,5,0,4,2,3,2,0,2,4,4],\n",
    "[1,3,2,4,0,2,3,5,4,2,3,2,4,4],\n",
    "[2,0,1,1,5,0,2,3,2,4,2,2,0,2],\n",
    "[3,1,5,0,4,2,4,2,1,0,4,2,1,1],\n",
    "[3,4,2,4,0,5,3,1,1,3,1,2,4,2],\n",
    "[4,4,5,3,5,1,2,3,1,2,2,4,2,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_zero():\n",
    "    return random.uniform(0, 49) / 100\n",
    "\n",
    "def generate_one():\n",
    "    return random.uniform(50, 100) / 100\n",
    "\n",
    "def generate_both(num_data_points, p):\n",
    "    Xs, Ys = [], []\n",
    "    for _ in range(num_data_points):\n",
    "        if random.random() < p:\n",
    "            Xs.append([generate_zero(), generate_zero(), 0]); Ys.append([0])\n",
    "            # or(1, 0) -> 1\n",
    "            Xs.append([generate_one(), generate_zero(), 0]); Ys.append([1])\n",
    "            # or(0, 1) -> 1\n",
    "            Xs.append([generate_zero(), generate_one(), 0]); Ys.append([1])\n",
    "            # or(1, 1) -> 1\n",
    "            Xs.append([generate_one(), generate_one(), 0]); Ys.append([1])\n",
    "        else:\n",
    "            # xor(0, 0) -> 0\n",
    "            Xs.append([generate_zero(), generate_zero(), 1]); Ys.append([0])\n",
    "            # xor(1, 0) -> 1\n",
    "            Xs.append([generate_one(), generate_zero(), 1]); Ys.append([1])\n",
    "            # xor(0, 1) -> 1\n",
    "            Xs.append([generate_zero(), generate_one(), 1]); Ys.append([1])\n",
    "            # xor(1, 1) -> 0\n",
    "            Xs.append([generate_one(), generate_one(), 1]); Ys.append([0])\n",
    "    return Xs, Ys\n",
    "\n",
    "def generate_or_XY(num_data_points):\n",
    "    Xs, Ys = [], []\n",
    "    for _ in range(num_data_points):\n",
    "        # or(0, 0) -> 0 \n",
    "        Xs.append([generate_zero(), generate_zero(), 0]); Ys.append([0])\n",
    "        # or(1, 0) -> 1\n",
    "        Xs.append([generate_one(), generate_zero(), 0]); Ys.append([1])\n",
    "        # or(0, 1) -> 1\n",
    "        Xs.append([generate_zero(), generate_one(), 0]); Ys.append([1])\n",
    "        # or(1, 1) -> 1\n",
    "        Xs.append([generate_one(), generate_one(), 0]); Ys.append([1])\n",
    "    return Xs, Ys\n",
    "\n",
    "def generate_xor_XY(num_data_points):\n",
    "    Xs, Ys = [], []\n",
    "    for _ in range(num_data_points):\n",
    "        # xor(0, 0) -> 0 \n",
    "        Xs.append([generate_zero(), generate_zero(), 1]); Ys.append([0])\n",
    "        # xor(1, 0) -> 1\n",
    "        Xs.append([generate_one(), generate_zero(), 1]); Ys.append([1])\n",
    "        # xor(0, 1) -> 1\n",
    "        Xs.append([generate_zero(), generate_one(), 1]); Ys.append([1])\n",
    "        # xor(1, 1) -> 0\n",
    "        Xs.append([generate_one(), generate_one(), 1]); Ys.append([0])\n",
    "    return Xs, Ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] False None\n",
      "Average loss or:  0.8620795825123787\n",
      "Average loss xor:  0.7334545511007309\n",
      "Average loss:  0.7977670668065548\n",
      "[0.8937549889087677, 0.9609179496765137, 0.8265920281410217, 70, [2], False, None]\n",
      "[2] True None\n",
      "Average loss or:  0.9075014442205429\n",
      "Average loss xor:  0.7316605460643768\n",
      "Average loss:  0.8195809951424599\n",
      "[0.9125300049781799, 0.9765307307243347, 0.8485292792320251, 88, [2], True, None]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-df2355e0b176>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0mxor_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0mxor_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m \u001b[0mxor_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0mxor_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-df2355e0b176>\u001b[0m in \u001b[0;36mxor_experiments\u001b[0;34m(initial_capacity, train_or, capacity)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def xor_experiments(initial_capacity, train_or, capacity):\n",
    "    lowest_loss = 0\n",
    "    lowest_settings = []    \n",
    "    losses_xor = []\n",
    "    losses_or = []\n",
    "    for seed in range(100):\n",
    "        # Set seeds\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        # Initialisation network\n",
    "        network = DQN(3,initial_capacity.copy(),1,F.relu)\n",
    "        # optimizer = optim.Adam(network.parameters(), amsgrad=False)\n",
    "        optimizer = optim.Adam(network.parameters(), amsgrad=True)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        if train_or:\n",
    "            for i in range(500):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                Xs, Ys = generate_both(25,0.1)\n",
    "                    \n",
    "                Xs = torch.tensor(Xs)\n",
    "                Ys = torch.tensor(Ys, dtype=torch.float)\n",
    "\n",
    "                prediction = network(Xs)\n",
    "                loss = criterion(prediction, Ys)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # Evaluation\n",
    "                    prediction = network(torch.tensor([[0,0,0],[0,1,0],[1,0,0],[1,1,0]], dtype=torch.float))\n",
    "                    Ys = torch.tensor([[0],[1],[1],[1]], dtype=torch.float)\n",
    "                    loss = 1.0/(1.0+criterion(prediction, Ys))\n",
    "\n",
    "                if loss>0.95:\n",
    "                    break\n",
    "        \n",
    "        \n",
    "        if capacity is not None:\n",
    "            network, optimizer = increase_capacity_keep_lr(network, capacity, optimizer, 'cpu')\n",
    "        \n",
    "        \n",
    "        iters = 500\n",
    "        if not train_or:\n",
    "            iters * 2\n",
    "            \n",
    "        for i in range(iters):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Uniform syllabus 20% of the time\n",
    "            Xs, Ys = generate_both(25,0.9)\n",
    "                \n",
    "            Xs = torch.tensor(Xs)\n",
    "            Ys = torch.tensor(Ys, dtype=torch.float)\n",
    "\n",
    "            prediction = network(Xs)\n",
    "            loss = criterion(prediction, Ys)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        average_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Test or\n",
    "            prediction = network(torch.tensor([[0,0,0],[0,1,0],[1,0,0],[1,1,0]], dtype=torch.float))\n",
    "            Ys = torch.tensor([[0],[1],[1],[1]], dtype=torch.float)\n",
    "            loss = 1.0/(1.0+criterion(prediction, Ys))\n",
    "        \n",
    "        average_loss += loss.item()\n",
    "        losses_or.append(loss.item())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Test xor\n",
    "            prediction = network(torch.tensor([[0,0,1],[0,1,1],[1,0,1],[1,1,1]], dtype=torch.float))\n",
    "            Ys = torch.tensor([[0],[1],[1],[0]], dtype=torch.float)\n",
    "            loss = 1.0/(1.0+criterion(prediction, Ys))\n",
    "        \n",
    "        average_loss += loss.item()\n",
    "        average_loss /= 2\n",
    "        losses_xor.append(loss.item())\n",
    "        \n",
    "        if average_loss > lowest_loss:\n",
    "            lowest_loss = copy.copy(average_loss)\n",
    "            lowest_settings = [average_loss, losses_or[-1], losses_xor[-1], seed, initial_capacity, train_or, capacity]\n",
    "        \n",
    "        if loss>0.95:\n",
    "            break\n",
    "        \n",
    "    # Print statistics\n",
    "    print(initial_capacity, train_or, capacity)\n",
    "    print('Average loss or: ', np.average(losses_or))\n",
    "    print('Average loss xor: ', np.average(losses_xor))\n",
    "    print('Average loss: ', (np.average(losses_or) +  np.average(losses_xor))/2)\n",
    "    print(lowest_settings)\n",
    "    \n",
    "xor_experiments([2],False, None)\n",
    "xor_experiments([2], True, None)\n",
    "xor_experiments([1], True, [1])\n",
    "\n",
    "xor_experiments([3],False,None)\n",
    "xor_experiments([3], True, None)\n",
    "xor_experiments([2], True, [1])\n",
    "xor_experiments([1], True, [2])\n",
    "\n",
    "xor_experiments([4],False,None)\n",
    "xor_experiments([4], True, None)\n",
    "xor_experiments([1], True, [3])\n",
    "xor_experiments([2], True, [2])\n",
    "xor_experiments([3], True, [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_capacity = [4]\n",
    "capacity = None\n",
    "train_or = True\n",
    "# Set seeds\n",
    "random.seed(99)\n",
    "np.random.seed(99)\n",
    "torch.manual_seed(99)\n",
    "\n",
    "# Initialisation network\n",
    "network = DQN(3,initial_capacity.copy(),1,F.relu)\n",
    "# optimizer = optim.Adam(network.parameters(), amsgrad=False)\n",
    "optimizer = optim.Adam(network.parameters(), amsgrad=True)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "if train_or:\n",
    "    for i in range(1000):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if np.random.rand() < 0.2:\n",
    "            if np.random.rand() < 0.5:\n",
    "                Xs, Ys = generate_or_XY(1)\n",
    "            else:\n",
    "                Xs, Ys = generate_xor_XY(1)\n",
    "        else:\n",
    "            Xs, Ys = generate_or_XY(1)\n",
    "\n",
    "        Xs = torch.tensor(Xs)\n",
    "        Ys = torch.tensor(Ys, dtype=torch.float)\n",
    "\n",
    "        prediction = network(Xs)\n",
    "        loss = criterion(prediction, Ys)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Evaluation\n",
    "            prediction = network(torch.tensor([[0,0,0],[0,1,0],[1,0,0],[1,1,0]], dtype=torch.float))\n",
    "            Ys = torch.tensor([[0],[1],[1],[1]], dtype=torch.float)\n",
    "            loss = criterion(prediction, Ys)\n",
    "\n",
    "        if loss<0.05:\n",
    "            break\n",
    "\n",
    "\n",
    "    nw_before = copy.deepcopy(network) \n",
    "if capacity is not None:\n",
    "    network, optimizer = increase_capacity_keep_lr(network, capacity, optimizer, 'cpu')\n",
    "\n",
    "nw_after_increase = copy.deepcopy(network)\n",
    "\n",
    "iters = 1000\n",
    "if not train_or:\n",
    "    iters * 2\n",
    "\n",
    "for i in range(iters):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Uniform syllabus 20% of the time\n",
    "    if np.random.rand() < 0.2:\n",
    "        if np.random.rand() < 0.5:\n",
    "            Xs, Ys = generate_or_XY(1)\n",
    "        else:\n",
    "            Xs, Ys = generate_or_XY(1)\n",
    "    else:\n",
    "        Xs, Ys = generate_xor_XY(1)\n",
    "\n",
    "    Xs = torch.tensor(Xs)\n",
    "    Ys = torch.tensor(Ys, dtype=torch.float)\n",
    "\n",
    "    prediction = network(Xs)\n",
    "    loss = criterion(prediction, Ys)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Test or\n",
    "    prediction = network(torch.tensor([[0,0,0],[0,1,0],[1,0,0],[1,1,0]], dtype=torch.float))\n",
    "    Ys = torch.tensor([[0],[1],[1],[1]], dtype=torch.float)\n",
    "    loss = criterion(prediction, Ys)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Test xor\n",
    "    prediction = network(torch.tensor([[0,0,1],[0,1,1],[1,0,1],[1,1,1]], dtype=torch.float))\n",
    "    Ys = torch.tensor([[0],[1],[1],[0]], dtype=torch.float)\n",
    "    loss = criterion(prediction, Ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising specific weights, biases, and activations\n",
    "from matplotlib import colors\n",
    "\n",
    "def show_mat_label(data, data2=None):\n",
    "    data_list = []\n",
    "    if data2 is not None:\n",
    "        vmin = data.min()\n",
    "        vmax = data.max()\n",
    "\n",
    "        fig, axs = plt.subplots(3,1)\n",
    "        data_list.append(axs[0].matshow(data))\n",
    "\n",
    "        vmin = min(vmin,data2.min())\n",
    "        vmax = max(vmax,data2.max())\n",
    "        \n",
    "        data_list.append(axs[1].matshow(data2))\n",
    "        axs[1].get_xaxis().set_visible(False)\n",
    "\n",
    "        vmin = min(vmin,np.abs(data-data2).min())\n",
    "        vmax = max(vmax,np.abs(data-data2).max())\n",
    "        data_list.append(axs[2].matshow(np.abs(data-data2)))\n",
    "        axs[2].get_xaxis().set_visible(False)\n",
    "        \n",
    "        norm = colors.Normalize(vmin=0, vmax=1)\n",
    "        \n",
    "        fig.colorbar(data_list[0], ax=axs)\n",
    "        \n",
    "        for d in data_list:\n",
    "            d.set_norm(norm)\n",
    "    else:\n",
    "        fig, axs = plt.subplots()\n",
    "        cax = axs.matshow(data)\n",
    "        if data.shape[0]==1:\n",
    "            axs.get_yaxis().set_visible(False)\n",
    "        fig.colorbar(cax)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "x = torch.tensor([[0,0,1]],dtype=torch.float)\n",
    "\n",
    "weights0 = network.layers[0].weight.data.numpy()\n",
    "bias0 = np.array([network.layers[0].bias.data.numpy()])\n",
    "\n",
    "weights1 = network.layers[1].weight.data.numpy()\n",
    "bias1 = np.array([network.layers[1].bias.data.numpy()])\n",
    "\n",
    "x = torch.tensor([[1,1,0]],dtype=torch.float)\n",
    "print(network(x))\n",
    "x2 = torch.tensor([[1,1,1]],dtype=torch.float)\n",
    "print(network(x2))\n",
    "\n",
    "print('Weights')\n",
    "show_mat_label(weights0)\n",
    "print('Bias')\n",
    "show_mat_label(bias0)\n",
    "print('Activations')\n",
    "show_mat_label(F.relu(network.layers[0](x)).data.numpy(),F.relu(network.layers[0](x2)).data.numpy())\n",
    "print(np.abs(F.relu(network.layers[0](x)).data.numpy()-F.relu(network.layers[0](x2)).data.numpy()))\n",
    "\n",
    "x = F.relu(network.layers[0](x))\n",
    "x2 = F.relu(network.layers[0](x2))\n",
    "\n",
    "print('Weights')\n",
    "show_mat_label(weights1)\n",
    "print('Bias')\n",
    "show_mat_label(bias1)\n",
    "print('Activations')\n",
    "\n",
    "show_mat_label(network.layers[1](x).data.numpy(),network.layers[1](x2).data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_change(initial_capacity, capacity, train_or):\n",
    "    common = []\n",
    "    new = []\n",
    "    total = []\n",
    "    common_bias = []\n",
    "    new_bias = []\n",
    "\n",
    "    for seed in range(100):  \n",
    "        # Set seeds\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        # Initialisation network\n",
    "        network = DQN(3,initial_capacity.copy(),1,F.relu)\n",
    "        # optimizer = optim.Adam(network.parameters(), amsgrad=False)\n",
    "        optimizer = optim.Adam(network.parameters(), amsgrad=True)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        if train_or:\n",
    "            for i in range(1000):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if np.random.rand() < 0.2:\n",
    "                    if np.random.rand() < 0.5:\n",
    "                        Xs, Ys = generate_or_XY(1)\n",
    "                    else:\n",
    "                        Xs, Ys = generate_xor_XY(1)\n",
    "                else:\n",
    "                    Xs, Ys = generate_or_XY(1)\n",
    "\n",
    "                Xs = torch.tensor(Xs)\n",
    "                Ys = torch.tensor(Ys, dtype=torch.float)\n",
    "\n",
    "                prediction = network(Xs)\n",
    "                loss = criterion(prediction, Ys)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # Evaluation\n",
    "                    prediction = network(torch.tensor([[0,0,0],[0,1,0],[1,0,0],[1,1,0]], dtype=torch.float))\n",
    "                    Ys = torch.tensor([[0],[1],[1],[1]], dtype=torch.float)\n",
    "                    loss = criterion(prediction, Ys)\n",
    "\n",
    "                if loss<0.05:\n",
    "                    break\n",
    "\n",
    "\n",
    "            nw_before = copy.deepcopy(network) \n",
    "        if capacity is not None:\n",
    "            network, optimizer = increase_capacity_keep_lr(network, capacity, optimizer, 'cpu')\n",
    "\n",
    "        nw_after_increase = copy.deepcopy(network)\n",
    "\n",
    "        iters = 1000\n",
    "        if not train_or:\n",
    "            iters * 2\n",
    "\n",
    "        for i in range(iters):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Uniform syllabus 20% of the time\n",
    "            if np.random.rand() < 0.2:\n",
    "                if np.random.rand() < 0.5:\n",
    "                    Xs, Ys = generate_or_XY(1)\n",
    "                else:\n",
    "                    Xs, Ys = generate_or_XY(1)\n",
    "            else:\n",
    "                Xs, Ys = generate_xor_XY(1)\n",
    "\n",
    "            Xs = torch.tensor(Xs)\n",
    "            Ys = torch.tensor(Ys, dtype=torch.float)\n",
    "\n",
    "            prediction = network(Xs)\n",
    "            loss = criterion(prediction, Ys)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Test or\n",
    "            prediction = network(torch.tensor([[0,0,0],[0,1,0],[1,0,0],[1,1,0]], dtype=torch.float))\n",
    "            Ys = torch.tensor([[0],[1],[1],[1]], dtype=torch.float)\n",
    "            loss = criterion(prediction, Ys)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Test xor\n",
    "            prediction = network(torch.tensor([[0,0,1],[0,1,1],[1,0,1],[1,1,1]], dtype=torch.float))\n",
    "            Ys = torch.tensor([[0],[1],[1],[0]], dtype=torch.float)\n",
    "            loss = criterion(prediction, Ys)\n",
    "\n",
    "        common.append([torch.abs(nw_after_increase.layers[0].weight[0]-network.layers[0].weight[0]).mean().data.numpy(), torch.abs(nw_after_increase.layers[1].weight[0][0]-network.layers[1].weight[0][0]).mean().data.numpy()])\n",
    "        new.append([torch.abs(nw_after_increase.layers[0].weight[1:]-network.layers[0].weight[1:]).mean().data.numpy(), torch.abs(nw_after_increase.layers[1].weight[0][1:]-network.layers[1].weight[0][1:]).mean().data.numpy()])\n",
    "        total.append([torch.abs(nw_after_increase.layers[0].weight[:]-network.layers[0].weight[:]).mean().data.numpy(), torch.abs(nw_after_increase.layers[1].weight[0][:]-network.layers[1].weight[0][:]).mean().data.numpy()])\n",
    "        \n",
    "        common_bias.append([torch.abs(nw_after_increase.layers[0].bias[0]-network.layers[0].bias[0]).mean().data.numpy(), torch.abs(nw_after_increase.layers[1].bias[0]-network.layers[1].bias[0]).mean().data.numpy()])\n",
    "        new_bias.append([torch.abs(nw_after_increase.layers[0].bias[1:]-network.layers[0].bias[1:]).mean().data.numpy(), torch.abs(nw_after_increase.layers[1].bias[0]-network.layers[1].bias[0]).mean().data.numpy()])\n",
    "        \n",
    "    print(\"Common layer 0:\\t\", np.average(np.array(common)[:,0]))\n",
    "    print(\"New layer 0:\\t\", np.average(np.array(new)[:,0]))\n",
    "    print(\"Total layer 0:\\t\", np.average(np.array(total)[:,0]))\n",
    "    print(\"Common bias 0:\\t\", np.average(np.array(common_bias)[:,0]))\n",
    "    print(\"New bias 0:\\t\", np.average(np.array(new_bias)[:,0]))\n",
    "    \n",
    "    print(\"Common layer 1:\\t\", np.average(np.array(common)[:,1]))\n",
    "    print(\"New layer 1:\\t\", np.average(np.array(new)[:,1]))\n",
    "    print(\"Total layer 1:\\t\", np.average(np.array(total)[:,1]))\n",
    "    print(\"Common bias 1:\\t\", np.average(np.array(common_bias)[:,1]))\n",
    "    print(\"New bias 1:\\t\", np.average(np.array(new_bias)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-496-98b4062f53cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcapacity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_or\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mweight_change\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_capacity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapacity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_or\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'----'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0minitial_capacity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-495-1ed4b587f53a>\u001b[0m in \u001b[0;36mweight_change\u001b[0;34m(initial_capacity, capacity, train_or)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "initial_capacity = [4]\n",
    "capacity = None\n",
    "train_or = True\n",
    "weight_change(initial_capacity, capacity, train_or)\n",
    "print('----')\n",
    "initial_capacity = [1]\n",
    "capacity = [3]\n",
    "train_or = True\n",
    "weight_change(initial_capacity, capacity, train_or)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2500)"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[0.2,0.3]]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
